[
  {
    "objectID": "guides/03_transpiling_a_tf_model.html",
    "href": "guides/03_transpiling_a_tf_model.html",
    "title": "Transpiling a TensorFlow model to build on top",
    "section": "",
    "text": "Transpile a keras model to haiku and leverage Jax efficiency.\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "guides/04_developing_a_convnet_with_ivy.html",
    "href": "guides/04_developing_a_convnet_with_ivy.html",
    "title": "Developing a convolutional network using Ivy",
    "section": "",
    "text": "Build a more involved convolutional network using Ivy‚Äôs stateful API.\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "guides/01_transpiling_a_torch_model.html",
    "href": "guides/01_transpiling_a_torch_model.html",
    "title": "Transpiling a PyTorch model to build on top",
    "section": "",
    "text": "Transpile a timm model to tensorflow and build a new model around it.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nIn Transpile Any Model we have seen how to transpile a very simple model. In the Guides, we will focus on transpiling more involved models developed in different frameworks.\nIn this first notebook, we will transpile a model from the PyTorch image models repo (timm) to TensorFlow, building a classifier on top of the resulting module.\nAs usual, let‚Äôs start with the imports\nimport ivy\nimport torch\nimport timm\nimport numpy as np\nimport tensorflow as tf\nNow, instead of building our own PyTorch model, we will get one directly from the timm package!\nIn this case, we are going to use a MLP-Mixer. We can download the pretrained weights with pretrained=True and set num_classes=0 to only retrieve the feature extractor.\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\nNow, we will transpile the MLP-Mixer feature extractor to TensorFlow using ivy.transpile and passing a sample torch.Tensor with noise.\nnoise = torch.randn(1, 3, 224, 224)\ntf_mlp_encoder = ivy.transpile(mlp_encoder, to=\"tensorflow\", args=(noise,))\nTo ensure that the transpilation has been correct, let‚Äôs check with a new input in both frameworks. Keep in mind that all the functions called within tf_mlp_encoder are now TensorFlow functions üîÄ\nx = np.random.random(size=(1, 3, 224, 224)).astype(np.float32)\noutput_torch = mlp_encoder(torch.tensor(x))\noutput_tf = tf_mlp_encoder(tf.constant(x))\nprint(np.allclose(output_torch.detach(), output_tf, rtol=1e-1))\n\nTrue\nNow, we can build or own classifier using the transpiled module as the feature extractor:\nclass Classifier(tf.keras.Model):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.encoder = tf_mlp_encoder\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\"softmax\")\n\n    def call(self, x):\n        x = self.encoder(x)\n        return self.output_dense(x)\nAnd finally, we can use our new model! As we have mentioned in ‚ÄúLearn the Basics‚Äù, the transpiled model is fully trainable in the target framework, so you can also fine-tune your transpiled modules or train them from the ground up! üìâ\nmodel = Classifier()\n\nx = tf.random.normal(shape=(1, 3, 224, 224))\nret = model(x)\nprint(type(ret), ret.shape)\n\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt; (1, 1000)"
  },
  {
    "objectID": "guides/01_transpiling_a_torch_model.html#round-up",
    "href": "guides/01_transpiling_a_torch_model.html#round-up",
    "title": "Transpiling a PyTorch model to build on top",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it! Now you are ready to transpile any PyTorch model, layer or trainable module and integrate it within TensorFlow, but let‚Äôs keep exploring how we can convert trainable modules from (and to!) other frameworks ‚û°Ô∏è"
  },
  {
    "objectID": "guides/02_transpiling_a_haiku_model.html",
    "href": "guides/02_transpiling_a_haiku_model.html",
    "title": "Transpiling a haiku model to build on top",
    "section": "",
    "text": "Transpile DeepMind‚Äôs PerceiverIO to PyTorch to use it as a pretrained backbone.\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "assets/template.html",
    "href": "assets/template.html",
    "title": "TO REPLACE: Title",
    "section": "",
    "text": "TO REPLACE: description\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "assets/template.html#round-up",
    "href": "assets/template.html#round-up",
    "title": "TO REPLACE: Title",
    "section": "Round Up",
    "text": "Round Up\nTO REPLACE"
  },
  {
    "objectID": "examples_and_demos/01_template.html",
    "href": "examples_and_demos/01_template.html",
    "title": "TO REPLACE: Title",
    "section": "",
    "text": "TO REPLACE: description\nIf you already have Ivy installed you can skip this cell, but if you are using Colab, you will have to install Ivy manually. You can do so by running the cell below ‚¨áÔ∏è\nKeep in mind that for the package to be available, you will have to click on ‚ÄúRuntime &gt; Restart Runtime‚Äù üòÑ\n\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && python3 -m pip install --user -e .\n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/08_transpile_any_model.html",
    "href": "learn_the_basics/08_transpile_any_model.html",
    "title": "Transpile any model",
    "section": "",
    "text": "Transpile a Keras model into a PyTorch module.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nAs we‚Äôve already seen, ivy.transpile can convert functions and whole libraries from one framework to another. However, in machine learning and deep learning, much of the focus is on trainable modules. Fortunately, Ivy can manage the parameters of these modules and ensure that the transpiled module is fully compatible with the target framework. This allows you to take full advantage of the training utilities provided by any framework and to build complex models on top of the transpiled ones. Let‚Äôs see how this works!\nLet‚Äôs start by importing the neccessary libraries:\nimport ivy\nimport torch\nimport numpy as np\nimport tensorflow as tf\nThere are examples which use more involved models in the Guides section, but to keep things simple, let‚Äôs define a basic convolutional network using Keras‚Äô Sequential API to use it as the starting point!\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 3)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\nNow, we can use ivy.transpile to convert this Keras model to PyTorch. Since we are passing a framework-specific object to the transpile function, there is no need to specify the source keyword argument this time.\ninput_array = tf.random.normal((1, 28, 28, 3))\ntorch_model = ivy.transpile(model, to=\"torch\", args=(input_array,))\nThanks to (eager) transpilation, we now have a fully-fledged torch.nn.module üöÄ\nisinstance(torch_model, torch.nn.Module)\n\nTrue\nThis means that we can pass PyTorch inputs (keeping the channels-last format of Keras, as the new computational graph is identical to the original one!) and get PyTorch tensors as the output:\ninput_array = torch.rand((1, 28, 28, 3))\noutput_array = torch_model(input_array)\nprint(output_array)\n\ntensor([[0.0768, 0.0727, 0.0942, 0.1300, 0.1350, 0.0839, 0.1511, 0.1061, 0.0606,\n         0.0896]], grad_fn=&lt;SoftmaxBackward0&gt;)\nFurthermore, having a torch.nn.Module also enables you to train the model using PyTorch training code, and also to use the transpiled model to build more complex torch models, as shown in the Transpiling a haiku model to build on top guide!"
  },
  {
    "objectID": "learn_the_basics/08_transpile_any_model.html#round-up",
    "href": "learn_the_basics/08_transpile_any_model.html#round-up",
    "title": "Transpile any model",
    "section": "Round up",
    "text": "Round up\nThis is the last tutorial related to the compiler/transpiler in Learn the Basics. In the next tutorial, we‚Äôll go over an introduction to building models directly using Ivy üë®‚Äçüíª. If you are interested in continuing to learn about transpilation, you can check out the more complex tutorials in the Guides section!"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html",
    "href": "learn_the_basics/06_how_to_use_decorators.html",
    "title": "How to use decorators",
    "section": "",
    "text": "Learn about the different ways to use compilation and transpilation functions.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html#unify",
    "href": "learn_the_basics/06_how_to_use_decorators.html#unify",
    "title": "How to use decorators",
    "section": "Unify",
    "text": "Unify\nFirstly, let‚Äôs create the dummy numpy arrays as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.random.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport ivy\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\n\nIn the example below, the ivy.unify function is called as a decorator.\n\nimport torch\n\n@ivy.unify(source=\"torch\")\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # unification happens here\n\nivy.array([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])\n\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.unify(source=\"torch\", args=(x,))\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # already unified\n\nivy.array([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html#compile",
    "href": "learn_the_basics/06_how_to_use_decorators.html#compile",
    "title": "How to use decorators",
    "section": "Compile",
    "text": "Compile\nIn the example below, the ivy.compile function is also called as a decorator. (Note that this is now an Ivy function!)\n\n@ivy.compile\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x, correction=1)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\n\nnormalize(x) # compilation happens here\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])&gt;\n\n\nLikewise, the function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.compile(args=(x,))\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x, correction=1)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\n\nnormalize(x) # already compiled\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])&gt;"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html#transpile",
    "href": "learn_the_basics/06_how_to_use_decorators.html#transpile",
    "title": "How to use decorators",
    "section": "Transpile",
    "text": "Transpile\nIn the example below, the ivy.transpile function is called as a decorator.\n\n@ivy.transpile(source=\"torch\", to=\"tensorflow\")\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # transpilation happens here\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])&gt;\n\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.transpile(source=\"torch\", to=\"tensorflow\", args=(x,))\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # already transpiled\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])&gt;"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html#round-up",
    "href": "learn_the_basics/06_how_to_use_decorators.html#round-up",
    "title": "How to use decorators",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you now know how ivy.unify, ivy.compile and ivy.transpile can all be used as function decorators! Next, we‚Äôll start exploring the transpilation of more involved objects, beginning with libraries üìö"
  },
  {
    "objectID": "learn_the_basics/09_write_a_model_using_ivy.html",
    "href": "learn_the_basics/09_write_a_model_using_ivy.html",
    "title": "Write a model using Ivy",
    "section": "",
    "text": "Develop a simple linear regression model using Ivy‚Äôs stateful API.\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/03_compile_code.html",
    "href": "learn_the_basics/03_compile_code.html",
    "title": "Compile code",
    "section": "",
    "text": "Turn your Ivy code into an efficient fully-functional graph, removing wrappers and unused parts of the code.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nFirstly, let‚Äôs pick up where we left off in the last notebook, with our unified normalize function:\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.unify(normalize, source=\"torch\")\nFor the purpose of illustration, we will use jax as our backend framework:\n# set ivy's backend to jax\nivy.set_backend(\"jax\")\n\n# Import jax\nimport jax\n\n# create random jax arrays for testing\nkey = jax.random.PRNGKey(42)\nx = jax.random.uniform(key, shape=(10,))\nAs in the previous example, the Ivy function can be executed like so (in this case it will trigger lazy unification, see the Lazy vs Eager section for more details):\nnormalize(x)\n\nivy.array([ 0.55563945, -0.65538704, -1.14150524,  1.46951997,  1.30220294,\n       -1.14739668, -0.57017946, -0.91962677,  0.51029003,  0.59644395])\nWhen calling this function, all of ivy‚Äôs function wrapping is included in the call stack of normalize, which adds runtime overhead. In general, ivy.compile strips any arbitrary function down to its constituent functions in the functional API of the target framework. The code can be compiled like so:\nivy.set_backend(\"jax\")\ncomp = ivy.compile(normalize)  # compiles to jax, due to ivy.set_backend\nThe compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\ncomp(x)\n\nArray([ 0.5556394 , -0.655387  , -1.1415051 ,  1.4695197 ,  1.3022028 ,\n       -1.1473966 , -0.5701794 , -0.91962665,  0.51028997,  0.5964439 ],      dtype=float32)\nWith all lazy compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:\n%%timeit\nnormalize(x)\n\n985 ¬µs ¬± 6.76 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n%%timeit\ncomp(x)\n\n69.5 ¬µs ¬± 1.24 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10,000 loops each)\nAs expected, we can see that normalize is slower, as it includes all ivy wrapping overhead. On the other hand, comp has no wrapping overhead and it‚Äôs more efficient!"
  },
  {
    "objectID": "learn_the_basics/03_compile_code.html#round-up",
    "href": "learn_the_basics/03_compile_code.html#round-up",
    "title": "Compile code",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now compile ivy code for more efficient inference! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to transpile code from one framework to another in a single line of code üîÑ"
  },
  {
    "objectID": "learn_the_basics/07_transpile_any_library.html",
    "href": "learn_the_basics/07_transpile_any_library.html",
    "title": "Transpile any library",
    "section": "",
    "text": "Transpile the kornia library to jax with just one line of code.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nIn previous tutorials, we demonstrated how to transpile simple functions from one framework to another using ivy.transpile. However, in real-world scenarios, you often need to access all functions from a specific library. Fortunately, the transpiler is capable of doing just that. Let‚Äôs explore a simple example where we convert the kornia library from torch to jax.\nFirst, let‚Äôs import everything we need:\nimport ivy\nimport kornia\nimport requests\nimport jax.numpy as jnp\nimport numpy as np\nfrom PIL import Image\nNow we can transpile the library to jax. Since it‚Äôs not practical to specify arguments for every function, we‚Äôll transpile it lazily.\njax_kornia = ivy.transpile(kornia, source=\"torch\", to=\"jax\")\nLet‚Äôs load a sample image and convert it to the format expected by kornia. Keep in mind that even though the operations will be performed in jax, the transpiler traces a computational graph, so we still need to use kornia‚Äôs data format.\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\nraw_img = Image.open(requests.get(url, stream=True).raw)\nimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))\nimg = jnp.expand_dims(img, 0) / 255\ndisplay(raw_img)\nNow that we have our sample image, we can easily call any kornia function using our transpiled version of the library, jax_kornia. As expected, both inputs and outputs of this function are jax.Array instances.\nout = jax_kornia.enhance.sharpness(img, 10)\ntype(out)\n\njaxlib.xla_extension.ArrayImpl\nFinally, we can verify that the transformation has been applied correctly!\nnp_image = np.uint8(np.array(out[0])*255)\ndisplay(Image.fromarray(np.transpose(np_image, (1, 2, 0))))"
  },
  {
    "objectID": "learn_the_basics/07_transpile_any_library.html#round-up",
    "href": "learn_the_basics/07_transpile_any_library.html#round-up",
    "title": "Transpile any library",
    "section": "Round Up",
    "text": "Round Up\nCongratulations! üéâ You are now capable of using any array computing library in your preferred framework leveraging ivy.transpile. In the next tutorial, we will explore how to convert trainable modules and layers from one framework to another ‚û°Ô∏è"
  },
  {
    "objectID": "learn_the_basics/04_transpile_code.html",
    "href": "learn_the_basics/04_transpile_code.html",
    "title": "Transpile code",
    "section": "",
    "text": "Convert a torch function to jax with just one line of code.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nUsing what we learnt in the previous two notebooks for Unify and Compile, the workflow for converting directly from torch to jax would be as follows, first unifying to ivy code, and then compiling to the jax backend:\nimport ivy\nimport torch\nivy.set_backend(\"jax\")\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n# convert the function to Ivy code\nivy_normalize = ivy.unify(normalize)\n\n# compile the Ivy code into jax functions\njax_normalize = ivy.compile(ivy_normalize)\nnormalize is now compiled to jax, ready to be integrated into your wider jax project.\nThis workflow is common, and so in order to avoid repeated calls to ivy.unify followed by ivy.compile, there is another convenience function ivy.transpile, which basically acts as a shorthand for this pair of function calls:\njax_normalize = ivy.transpile(normalize, source=\"torch\", to=\"jax\")\nAgain, normalize is now a jax function, ready to be integrated into your jax project.\nimport jax\n\nkey = jax.random.PRNGKey(42)\njax.config.update('jax_enable_x64', True)\nx = jax.random.uniform(key, shape=(10,))\n\nprint(jax_normalize(x))\n\n[-0.93968587  0.26075466 -0.22723222 -1.06276492 -0.47426987  1.72835908\n  1.71737559 -0.50411096 -0.65419174  0.15576624]"
  },
  {
    "objectID": "learn_the_basics/04_transpile_code.html#round-up",
    "href": "learn_the_basics/04_transpile_code.html#round-up",
    "title": "Transpile code",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now transpile code from one framework to another with one line of code! However, there are still other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. In the next notebooks we‚Äôll be learning about the various different ways that ivy.unify, ivy.compile and ivy.transpile can be called, and what implications each of these have!"
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html",
    "href": "learn_the_basics/05_lazy_vs_eager.html",
    "title": "Lazy vs Eager",
    "section": "",
    "text": "Understand the difference between eager and lazy compilation and transpilation.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nivy.unify, ivy.compile and ivy.transpile can all be performed either eagerly or lazily. All previous examples have been performed lazily, which means that the unification, compilation, or transpilation process actually occurs during the first call of the returned function.\nThis is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the ivy.unify, ivy.compile or ivy.transpile call itself, in which case the process is performed eagerly. We show some simple examples for each case below."
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html#unify",
    "href": "learn_the_basics/05_lazy_vs_eager.html#unify",
    "title": "Lazy vs Eager",
    "section": "Unify",
    "text": "Unify\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\nAnd let‚Äôs also create the dummy numpy arrays as before:\n\n# import NumPy\nimport numpy as np\n\n# create random numpy array for testing\nx = np.random.uniform(size=10)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\n\nIn the example below, the function is unified lazily, which means the first function call will execute slowly, as this is when the unification process actually occurs.\n\nnorm = ivy.unify(normalize, source=\"torch\")\nnorm(x) # slow, lazy unification\nnorm(x) # fast, unified on previous call\n\nivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])\n\n\nHowever, in the following example the unification occurs eagerly, and both function calls will be fast:\n\nivy.set_backend(\"tensorflow\")\nnorm = ivy.unify(normalize, source=\"torch\", args=(x,))\nnorm(x) # fast, unified at ivy.unify\nnorm(x) # fast, unified at ivy.unify\n\nivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])"
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html#compile",
    "href": "learn_the_basics/05_lazy_vs_eager.html#compile",
    "title": "Lazy vs Eager",
    "section": "Compile",
    "text": "Compile\nThe same is true for compiling. In the example below, the function is compiled lazily, which means the first function call will execute slowly, as this is when the compilation process actually occurs.\n\nnorm_comp = ivy.compile(norm)\nnorm_comp(x) # slow, lazy compilation\nnorm_comp(x) # fast, compiled on previous call\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])&gt;\n\n\nHowever, in the following example the compilation occurs eagerly, and both function calls will be fast:\n\nnorm_comp = ivy.compile(norm, args=(x,))\nnorm_comp(x) # fast, compiled at ivy.compile\nnorm_comp(x) # fast, compiled at ivy.compile\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])&gt;"
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html#transpile",
    "href": "learn_the_basics/05_lazy_vs_eager.html#transpile",
    "title": "Lazy vs Eager",
    "section": "Transpile",
    "text": "Transpile\nThe same is true for transpiling. In the example below, the function is transpiled lazily, which means the first function call will execute slowly, as this is when the transpilation process actually occurs.\n\nnorm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\")\nnorm_trans(x) # slow, lazy transpilation\nnorm_trans(x) # fast, transpiled on previous call\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])&gt;\n\n\nHowever, in the following example the transpilation occurs eagerly, and both function calls will be fast:\n\nnorm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\", args=(x,))\nnorm_trans(x) # fast, transpiled at ivy.transpile\nnorm_trans(x) # fast, transpiled at ivy.transpile\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])&gt;"
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html#round-up",
    "href": "learn_the_basics/05_lazy_vs_eager.html#round-up",
    "title": "Lazy vs Eager",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you now know the difference between lazy vs eager execution for ivy.unify, ivy.compile and ivy.transpile! Next, we‚Äôll be exploring how these three functions can all be called as function decorators!"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html",
    "href": "learn_the_basics/01_write_ivy_code.html",
    "title": "Write Ivy code",
    "section": "",
    "text": "Get familiar with Ivy‚Äôs basic concepts and start writing framework-agnostic code.\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nIn this introduction we‚Äôll go over the basics of using Ivy to write your own framework-indepent, future-proof code!\nIf you want to delve deeper into the theory behind the contents of this notebook you can check out the Design and the Deep Dive sections of the documentation!\nFirst of all, let‚Äôs import Ivy!\nimport ivy"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html#ivy-backend-handler",
    "href": "learn_the_basics/01_write_ivy_code.html#ivy-backend-handler",
    "title": "Write Ivy code",
    "section": "Ivy Backend Handler",
    "text": "Ivy Backend Handler\nIvy, when used as a ML framework, is esentially an abstraction layer that supports multiple frameworks as the backend. This means that any code written in Ivy can be executed in any of the supported frameworks, with their framework-specific data structures, functions, optimizations, quirks and perks, all managed by Ivy under the hood.\nTo change the backend, we can simply call ivy.set_backend with the appropiate framework passed as a string. This is the simplest way to interact with the Backend Handler submodule, which keeps track of the current backend and links Ivy‚Äôs objects and functions with the appropriate framework-specific ones.\nFor example:\n\nivy.set_backend(\"tensorflow\")"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html#data-structures",
    "href": "learn_the_basics/01_write_ivy_code.html#data-structures",
    "title": "Write Ivy code",
    "section": "Data Structures",
    "text": "Data Structures\nThe basic data structure in Ivy is the ivy.Array. This is an abstraction of the array classes of the supported frameworks. Likewise, we also have ivy.NativeArray, which is an alias for the array class of the selected backend.\nLastly, there is another structure, the ivy.Container, which is a subclass of dict optimized for recursive operations, you can learn more about it here!\nLet‚Äôs create an array using ivy.array(). In a similar fashion, we can use ivy.native_array() to create a torch.Tensor, as the backend is now torch.\n\nivy.set_backend(\"torch\")\n\nx = ivy.array([1, 2, 3])\nprint(type(x))\n\nx = ivy.native_array([1, 2, 3])\nprint(type(x))\n\n&lt;class 'ivy.data_classes.array.array.Array'&gt;\n&lt;class 'torch.Tensor'&gt;"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html#ivy-functional-api",
    "href": "learn_the_basics/01_write_ivy_code.html#ivy-functional-api",
    "title": "Write Ivy code",
    "section": "Ivy Functional API",
    "text": "Ivy Functional API\nIvy does not implement its own low-level (C++/CUDA) backend for its functions. Instead, it wraps the functional API of existing frameworks, unifying their fundamental functions under a common signature. For example, let‚Äôs take a look at ivy.matmul():\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\n&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;\n&lt;class 'torch.Tensor'&gt;\n\n\nThe output arrays above are ivy.Array instances, which is why we need to call the to_native() method to retrieve the underlying, native array.\nHowever, if you want the functions to return the native arrays directly, you can disable the array_mode of Ivy using ivy.set_array_mode().\n\nivy.set_array_mode(False)\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_array_mode(True)\n\n&lt;class 'jaxlib.xla_extension.ArrayImpl'&gt;\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;\n&lt;class 'torch.Tensor'&gt;\n\n\nKeeping this in mind, you can build any function you want as a composition of Ivy functions. When executed, this function will ultimately call the current backend functions from its functional API.\n\ndef sigmoid(z):\n    return ivy.divide(1, (1 + ivy.exp(-z)))"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html#ivy-stateful-api",
    "href": "learn_the_basics/01_write_ivy_code.html#ivy-stateful-api",
    "title": "Write Ivy code",
    "section": "Ivy Stateful API",
    "text": "Ivy Stateful API\nAlongside the Functional API, Ivy also has a stateful API, which builds on its functional API and the ivy.Container class to provide high-level classes such as optimizers, network layers, or trainable modules.\nThe most important stateful class within Ivy is ivy.Module, which can be used to create trainable layers and entire networks. Given the importance of this class, we will explore it further in the Write a model using Ivy tutorial!"
  },
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html#round-up",
    "href": "learn_the_basics/01_write_ivy_code.html#round-up",
    "title": "Write Ivy code",
    "section": "Round Up",
    "text": "Round Up\nCongratulations! There is much more to come, but you now have a basic understanding of Ivy and how it can be used to write framework-independent, future-proof code! Now that you have a good foundation, let‚Äôs keep exploring Ivy‚Äôs tools and their powerful features! üöÄ"
  },
  {
    "objectID": "learn_the_basics/02_unify_code.html",
    "href": "learn_the_basics/02_unify_code.html",
    "title": "Unify code",
    "section": "",
    "text": "Unify a simple torch function and use it alongside any ML framework!\n‚ö†Ô∏è If you are running this notebook in Colab, you will have to install Ivy and some dependencies manually. You can do so by running the cell below ‚¨áÔ∏è\nIf you want to run the notebook locally but don‚Äôt have Ivy installed just yet, you can check out the Setting Up section of the docs.\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout d6bc18c64a47a135fe18404d9f83f98d9f3b63cf && python3 -m pip install --user -e .\nFor the installed packages to be available you will have to restart your kernel. In Colab, you can do this by clicking on ‚ÄúRuntime &gt; Restart Runtime‚Äù. Once the runtime has been restarted you should skip the previous cell üòÑ\nTo use the compiler and the transpiler now you will need an API Key. If you already have one, you should replace the string in the next cell.\nAPI_KEY = \"PASTE_YOUR_KEY_HERE\"\n!mkdir -p .ivy\n!echo -n $API_KEY &gt; .ivy/key.pem\nFirstly, let‚Äôs import the dependencies and define a torch function.\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\nBy using ivy.unify(), you can convert any code from any framework into Ivy code, which as we have already seen, can be executed using any framework as the backend.\nLet‚Äôs unify the function!\nnormalize = ivy.unify(normalize, source=\"torch\")\nAnd that‚Äôs it! The normalize function can now be used with any ML framework. It‚Äôs as simple as that!\nSo, let‚Äôs give it a try!\n# import the frameworks\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow as tf\n# create random numpy arrays for testing\nx = np.random.uniform(size=10).astype(np.float32)\nivy.set_backend(\"numpy\")\nprint(normalize(x))\n\n# jax\nx_ = jnp.array(x)\nivy.set_backend(\"jax\")\nprint(normalize(x_))\n\n# tensorflow\nx_ = tf.constant(x)\nivy.set_backend(\"tensorflow\")\nprint(normalize(x_))\n\n# torch\nx_ = torch.tensor(x)\nivy.set_backend(\"torch\")\nprint(normalize(x_))\n\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nWe can see that the new normalize function can operate with any ML framework. ivy.unify converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks."
  },
  {
    "objectID": "learn_the_basics/02_unify_code.html#round-up",
    "href": "learn_the_basics/02_unify_code.html#round-up",
    "title": "Unify code",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now unify ML code! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to make our unified Ivy code run much more efficiently! ‚ö°"
  },
  {
    "objectID": "wip/resnet_18.html",
    "href": "wip/resnet_18.html",
    "title": "Resnet 18",
    "section": "",
    "text": "ToDo: description\n\n \n\nLets first install Ivy (pip install ivy-core) and compatable version of matplotlib to be able to see generated graph\n\n!pip install ivy-core\n!pip install matplotlib==3.5.2\n!pip install networkx==2.8.4\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting ivy-core\n  Downloading ivy_core-1.1.10-py3-none-any.whl (228 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 228.8/228.8 KB 19.0 MB/s eta 0:00:00\nCollecting einops\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 41.6/41.6 KB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ivy-core) (5.4.8)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from ivy-core) (2.1.1)\nCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ivy-core) (1.21.6)\nBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... done\n  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=241af6b4a51197474b0da3ee7bfa32d847756c8f0d93b51448655d6458312714\n  Stored in directory: /root/.cache/pip/wheels/b9/b1/68/cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3, einops, ivy-core\nSuccessfully installed einops-0.6.0 ivy-core-1.1.10 nvidia-ml-py3-7.352.0\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting matplotlib==3.5.2\n  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 11.3/11.3 MB 82.1 MB/s eta 0:00:00\nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (0.11.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (21.3)\nCollecting fonttools&gt;=4.22.0\n  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 965.4/965.4 KB 69.7 MB/s eta 0:00:00\nRequirement already satisfied: pillow&gt;=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (7.1.2)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (3.0.9)\nRequirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (1.21.6)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (1.4.4)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil&gt;=2.7-&gt;matplotlib==3.5.2) (1.15.0)\nInstalling collected packages: fonttools, matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.2.2\n    Uninstalling matplotlib-3.2.2:\n      Successfully uninstalled matplotlib-3.2.2\nSuccessfully installed fonttools-4.38.0 matplotlib-3.5.2\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting networkx==2.8.4\n  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.0/2.0 MB 65.0 MB/s eta 0:00:00\nInstalling collected packages: networkx\n  Attempting uninstall: networkx\n    Found existing installation: networkx 2.8.8\n    Uninstalling networkx-2.8.8:\n      Successfully uninstalled networkx-2.8.8\nSuccessfully installed networkx-2.8.4\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\nimport ivy\nimport numpy as np\nimport timeit\nimport torch\nimport urllib\nfrom ivy import compile_graph\nfrom ivy import show_graph\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\nHere we load resnet18 pytorch model. Copy model to chosen device and convert the model into ivy (using ivy.to_ivy_module). Then, we compile the model (using compile_graph) and check if it is working as expected.\n\nivy.set_framework(\"torch\")\n# dev = 'cpu'\ndev = 'gpu:0'\nbatch_size = 1\nimage_dims = [64,64]\n\ntry:\n    model = torch.hub.load(\n        \"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True\n    )\nexcept urllib.error.URLError:\n  pass\n\nnet = ivy.to_ivy_module(model.to('cuda:0' if dev == 'gpu:0' else 'cpu'))\n\nx0 = ivy.random_uniform(\n    low=0.0,\n    high=1.0,\n    shape=[batch_size] + [3] + image_dims,\n    dev_str=dev,\n)\nx1 = ivy.random_uniform(\n    low=0.0,\n    high=1.0,\n    shape=[batch_size] + [3] + image_dims,\n    dev_str=dev,\n)\n\ncomp_network, graph = compile_graph(net, x0, return_graph=True, time_chronological=False)\n\nshow_graph(\n    net,\n    x0,\n    save_to_disk=True,\n    fname='resnet18'\n)\n\n\nret0_nc = net(x0)\nret1_nc = net(x1)\n\nassert not np.allclose(ivy.to_numpy(ret0_nc), ivy.to_numpy(ret1_nc))\n\nret0_c = comp_network(x0)\nret1_c = comp_network(x1)\n\nassert not np.allclose(ivy.to_numpy(ret0_c), ivy.to_numpy(ret1_c))\nassert np.allclose(ivy.to_numpy(ret0_nc), ivy.to_numpy(ret0_c))\nassert np.allclose(ivy.to_numpy(ret1_nc), ivy.to_numpy(ret1_c))\n\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n  warnings.warn(\n\n\nLet‚Äôs compare inference speed of original network and compiled network. In general compiled network is faster due to graph compiler optimizations.\n\nN = 1000\nres = timeit.timeit(lambda: net(x0), number=N)\nprint(res/N,'ms')\n\n0.004645566477999864 ms\n\n\n\nN = 1000\nres = timeit.timeit(lambda: comp_network(x0), number=N)\nprint(res/N,'ms')\n\n0.0044566806820000695 ms\n\n\nHere is how graph of the network looks like.\n\nImage('resnet18.png')\n\n\n\n\nBy passing return_graph=True parameter to compile_graph you can obtain graph object. It has _all_functions attribute that stores all the functions in order of their execution.\nIf time_chronological=True the order of functions will the same as in the definition of network. If time_chronological=False Ivy‚Äôs graph compiler will try to find optimal order for better inference speed without affecting the accuracy.\n\ncomp_network, graph = compile_graph(net, x0, return_graph=True, time_chronological=True)\nprint([fn.__name__ for fn in graph._all_functions])\n\n/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n  warnings.warn(\n\n\n['conv2d', 'batch_norm', 'relu', 'max_pool2d', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'adaptive_avg_pool2d', 'flatten', 'linear']"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html",
    "href": "wip/deepmind_perceiver_io.html",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "",
    "text": "In this example, we will transpile DeepMind‚Äôs original implementation of the PerceiverIO module written in JAX and build a classifier in a fully PyTorch pipeline!"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#table-of-contents",
    "href": "wip/deepmind_perceiver_io.html#table-of-contents",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Table of Contents",
    "text": "Table of Contents\n\nDefining the model\n\nSome helper functions\n\nTranspiling the model\nDataset download\nPyTorch pipeline\n\nDataLoader\nTraining\nTesting\n\n\nLet‚Äôs install Ivy &gt; if you are using Google Colab, make sure to restart your runtime before proceeding üòâ\n\n# install the latest Ivy version for this purpose\n!pip install git+https://github.com/unifyai/ivy.git@master\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/unifyai/ivy.git@master\n  Cloning https://github.com/unifyai/ivy.git (to revision master) to /tmp/pip-req-build-tabqrujw\n  Running command git clone --filter=blob:none --quiet https://github.com/unifyai/ivy.git /tmp/pip-req-build-tabqrujw\n  Resolved https://github.com/unifyai/ivy.git to commit f3be3702c9fab1c9fa97c743813a4bdb39525705\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ivy-core==1.1.9) (1.22.4)\nCollecting einops==0.4.1\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nCollecting psutil==5.9.1\n  Downloading psutil-5.9.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 281.1/281.1 kB 8.5 MB/s eta 0:00:00\nCollecting termcolor==1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... done\nCollecting colorama==0.4.5\n  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\nCollecting packaging==21.3\n  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 40.8/40.8 kB 4.8 MB/s eta 0:00:00\nCollecting nvidia-ml-py&lt;=11.495.46\n  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\nCollecting diskcache\n  Downloading diskcache-5.5.1-py3-none-any.whl (45 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 45.2/45.2 kB 4.9 MB/s eta 0:00:00\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging==21.3-&gt;ivy-core==1.1.9) (3.0.9)\nBuilding wheels for collected packages: ivy-core, termcolor\n  Building wheel for ivy-core (setup.py) ... done\n  Created wheel for ivy-core: filename=ivy_core-1.1.9-py3-none-any.whl size=1338326 sha256=e5c4205c80116b781373daf4502d61881235c5e3eb0d55096ab07dcc6eb66bec\n  Stored in directory: /tmp/pip-ephem-wheel-cache-njrc_e6b/wheels/07/46/2e/ae2d7c5ce8708e605368a33e08d57d1de8e107e3db157c3063\n  Building wheel for termcolor (setup.py) ... done\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=a8cde63eca203d3bd7f900fa32f44dbd038476606a3836de14caf2b0a5ff7460\n  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\nSuccessfully built ivy-core termcolor\nInstalling collected packages: termcolor, nvidia-ml-py, einops, psutil, packaging, diskcache, colorama, ivy-core\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.2.0\n    Uninstalling termcolor-2.2.0:\n      Successfully uninstalled termcolor-2.2.0\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.4\n    Uninstalling psutil-5.9.4:\n      Successfully uninstalled psutil-5.9.4\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.0\n    Uninstalling packaging-23.0:\n      Successfully uninstalled packaging-23.0\nSuccessfully installed colorama-0.4.5 diskcache-5.5.1 einops-0.4.1 ivy-core-1.1.9 nvidia-ml-py-11.495.46 packaging-21.3 psutil-5.9.1 termcolor-1.1.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nFirst of all, let‚Äôs import Ivy üöÄ\n\nimport ivy"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#defining-the-model",
    "href": "wip/deepmind_perceiver_io.html#defining-the-model",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Defining the model ",
    "text": "Defining the model \n(Psst! The PerceiverIO code we will use comes from the original repo)\nIn order to use an image classifier on top of the PerceiverIO model, we must import some things first from the DeepMind repo:\n\nimport torch\n# utils\nimport os\nimport cv2\nimport pickle\nimport imageio\nimport urllib.request\n# JAX imports\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nrng = jax.random.PRNGKey(23)\n\nWARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\n\n!pip install dm-haiku\n!pip install einops\n\n!mkdir /content/perceiver\n!touch /content/perceiver/__init__.py\n!wget -O /content/perceiver/io_processors.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n!wget -O /content/perceiver/perceiver.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n!wget -O /content/perceiver/position_encoding.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: dm-haiku in /usr/local/lib/python3.9/dist-packages (0.0.9)\nRequirement already satisfied: absl-py&gt;=0.7.1 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.4.0)\nRequirement already satisfied: jmp&gt;=0.0.2 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (0.0.4)\nRequirement already satisfied: tabulate&gt;=0.8.9 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (0.8.10)\nRequirement already satisfied: numpy&gt;=1.18.0 in /usr/local/lib/python3.9/dist-packages (from dm-haiku) (1.22.4)\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting einops\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 41.6/41.6 kB 2.0 MB/s eta 0:00:00\nInstalling collected packages: einops\nSuccessfully installed einops-0.6.0\n--2023-04-15 11:43:56--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 29359 (29K) [text/plain]\nSaving to: ‚Äò/content/perceiver/io_processors.py‚Äô\n\n/content/perceiver/ 100%[===================&gt;]  28.67K  --.-KB/s    in 0.002s  \n\n2023-04-15 11:43:56 (11.2 MB/s) - ‚Äò/content/perceiver/io_processors.py‚Äô saved [29359/29359]\n\n--2023-04-15 11:43:56--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 30179 (29K) [text/plain]\nSaving to: ‚Äò/content/perceiver/perceiver.py‚Äô\n\n/content/perceiver/ 100%[===================&gt;]  29.47K  --.-KB/s    in 0.002s  \n\n2023-04-15 11:43:56 (12.5 MB/s) - ‚Äò/content/perceiver/perceiver.py‚Äô saved [30179/30179]\n\n--2023-04-15 11:43:56--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8107 (7.9K) [text/plain]\nSaving to: ‚Äò/content/perceiver/position_encoding.py‚Äô\n\n/content/perceiver/ 100%[===================&gt;]   7.92K  --.-KB/s    in 0s      \n\n2023-04-15 11:43:56 (61.9 MB/s) - ‚Äò/content/perceiver/position_encoding.py‚Äô saved [8107/8107]\n\n\n\n##Imports\n\nimport functools\nimport itertools\nimport pickle\n\nimport cv2\nimport haiku as hk\nimport imageio\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom perceiver import perceiver, io_processors"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#model-construction",
    "href": "wip/deepmind_perceiver_io.html#model-construction",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Model construction",
    "text": "Model construction\nWe are now building an Haiku module that will be used as a backbone for our classification model. It is composed by a preprocessor that handles our input and an encoder that will output our feature vectors! üòÄ\n\nclass PerceiverBackbone(hk.Module):\n    \"\"\"Perceiver image preprocessor + encoder\"\"\"\n\n    def __init__(\n        self,\n        encoder,\n        input_preprocessor,\n        name=\"perceiver\",\n    ):\n        super().__init__(name=name)\n\n        # Feature parameters:\n        self._input_preprocessor = input_preprocessor\n        self._encoder = encoder\n\n    def __call__(\n        self,\n        inputs,\n        *,\n        is_training,\n        pos=None,\n        input_mask=None,\n    ):\n        network_input_is_1d = self._encoder._input_is_1d\n        inputs, _, _ = self._input_preprocessor(\n            inputs,\n            pos=pos,\n            is_training=is_training,\n            network_input_is_1d=network_input_is_1d,\n        )\n\n        # Get the queries for encoder and decoder cross-attends.\n        encoder_query = self._encoder.latents(inputs)\n\n        # Run the network forward:\n        z = self._encoder(\n            inputs, encoder_query, is_training=is_training, input_mask=input_mask\n        )\n\n        return z\n\nIn our case, the preprocessor is an ImagePreprocessor since we are dealing with images but you can change it based on your needs! üòâ\n\nIMAGE_SIZE = (224, 224)\n\nfourier_pos_configs = dict(\n    input_preprocessor=dict(\n        position_encoding_type=\"fourier\",\n        fourier_position_encoding_kwargs=dict(\n            concat_pos=True, max_resolution=(224, 224), num_bands=64, sine_only=False\n        ),\n        prep_type=\"pixels\",\n        spatial_downsample=1,\n    ),\n    encoder=dict(\n        cross_attend_widening_factor=1,\n        cross_attention_shape_for_attn=\"kv\",\n        dropout_prob=0,\n        num_blocks=8,\n        num_cross_attend_heads=1,\n        num_self_attend_heads=8,\n        num_self_attends_per_block=6,\n        num_z_channels=1024,\n        self_attend_widening_factor=1,\n        use_query_residual=True,\n        z_index_dim=512,\n        z_pos_enc_init_scale=0.02,\n    ),\n)\n\ndef perceiver_backbone(images):\n    config = fourier_pos_configs\n    input_preprocessor = io_processors.ImagePreprocessor(**config[\"input_preprocessor\"])\n    encoder = perceiver.PerceiverEncoder(**config[\"encoder\"])\n    model = PerceiverBackbone(encoder=encoder, input_preprocessor=input_preprocessor)\n    logits = model(images, is_training=False)\n    return logits\n\nperceiver_backbone = hk.transform(perceiver_backbone)\n\nAt the end, we will get our hk.Transformed object that is (almost) ready to be transpiled üòé\n\ntype(perceiver_backbone)\n\nhaiku._src.transform.Transformed\n\n\nWe also need to download the original parameters published by DeepMind if we want to save some computing üòÖ\nThankfully Ivy will transform those for us as well üòâ\n\nurl = 'https://storage.googleapis.com/perceiver_io/imagenet_fourier_position_encoding.pystate'\n!wget -O imagenet_checkpoint.pystate $url\n\nwith open('imagenet_checkpoint.pystate', 'rb') as f:\n  ckpt = pickle.loads(f.read())\n\nparams = ckpt['params']\n\n--2023-04-09 13:39:56--  https://storage.googleapis.com/perceiver_io/imagenet_fourier_position_encoding.pystate\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.217.128, 74.125.31.128, 108.177.12.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.217.128|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 193776248 (185M) [application/octet-stream]\nSaving to: ‚Äòimagenet_checkpoint.pystate‚Äô\n\nimagenet_checkpoint 100%[===================&gt;] 184.80M   144MB/s    in 1.3s    \n\n2023-04-09 13:39:57 (144 MB/s) - ‚Äòimagenet_checkpoint.pystate‚Äô saved [193776248/193776248]"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#some-helper-functions",
    "href": "wip/deepmind_perceiver_io.html#some-helper-functions",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Some helper functions ",
    "text": "Some helper functions \nIn order to correctly use the classifier, we will need to use some preprocessing functions (also from the DeepMind repo)\n\nMEAN_RGB = (0.485 * 255, 0.456 * 255, 0.406 * 255)\nSTDDEV_RGB = (0.229 * 255, 0.224 * 255, 0.225 * 255)\n\n\ndef normalize(im):\n    return (im - np.array(MEAN_RGB)) / np.array(STDDEV_RGB)\n\ndef denormalize(im):\n    return im * np.array(STDDEV_RGB) + np.array(MEAN_RGB)\n\n\ndef resize_and_center_crop(image):\n    \"\"\"Crops to center of image with padding then scales.\"\"\"\n    shape = image.shape\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = (\n        (224 / (224 + 32)) * np.minimum(image_height, image_width).astype(np.float32)\n    ).astype(np.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = [\n        offset_height,\n        offset_width,\n        padded_center_crop_size,\n        padded_center_crop_size,\n    ]\n    image = image[\n        crop_window[0] : crop_window[0] + crop_window[2],\n        crop_window[1] : crop_window[1] + crop_window[3],\n    ]\n    return cv2.resize(image, (224, 224), interpolation=cv2.INTER_CUBIC)"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#dataloader",
    "href": "wip/deepmind_perceiver_io.html#dataloader",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "DataLoader ",
    "text": "DataLoader \n\nfrom torch.utils.data import Dataset\n\n# Define a custom PyTorch dataset class that wraps a HugginFace\n# dataset with a torch Dataset object\nclass CustomDataset(Dataset):\n    def __init__(self, dataset, transform=None):\n        self.dataset = dataset\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        item = self.dataset[idx]\n        image = item['image']\n        label = item['style']\n        if self.transform:\n            image = self.transform(image)\n        return {'image': image, 'label': label}\n\n\nfrom torchvision.transforms import transforms\nfrom torch.utils.data import DataLoader, random_split\n\n# Define the transformations to be applied to each image\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),  # Resize the image to 224x224 pixels\n    transforms.ToTensor()  # Convert the image to a PyTorch tensor\n])\n\ndataset = CustomDataset(dataset, transform=transform)\n\ntrain_size = int(0.7 * len(dataset))\nval_size = int(0.15 * len(dataset))\ntest_size = len(dataset) - train_size - val_size\ndataset_train, dataset_val, dataset_test = random_split(dataset, [train_size, val_size, test_size])\n\n\ndataloader_train = DataLoader(dataset_train, batch_size=1)\ndataloader_val = DataLoader(dataset_val, batch_size=1)\ndataloader_test = DataLoader(dataset_test, batch_size=1)\n\n\nimport matplotlib.pyplot as plt\n\nbatch = next(iter(dataloader_train))\ntrain_features = batch['image']\ntrain_labels = batch['label']\nprint(f\"Feature batch shape: {train_features.size()}\")\nprint(f\"Labels batch shape: {train_labels.size()}\")\nimg = train_features[0].squeeze()\nlabel = train_labels[0]\nimg = img.permute(1, 2, 0)\nprint(img.shape)\n\nplt.imshow(img)\nplt.show()\nprint(f\"Label: {LABELS[int(label)]}\")\n\nFeature batch shape: torch.Size([1, 3, 224, 224])\nLabels batch shape: torch.Size([1])\ntorch.Size([224, 224, 3])\nLabel: Northern_Renaissance"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#training",
    "href": "wip/deepmind_perceiver_io.html#training",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Training ",
    "text": "Training \n\nclassifier = PerceiverIOClassifier(backbone=torch_perceiver_backbone, num_classes=NUM_CLASSES)\n\n\nloss_fn = torch.nn.CrossEntropyLoss()\n\n\noptimizer = torch.optim.SGD(classifier.parameters(), lr=0.001, momentum=0.9)\n\n\ndef train_step():\n    running_loss = 0.\n    last_loss = 0.\n    \n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, data in enumerate(dataloader_train):\n        # Every data instance is an input + label pair\n        inputs, labels = data[\"image\"], data[\"label\"]\n\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = classifier(inputs)\n\n        # Compute the loss and its gradients\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n\n        # Adjust learning weights\n        optimizer.step()\n\n        # Gather data and report\n        running_loss += loss.item()\n        if i % 1000 == 999:\n            last_loss = running_loss / 1000 # loss per batch\n            print('  batch {} loss: {}'.format(i + 1, last_loss))\n            running_loss = 0.\n\n    return last_loss\n\n\nepoch_number = 0\n\nEPOCHS = 5\n\nbest_vloss = 1_000_000.\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}:'.format(epoch_number + 1))\n\n    # Make sure gradient tracking is on, and do a pass over the data\n    classifier.train(True)\n    avg_loss = train_step()\n\n    # We don't need gradients on to do reporting\n    classifier.train(False)\n\n    running_vloss = 0.0\n    for i, vdata in enumerate(dataloader_val):\n        vinputs, vlabels = vdata[\"image\"], vdata[\"label\"]\n        voutputs = classifier(vinputs)\n        vloss = loss_fn(voutputs, vlabels)\n        running_vloss += vloss\n\n    avg_vloss = running_vloss / (i + 1)\n    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n\n    # Track best performance, and save the model's state\n    if avg_vloss &lt; best_vloss:\n        best_vloss = avg_vloss\n        model_path = 'model_{}'.format(epoch_number)\n        torch.save(classifier.state_dict(), model_path)\n\n    epoch_number += 1"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html#testing",
    "href": "wip/deepmind_perceiver_io.html#testing",
    "title": "Demo: Transpiling DeepMind‚Äôs PerceiverIO",
    "section": "Testing ",
    "text": "Testing \n\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in dataloader_test:\n        images, labels = data[\"image\"], data[\"label\"]\n        # calculate outputs by running images through the network\n        outputs = classifier(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html",
    "href": "wip/1_the_basics/1_1_framework_selection.html",
    "title": "1.1: Framework Selection",
    "section": "",
    "text": "The source and target frameworks for ivy.unify, ivy.compile and ivy.transpile can be: (a) inferred from the arguments and/or inspection of the function, (b) specified globally or (c) specified locally. All examples in the Building Blocks either infer the source and target frameworks or specify them globally (via ivy.set_backend). We‚Äôll explore these various options, and also explore which modes take priority. For these examples, all functions are called eagerly. Please go through the Lazy vs Eager notebook if you haven‚Äôt already."
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html#unify",
    "href": "wip/1_the_basics/1_1_framework_selection.html#unify",
    "title": "1.1: Framework Selection",
    "section": "Unify",
    "text": "Unify\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet‚Äôs also create the dummy data as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nThis time, let‚Äôs assume that our target framework is jax:\n\nimport jax.numpy as jnp\n\nx = jnp.array(x)\nmean = jnp.array(mean)\nstd = jnp.array(std)\n\nIn the example below, the source framework of torch is inferred from the function normalize.\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\n\nAs mentioned in the Unify notebook, ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic Ivy implementation, which is compatible with all frameworks.\nFor some functions, this would not be possible. Consider the example below:\n\ndef normalize_via_operators(x, mean, std):\n    return (x - mean) / std\n\nThere is no way to determine the source framework from this function via the inspection module. This code uses built-in operators only, which are compatible with all ML frameworks. You might therefore think ‚Äúthis is already unified‚Äù, but that‚Äôs not true. Every ML framework has its own unique rules for broadcasting shapes and data types for elementwise functions, which must all be taken into account when converting code to ivy.\nRather than inferring the framework, the framework can be specified locally as follows:\n\nnorm = ivy.unify(normalize_via_operators, args=(x, mean, std), from=\"torch\")\n\nNote that in all of the examples above, the arguments are in fact jax arrays. During function tracing, the jax arrays are converted to torch tensors automatically."
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html#compile",
    "href": "wip/1_the_basics/1_1_framework_selection.html#compile",
    "title": "1.1: Framework Selection",
    "section": "Compile",
    "text": "Compile\nIn the example below, the target framework of jax is inferred from the arguments.\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nHowever, if the Ivy function norm was purely generative (not consuming any arrays in the input), then this would not be possible. In such cases, we could set the target framework globally like so. If the type of the arguments conflicts with the globally set backend, then an error will be thrown.\n\nivy.set_backend(\"jax\")\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nFinally, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm_comp = ivy.compile(norm, args=(x, mean, std), to=\"jax\") # doesn't matter, jax specified locally"
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html#transpile",
    "href": "wip/1_the_basics/1_1_framework_selection.html#transpile",
    "title": "1.1: Framework Selection",
    "section": "Transpile",
    "text": "Transpile\nAll consideration for both ivy.unify and ivy.compile are combined for ivy.transpile, which is effectively shorthand for the combination of these two functions (as explained in the Transpile section).\nIn the example below, the source framework of torch is inferred from the function normalize, and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std))\n\nIn the example below, the source framework is specified locally (would be necessary if transpiling normalize_via_operators for example) and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nIn the example below, the source framework is specified locally and the target framework of jax is specified globally. This might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"jax\")\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nAs with ivy.compile, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\nIn the example below, the source framework is specified locally and the target framework of jax is also specified locally. Again, this might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\", to=\"jax\") # doesn't matter, jax specified locally"
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html#round-up",
    "href": "wip/1_the_basics/1_1_framework_selection.html#round-up",
    "title": "1.1: Framework Selection",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you now know the difference between inferring, locally specifying, and globally specifying source and target frameworks for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be exploring how these three functions can all be called as function decorators!"
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html",
    "title": "1.2: As a Decorator",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be called either as a function decorator or as a standalone function. All examples in the Building Blocks section and all previous examples in The Basics are called as standalone functions. In this section, we‚Äôll see how they can each be instead called as function decorators."
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html#unify",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html#unify",
    "title": "1.2: As a Decorator",
    "section": "Unify",
    "text": "Unify\nFirstly, let‚Äôs create the dummy numpy arrays as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the ivy.unify function is called as a decorator.\n\nimport ivy\nimport torch\n\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.unify(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally. This argument can be passed when ivy.unify is used as a decorator."
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html#compile",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html#compile",
    "title": "1.2: As a Decorator",
    "section": "Compile",
    "text": "Compile\nIn the example below, the ivy.compile function is also called as a decorator.\n\n@ivy.compile\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLikewise, the function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.compile(args=(x, mean, std))\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as to for specifying the target framework locally. This argument can be passed when ivy.compile is used as a decorator."
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html#transpile",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html#transpile",
    "title": "1.2: As a Decorator",
    "section": "Transpile",
    "text": "Transpile\nIn the example below, the ivy.transpile function is called as a decorator.\n\n@ivy.transpile\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.transpile(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally, and to for specifying the target framework locally. These arguments can be passed when ivy.transpile is used as a decorator."
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html#round-up",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html#round-up",
    "title": "1.2: As a Decorator",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you now know how ivy.unify, ivy.compile and ivy.transpile can all be used as function decorators! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be exploring the difference between dynamic vs static computation graphs!"
  },
  {
    "objectID": "wip/1_the_basics/1_3_dynamic_vs_static.html",
    "href": "wip/1_the_basics/1_3_dynamic_vs_static.html",
    "title": "1.3: Dynamic vs Static",
    "section": "",
    "text": "The functions ivy.unify, ivy.compile and ivy.transpile can all be executed either in dynamic mode or static mode. In this demo, we explore how this mode is set, and what implications this has."
  },
  {
    "objectID": "wip/1_the_basics/1_3_dynamic_vs_static.html#dynamic",
    "href": "wip/1_the_basics/1_3_dynamic_vs_static.html#dynamic",
    "title": "1.3: Dynamic vs Static",
    "section": "Dynamic",
    "text": "Dynamic\nDynamic mode means that Python dynamic control flow is included in the extracted computation graph. For example, if statements, for loops, while loops etc. would all be included if dynamic mode is set to True."
  },
  {
    "objectID": "wip/1_the_basics/1_3_dynamic_vs_static.html#static",
    "href": "wip/1_the_basics/1_3_dynamic_vs_static.html#static",
    "title": "1.3: Dynamic vs Static",
    "section": "Static",
    "text": "Static\nStatic mode means the opposite, where Python dynamic control flow is not included in the extracted computation graph. if statements, for loops, while loops etc. would all be excluded if dynamic mode is set to False."
  },
  {
    "objectID": "wip/1_the_basics/1_3_dynamic_vs_static.html#todo-explain-via-examples-why-dynamic-mode-is-set-to-true-by-default-when-transpiling-to-and-from-numpy-and-torch-but-set-to-false-by-default-when-transpiling-to-and-from-tensorflow-and-jax.",
    "href": "wip/1_the_basics/1_3_dynamic_vs_static.html#todo-explain-via-examples-why-dynamic-mode-is-set-to-true-by-default-when-transpiling-to-and-from-numpy-and-torch-but-set-to-false-by-default-when-transpiling-to-and-from-tensorflow-and-jax.",
    "title": "1.3: Dynamic vs Static",
    "section": "ToDo: explain via examples why dynamic mode is set to True by default when transpiling to and from numpy and torch, but set to False by default when transpiling to and from tensorflow and jax.",
    "text": "ToDo: explain via examples why dynamic mode is set to True by default when transpiling to and from numpy and torch, but set to False by default when transpiling to and from tensorflow and jax."
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html",
    "title": "1.0: Lazy vs Eager",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be performed either eagerly or lazily. All examples in the Building Blocks section are performed lazily, which means that the unification, compilation, or transpilation process actually occurs during the first call of the returned function. This is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the ivy.unify, ivy.compile or ivy.transpile call itself, in which case the process is performed eagerly. We show some simple examples for each case below."
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html#unify",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html#unify",
    "title": "1.0: Lazy vs Eager",
    "section": "Unify",
    "text": "Unify\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet‚Äôs also create the dummy numpy arrays as before:\n\n# import NumPy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the function is unified lazily, which means the first function call will execute slowly, as this is when the unification process actually occurs.\n\nnorm = ivy.unify(normalize)\nnorm(x, mean, std) # slow, lazy unification\nnorm(x, mean, std) # fast, unified on previous call\n\nHowever, in the following example the unification occurs eagerly, and both function calls will be fast:\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\nnorm(x, mean, std) # fast, unified at ivy.unify\nnorm(x, mean, std) # fast, unified at ivy.unify"
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html#compile",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html#compile",
    "title": "1.0: Lazy vs Eager",
    "section": "Compile",
    "text": "Compile\nThe same is true for compiling. In the example below, the function is compiled lazily, which means the first function call will execute slowly, as this is when the compilation process actually occurs.\n\nnorm_comp = ivy.compile(norm)\nnorm_comp(x, mean, std) # slow, lazy compilation\nnorm_comp(x, mean, std) # fast, compiled on previous call\n\nHowever, in the following example the compilation occurs eagerly, and both function calls will be fast:\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile"
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html#transpile",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html#transpile",
    "title": "1.0: Lazy vs Eager",
    "section": "Transpile",
    "text": "Transpile\nThe same is true for transpiling. In the example below, the function is transpiled lazily, which means the first function call will execute slowly, as this is when the transpilation process actually occurs.\n\nnorm_trans = ivy.transpile(normalize)\nnorm_trans(x, mean, std) # slow, lazy transpilation\nnorm_trans(x, mean, std) # fast, transpiled on previous call\n\nHowever, in the following example the transpilation occurs eagerly, and both function calls will be fast:\n\nnorm_trans = ivy.transpile(normalize, args=(x, mean, std))\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile"
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html#round-up",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html#round-up",
    "title": "1.0: Lazy vs Eager",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you now know the difference between lazy vs eager execution for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how the frameworks are selected, either inferred from the inputs and the function, specified globally, or specified locally. We‚Äôll also learn what the implications are for each of these approaches!"
  },
  {
    "objectID": "wip/deepmind_perceiverio.html",
    "href": "wip/deepmind_perceiverio.html",
    "title": "Deepmind PerceiverIO on GPU",
    "section": "",
    "text": "This notebook presents a demonstration of how to run the PercieverIO model on GPU using any of the following backends currently supported by ivy (numpy, torch, tensorflow and JAX). To find out more about ivy please feel free to checkout the ivy repo https://github.com/unifyai/ivy as well as the docs https://lets-unify.ai/ivy/index.html. Contributions are highly welcomed and you can interact with our community as well on discord https://discord.com/invite/G4aR9Q7DTN.\n\n\nNOTE: After running the cell below, you‚Äôll need to restart the runtime for the newly installed kernel to work.\n\n# Step1\n!printf \"#Step1 start Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n!printf \"10_wfp1U4rMzc20eiGNrdQa9V2S9ByJwV\" &gt; ./FILE_ID ;\\\n printf \"wget  -O `cat ./FILE_ID`.tar.gz \\\"https://drive.google.com/uc?export=download&id=`cat ./FILE_ID`&confirm=t&\"|tee  ./FILE_ID_WGET_CMD  ;\\\n  printf \"`wget -q \"https://drive.google.com/uc?export=download&id=\\`cat ./FILE_ID\\`\" -O - | perl -pe 's/\\r*\\n//g' | perl -pe 's/^.*(uuid\\=[^\\\"]+)\\\".*$/${1}/g'`\"|tee -a ./FILE_ID_WGET_CMD  ;\\\n    printf \"&uc-download-link=Download anyway\\\"\" |tee -a ./FILE_ID_WGET_CMD ;\\\n      /bin/bash ./FILE_ID_WGET_CMD\n!printf \"#Step1 End Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n\n# Step2 Install python3.8 interpreter and python3-pip\n!printf \"Step2 Start python3.8 interpreter and install python3-pip\\n\"\n!sudo apt-get install -y python3.8\n!sudo apt-get install -yf python3-pip\n!printf \"Step2 End install python3.8 interpreter and python3-pip\\n\"\n\n# Step3 \n!printf \"#Step3 Start Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n!mkdir ./dist-packages\n!tar xvzf `cat ./FILE_ID`.tar.gz -C ./dist-packages\n!rm `cat ./FILE_ID`.tar.gz\n!printf \"#Step3 End Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n\n# Step4 \n!printf \"# Step4 Start symlink python3.7 google* package to unzipped python3.8 package directory\\n\"\n!rm -fr /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!ls -d /usr/local/lib/python3.7/dist-packages/google* \\\n  | perl -pe 's/^(.+)$/sudo ln -sf ${1} \\/content\\/dist-packages\\/usr\\/local\\/lib\\/python3.8\\/dist-packages\\//g' \\\n    |/bin/bash -\n!ls -la /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!printf \"# Step4 End symlink the python3.7 google* package to the unzipped python3.8 package directory\\n\"\n\n# Step5\n!printf \"# Step5 Start and replace the unzipped python3.8 package directory with a regular python3.8 package directory with a symlink\\n\"\n!rm -fr /usr/local/lib/python3.8/dist-packages\n!sudo ln -s /content/dist-packages/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!printf \"# Step5 End And symlink the unzipped python3.8 package directory to the regular python3.8 package directory\\n\"\n\n# Step6\n!printf \"# Step6 Start change python interpreter to 3.8\\n\"\n!printf \"python3.8 restart step 4\\n\"\n!sudo ln -sf `which python3.8` /etc/alternatives/python3\n!python --version\n!printf \"# Step6 End Change python interpreter to 3.8\\n\"\n\n# Run ipkykernel with Step7 3.8 python, name it \"engbjapanpython3.8\" and install the runtime separately\n!printf \"#Step7 Start Name engbjapanpython3.8 and start runtime (Python 3.8) ipykernel\\n\"\n!sudo python -m ipykernel install --name \"engbjapanpython3.8\" --user\n!printf \"#Step7 End Start the runtime (Python 3.8) ipykernel named engbjapanpython3.8\\n\"\n!printf \"When everything is finished, please execute ``Change runtime type and reconnect''\\n\"\n\n\nimport sys\nprint(\"User Current Version:-\", sys.version)\n\nUser Current Version:- 3.8.15 (default, Oct 12 2022, 19:14:39) \n[GCC 7.5.0]\n\n\n\n\n\n\n!git lfs clone --depth 1 https://github.com/unifyai/models.git\n\n\n!git clone --depth 1 https://github.com/unifyai/ivy.git \n\n\n\n\n\n!pip install models/ --upgrade\n\n\n!pip install ivy/\n\n\n# install the optional requirements to use JAX, tensorflow and torch backends\n!pip install -r ivy/requirements/optional.txt\n\n\n# Install jaxlib with with the corresponding CUDA version.\n!pip install  https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14+cuda11.cudnn805-cp38-none-manylinux2014_x86_64.whl"
  },
  {
    "objectID": "wip/deepmind_perceiverio.html#introduction",
    "href": "wip/deepmind_perceiverio.html#introduction",
    "title": "Deepmind PerceiverIO on GPU",
    "section": "",
    "text": "This notebook presents a demonstration of how to run the PercieverIO model on GPU using any of the following backends currently supported by ivy (numpy, torch, tensorflow and JAX). To find out more about ivy please feel free to checkout the ivy repo https://github.com/unifyai/ivy as well as the docs https://lets-unify.ai/ivy/index.html. Contributions are highly welcomed and you can interact with our community as well on discord https://discord.com/invite/G4aR9Q7DTN.\n\n\nNOTE: After running the cell below, you‚Äôll need to restart the runtime for the newly installed kernel to work.\n\n# Step1\n!printf \"#Step1 start Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n!printf \"10_wfp1U4rMzc20eiGNrdQa9V2S9ByJwV\" &gt; ./FILE_ID ;\\\n printf \"wget  -O `cat ./FILE_ID`.tar.gz \\\"https://drive.google.com/uc?export=download&id=`cat ./FILE_ID`&confirm=t&\"|tee  ./FILE_ID_WGET_CMD  ;\\\n  printf \"`wget -q \"https://drive.google.com/uc?export=download&id=\\`cat ./FILE_ID\\`\" -O - | perl -pe 's/\\r*\\n//g' | perl -pe 's/^.*(uuid\\=[^\\\"]+)\\\".*$/${1}/g'`\"|tee -a ./FILE_ID_WGET_CMD  ;\\\n    printf \"&uc-download-link=Download anyway\\\"\" |tee -a ./FILE_ID_WGET_CMD ;\\\n      /bin/bash ./FILE_ID_WGET_CMD\n!printf \"#Step1 End Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n\n# Step2 Install python3.8 interpreter and python3-pip\n!printf \"Step2 Start python3.8 interpreter and install python3-pip\\n\"\n!sudo apt-get install -y python3.8\n!sudo apt-get install -yf python3-pip\n!printf \"Step2 End install python3.8 interpreter and python3-pip\\n\"\n\n# Step3 \n!printf \"#Step3 Start Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n!mkdir ./dist-packages\n!tar xvzf `cat ./FILE_ID`.tar.gz -C ./dist-packages\n!rm `cat ./FILE_ID`.tar.gz\n!printf \"#Step3 End Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n\n# Step4 \n!printf \"# Step4 Start symlink python3.7 google* package to unzipped python3.8 package directory\\n\"\n!rm -fr /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!ls -d /usr/local/lib/python3.7/dist-packages/google* \\\n  | perl -pe 's/^(.+)$/sudo ln -sf ${1} \\/content\\/dist-packages\\/usr\\/local\\/lib\\/python3.8\\/dist-packages\\//g' \\\n    |/bin/bash -\n!ls -la /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!printf \"# Step4 End symlink the python3.7 google* package to the unzipped python3.8 package directory\\n\"\n\n# Step5\n!printf \"# Step5 Start and replace the unzipped python3.8 package directory with a regular python3.8 package directory with a symlink\\n\"\n!rm -fr /usr/local/lib/python3.8/dist-packages\n!sudo ln -s /content/dist-packages/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!printf \"# Step5 End And symlink the unzipped python3.8 package directory to the regular python3.8 package directory\\n\"\n\n# Step6\n!printf \"# Step6 Start change python interpreter to 3.8\\n\"\n!printf \"python3.8 restart step 4\\n\"\n!sudo ln -sf `which python3.8` /etc/alternatives/python3\n!python --version\n!printf \"# Step6 End Change python interpreter to 3.8\\n\"\n\n# Run ipkykernel with Step7 3.8 python, name it \"engbjapanpython3.8\" and install the runtime separately\n!printf \"#Step7 Start Name engbjapanpython3.8 and start runtime (Python 3.8) ipykernel\\n\"\n!sudo python -m ipykernel install --name \"engbjapanpython3.8\" --user\n!printf \"#Step7 End Start the runtime (Python 3.8) ipykernel named engbjapanpython3.8\\n\"\n!printf \"When everything is finished, please execute ``Change runtime type and reconnect''\\n\"\n\n\nimport sys\nprint(\"User Current Version:-\", sys.version)\n\nUser Current Version:- 3.8.15 (default, Oct 12 2022, 19:14:39) \n[GCC 7.5.0]\n\n\n\n\n\n\n!git lfs clone --depth 1 https://github.com/unifyai/models.git\n\n\n!git clone --depth 1 https://github.com/unifyai/ivy.git \n\n\n\n\n\n!pip install models/ --upgrade\n\n\n!pip install ivy/\n\n\n# install the optional requirements to use JAX, tensorflow and torch backends\n!pip install -r ivy/requirements/optional.txt\n\n\n# Install jaxlib with with the corresponding CUDA version.\n!pip install  https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14+cuda11.cudnn805-cp38-none-manylinux2014_x86_64.whl"
  },
  {
    "objectID": "wip/deepmind_perceiverio.html#run-the-demo",
    "href": "wip/deepmind_perceiverio.html#run-the-demo",
    "title": "Deepmind PerceiverIO on GPU",
    "section": "Run the demo‚Ä¶",
    "text": "Run the demo‚Ä¶\n\n#!/usr/local/bin/python3.8\n\nimport os\nimport ivy\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom ivy_models.transformers.helpers import FeedForward, PreNorm\nfrom ivy_models.transformers.perceiver_io import PerceiverIOSpec, PerceiverIO\n\n\n# Fetch sample images for inference\n!git clone https://github.com/ogbanugot/imagenet-samples.git\n\n#Fetch the class labels \n!git clone https://github.com/xmartlabs/caffeflow.git\n\n\ndef fetch_classes():\n  with open(\"/content/caffeflow/examples/imagenet/imagenet-classes.txt\", \"r\") as class_labels:\n    lines = class_labels.readlines()\n\n    classes = []\n    for l in lines:\n          classes.append(l.replace(\"\\n\", \"\"))\n    return classes\n\nclasses = fetch_classes()\nground_truth = [127, 31, 101, 32, 1]\npath_to_images = \"/content/imagenet-samples/\"\n\n\n#Helpers\n\ndef get_image(path, display=True, normalize=False):\n  img = Image.open(path).resize((224, 224))\n  if display:\n    return img\n\n  img = np.array(img)\n  img = img.astype(\"float32\")\n  img /= 255\n  if normalize:\n    mean = np.array([0.5, 0.5, 0.5])\n    std = np.array([0.5, 0.5, 0.5])\n    img[:, :] -= mean\n    img[:, :] /= std\n  return img\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    return ax\n\ndef show_results(path_to_images, preds, ground_truth):\n  # plot the images in the batch, along with predicted and true labels\n  fig = plt.figure(figsize=(25, 25))\n  idx = 0\n  for image in os.listdir(path_to_images):\n    if (image.endswith(\".JPEG\")):\n      this_dir = os.path.dirname(path_to_images)\n      image = get_image(os.path.join(this_dir, image))\n      ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n      imshow(image, ax)\n      ax.set_title(\"{} ({})\".format(classes[preds[idx]].split(',')[0], classes[ground_truth[idx]].split(',')[0]),\n                  color=(\"green\" if preds[idx]==ground_truth[idx] else \"red\"))\n      idx += 1\n    \n\n\n# Perceiver IO #\n# -------------#\ndef perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights):\n    # params\n    input_dim = 3\n    num_input_axes = 2\n    output_dim = 1000\n    network_depth = 8 if load_weights else 1\n    num_lat_att_per_layer = 6 if load_weights else 1\n\n    model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                        num_input_axes=num_input_axes,\n                                        output_dim=output_dim,\n                                        queries_dim=queries_dim,\n                                        network_depth=network_depth,\n                                        learn_query=learn_query,\n                                        query_shape=[1],\n                                        num_fourier_freq_bands=64,\n                                        num_lat_att_per_layer=num_lat_att_per_layer,\n                                        device=device))\n    \n    this_dir = os.path.dirname(\"models/\")\n    # maybe load weights\n    if load_weights:\n        weight_fpath = os.path.join(this_dir, 'ivy_models/transformers/pretrained_weights/perceiver_io.pickled')\n        assert os.path.isfile(weight_fpath)\n        # noinspection PyBroadException\n        try:\n            v = ivy.Container.from_disk_as_pickled(weight_fpath)\n            v = ivy.asarray(v)\n        except Exception:\n            # If git large-file-storage is not enabled (for example when testing in github actions workflow), then the\n            #  code will fail here. A placeholder file does exist, but the file cannot be loaded as pickled variables.\n            raise\n        # noinspection PyUnboundLocalVariable\n        model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                            num_input_axes=num_input_axes,\n                                            output_dim=output_dim,\n                                            queries_dim=queries_dim,\n                                            network_depth=network_depth,\n                                            learn_query=learn_query,\n                                            query_shape=[1],\n                                            max_fourier_freq=img_dims[0],\n                                            num_fourier_freq_bands=64,\n                                            num_lat_att_per_layer=num_lat_att_per_layer,\n                                            device=device), v=v)\n        \n    logits = []\n    for image in os.listdir(path_to_images):\n      if (image.endswith(\".JPEG\")):\n        # inputs\n        this_dir = os.path.dirname(\"/content/imagenet-samples/\")\n\n        img = get_image(os.path.join(this_dir, image), False, normalize_images)\n        img = ivy.array(img[None], dtype='float32', device=device)\n        queries = None if learn_query else ivy.random_uniform(shape=batch_shape + [1, queries_dim], device=device)\n        # output\n        output = model(img, queries=queries)\n        logits.append(ivy.argmax(output, axis=2).to_numpy()[0][0])\n\n    return logits\n    \n\n\n‚Ä¶with torch backend\n\nivy.set_backend(\"torch\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n‚Ä¶.with tensorflow backend\n\nivy.set_backend(\"tensorflow\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n2022-11-03 20:04:33.817437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n\n\n\n\n\n\n\n‚Ä¶with jax backend\n\nivy.set_backend(\"jax\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n‚Ä¶with numpy backend\n\nivy.set_backend(\"numpy\")\ndevice = \"cpu\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend."
  },
  {
    "objectID": "wip/0_building_blocks/0_2_transpile.html",
    "href": "wip/0_building_blocks/0_2_transpile.html",
    "title": "0.2: Transpile",
    "section": "",
    "text": "In this example, we transpile the original normalize function from torch to jax in one line of code. This is a common use case, where there is one target framework in mind.\nUsing what we learnt in the previous two notebooks for Unify and Compile, the workflow for converting directly from torch to jax would be as follows, first unifying to ivy code, and then compiling to the jax backend:\nimport ivy\nimport torch\nivy.set_backend(\"jax\")\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.compile(ivy.unify(normalize))\nnormalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project.\nThis workflow is common, and so in order to avoid repeated calls to ivy.unify followed by ivy.compile, there is another convenience function ivy.transpile, which basically acts as a shorthand for this pair of function calls:\nnormalize = ivy.transpile(normalize)\nAgain, normalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project."
  },
  {
    "objectID": "wip/0_building_blocks/0_2_transpile.html#round-up",
    "href": "wip/0_building_blocks/0_2_transpile.html#round-up",
    "title": "0.2: Transpile",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now transpile code from one framework to another with one line of code! That concludes the collection of notebooks on the ‚ÄúBuilding Blocks‚Äù. However, there are still many other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. In the next collection of notebooks ‚ÄúThe Basics‚Äù, we‚Äôll be learning about the various different ways that ivy.unify, ivy.compile and ivy.transpile can be called, and what implications each of these have."
  },
  {
    "objectID": "wip/0_building_blocks/0_0_unify.html",
    "href": "wip/0_building_blocks/0_0_unify.html",
    "title": "0.0: Unify",
    "section": "",
    "text": "In this example, we unify a simple torch function normalize. We then show how this newly unified normalize function can be used alongside any ML framework!\nFirstly, let‚Äôs import the dependencies and define the torch function.\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\nNow, let‚Äôs unify the function!\nnormalize = ivy.unify(normalize)\nAnd that‚Äôs it! The normalize function can now be used with any ML framework. It‚Äôs as simple as that!\nSo, let‚Äôs give it a try!\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\n# numpy\nprint(normalize(x, mean, std))\n\n# jax\nimport jax.numpy as jnp\nx_ = jnp.array(x)\nmean_ = jnp.array(mean)\nstd_ = jnp.array(std)\nprint(normalize(x_, mean_, std_))\n\n# tensorflow\nimport tensorflow as tf\nx_ = tf.constant(x)\nmean_ = tf.constant(mean)\nstd_ = tf.constant(std)\nprint(normalize(x_, mean_, std_))\n\n# torch\nx_ = torch.tensor(x)\nmean_ = torch.tensor(mean)\nstd_ = torch.tensor(std)\nprint(normalize(x_, mean_, std_))\nWe can see that the new normalize function can operate with any ML framework. ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks."
  },
  {
    "objectID": "wip/0_building_blocks/0_0_unify.html#round-up",
    "href": "wip/0_building_blocks/0_0_unify.html#round-up",
    "title": "0.0: Unify",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now unify ML code! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to make our unified Ivy code run much more efficiently! ‚ö°"
  },
  {
    "objectID": "wip/0_building_blocks/0_1_compile.html",
    "href": "wip/0_building_blocks/0_1_compile.html",
    "title": "0.1: Compile",
    "section": "",
    "text": "In this example, we compile our simple unified ivy function normalize from the last notebook. We then show how this newly compiled normalize function exhibits much better runtime performance than the non-compiled version.\nFirstly, let‚Äôs pick up where we left off in the last notebook, with our unified normalize function:\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.unify(normalize)\nFor the purpose of illustration, we will use jax as our backend framework:\n# set ivy's backend to jax\nivy.set_backend(\"jax\")\n\n# Import jax numpy API\nimport jax.numpy as jnp\n\n# create random jax arrays for testing\nx = jnp.randon.uniform(size=10)\nmean = jnp.mean(x)\nstd = jnp.std(x)\nAs in the previous example, the unified function can be executed like so (in this case it will trigger lazy unification, see the Lazy vs Eager section for more details):\nnormalize(x, mean, std)\nWhen calling this function, all of ivy‚Äôs function wrapping is included in the call stack of normalize, which adds runtime overhead. In general, ivy.compile strips any arbitrary function down to its constituent functions in the functional API of the target framework. It will then also be compiled to machine-code if the target framework supports low-level compiling (via functions such as tf.function, torch.jit.script, torch.jit.trace, torch.compile, jax.jit etc.). The code can be compiled like so:\ncomp = ivy.compile(normalize)  # compiles to jax, due to ivy.set_backend\nThe compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\ncomp(x, mean, std)\nThe machine-code compilation can be turned off by setting the argument low_level = False, in which case it will simply return a chain of Python function in the functional API of the target framework (in this case JAX). This will still improve the runtime efficiency over the original un-compiled version due to the removal of all ivy wrapping overhead, but it will not be as runtime efficient as the low-level compiled version:\npartial_comp = ivy.compile(normalize, low_level=False)\nAgain, the compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\npartial_comp(x, mean, std)\nWith all lazy unification and compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:\nivy.time_function(normalize)(x, mean, std)\nivy.time_function(partial_comp)(x, mean, std)\nivy.time_function(comp)(x, mean, std)\nAs expected, we can see that the slowest is normalize, which includes all ivy wrapping overhead. Next is partial_comp which has no wrapping overhead but is still expressed entirely in Python, without compiling to low-level code. The fastest is comp because the wrapping overhead is removed and the function is compiled to low-level code for maximal efficiency."
  },
  {
    "objectID": "wip/0_building_blocks/0_1_compile.html#round-up",
    "href": "wip/0_building_blocks/0_1_compile.html#round-up",
    "title": "0.1: Compile",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now compile ivy code for more efficient inference! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to transpile code from one framework to another in a single line of code üîÑ"
  },
  {
    "objectID": "wip/ivy_as_a_transpiler_intro.html",
    "href": "wip/ivy_as_a_transpiler_intro.html",
    "title": "Ivy as a Transpiler Introduction",
    "section": "",
    "text": "Head to our website\nCreate an account and generate an API Key\nSet theIVY_API_KEYenvironment variable to your generated key"
  },
  {
    "objectID": "wip/ivy_as_a_transpiler_intro.html#transpiler-interface",
    "href": "wip/ivy_as_a_transpiler_intro.html#transpiler-interface",
    "title": "Ivy as a Transpiler Introduction",
    "section": "Transpiler Interface",
    "text": "Transpiler Interface\n    def transpile(\n        self,\n        *objs,\n        to: Optional[str] = None,\n        args: Optional[tuple] = None,\n        kwargs: Optional[dict] = None,\n    ) -&gt; Callable:\n        \"\"\"\n        objs\n            the functions, models or modules to be transpiled\n        to \n            the framework to be transpiled to\n        args\n            The positional arguments passed to the function for tracing\n        kwargs\n            The keyword arguments passed to the function for tracing\n        \"\"\"\n\nTranspile either functions, trainable models or importable python modules, with any number and combo permitted\nIf no ‚Äúobjs‚Äù are provided, the function returns a new transpilation function which receives only one object as input, making it usable as a decorator\nIf neither ‚Äúargs‚Äù nor ‚Äúkwargs‚Äù are specified, then the transpilation will occur lazily, upon the first call of the transpiled function, otherwise transpilation is eager\n\n\nTelemetry\nTelemetry helps us better understand how users are interacting with the transpiler & how to make it better, we specifcally collect: 1. Invocations of the transpiler 2. graph representation of the transpiled object(s) 3. General machine information (e.g.¬†number of CPUs, GPUs, OS)\nadd option to opt-out?\n\n\n1. Transpile Functions üî¢\n\n!pip install kornia ivy-core\n!wget https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/logo.png -O image.png\n\n\nfrom google.colab.patches import cv2_imshow\nimport cv2 \n\nimg = cv2.imread(\"image.png\")\ncv2_imshow(img)\n\n\n\n\n\nimport os\nimport ivy\nimport kornia\nimport jax.numpy as jnp\n\n# set the API key\nos.envrion[\"IVY_API_KEY\"] = \"\"\n\n# load image into jax \nimg = jnp.array(cv2.imread('image.png'))/255\nimg = jnp.expand_dims(jnp.transpose(img, (2, 0, 1)), 0)\n\n# transpile function lazily\ncanny = ivy.transpile(kornia.feature.canny,to='jax')\n\ncv2_imshow(canny(img))\n\n\nimport tensorflow as tf\n# load image in tensorflow\noriginal_img = tf.array(cv2.imread('image.png'))/255\noriginal_img = tf.expand_dims(tf.transpose(img, (2, 0, 1)), 0)\n\n\n#@title Run transpilation in eager/lazy mode { display-mode: \"form\" }\nimport torch \nmode = \"Lazy\" #@param [\"Eager\", \"Lazy\"]\nfn_args = (original_img,) if mode == \"Eager\" else None\n\n@ivy.transpile(to=\"tensorflow\",args=fn_args)\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n\n%%time\nnew_img = dilate_edges(original_img)\n\nYou selected Eager\n\n\n\ncv2_imshow(new_img)\n\n\n\n2. Transpile Libraries üìö\n\n# transpile module lazily\nkornia = ivy.transpile(kornia,to=\"tensorflow\")\n\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n%%time\nnew_img = dilate_edges(original_img)\n\n\ncv2_imshow(img)\n\n\n\n3. Transpile Models üåê\n\nimport haiku as hk\n\nnum_classes = 10\nhk_model = hk.nets.ResNet18(num_classes)\n\n# transpile to framework of your choice\ntorch_model = ivy.transpile(hk_model,to=torch.nn.Module)\nkeras_model = ivy.transpile(hk_model,to=tf.keras.Model)\n\n\n# visualize function counting as in the odsc talk"
  },
  {
    "objectID": "wip/3_models/3_0_perceiver.html",
    "href": "wip/3_models/3_0_perceiver.html",
    "title": "3.0: Perceiver",
    "section": "",
    "text": "ToDo: description"
  },
  {
    "objectID": "wip/3_models/3_1_stable_diffusion.html",
    "href": "wip/3_models/3_1_stable_diffusion.html",
    "title": "3.1: Stable Diffusion",
    "section": "",
    "text": "ToDo: description"
  },
  {
    "objectID": "wip/2_libraries/2_0_kornia.html",
    "href": "wip/2_libraries/2_0_kornia.html",
    "title": "2.0: Kornia",
    "section": "",
    "text": "ToDo: description"
  },
  {
    "objectID": "wip/odsc.html",
    "href": "wip/odsc.html",
    "title": "ODSC East Ivy Demo",
    "section": "",
    "text": "First, let‚Äôs install Ivy. For the package to be available, you will have to click on ‚ÄúRuntime &gt; Restart Runtime‚Äù after running the cell below üòÑ\n!git clone https://github.com/unifyai/ivy.git\n!cd ivy && git checkout 4903d4fa229f729c4b140218e3bc7f644db6f01e && python3 -m pip install --user -e .\n!pip install dm-haiku\n!pip install kornia\n!pip install timm\n!mkdir -p .ivy\n!echo -n YOUR_API_TOKEN &gt; .ivy/key.pem"
  },
  {
    "objectID": "wip/odsc.html#ivy-as-a-framework",
    "href": "wip/odsc.html#ivy-as-a-framework",
    "title": "ODSC East Ivy Demo",
    "section": "Ivy as a Framework",
    "text": "Ivy as a Framework\nIn this introduction, we will cover the fundamentals of using Ivy to write your own framework-indepent and future-proof code!\nIf you are interested in exploring the theoretical aspects behind the contents of this notebook you can check out the Design and the Deep Dive sections of the documentation!\nFirst of all, let‚Äôs import Ivy\n\nimport ivy\n\n\nIvy Backend Handler\nWhen used as a ML framework, Ivy is esentially an abstraction layer that supports multiple frameworks as the backend. This means that any code written in Ivy can be executed in any of the supported frameworks, with Ivy managing the framework-specific data structures, functions, optimizations, quirks and perks under the hood.\nTo switch the backend, we can use the ivy.set_backend function and pass the appropriate framework as a string. This is the easiest way to interact with the Backend Handler submodule, which manages the current backend and links Ivy‚Äôs objects and functions with the corresponding framework-specific ones.\nFor example:\n\nivy.set_backend(\"tensorflow\")\n\n\n\nData Structures\nThe basic data structure in Ivy is the ivy.Array. This is an abstraction of the array classes of the supported frameworks. Likewise, we also have ivy.NativeArray, which is an alias for the array class of the selected backend.\nLastly, there is another structure called the ivy.Container. It‚Äôs a subclass of dict that is optimized for recursive operations. If you want to learn more about it, you can defer to the following link!\nLet‚Äôs create an array using ivy.array(). Similarly, we can use ivy.native_array() to create a torch.Tensor now that the backend is set to torch.\n\nivy.set_backend(\"torch\")\n\nx = ivy.array([1, 2, 3])\nprint(type(x))\n\nx = ivy.native_array([1, 2, 3])\nprint(type(x))\n\n\n\nIvy Functional API\nIvy does not implement its own low-level (C++/CUDA) backend for its functions. Instead, it wraps the functional API of existing frameworks, unifying their fundamental functions under a common signature. For example, let‚Äôs take a look at ivy.matmul():\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.array([[1], [2], [3]]), ivy.array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output.to_native()))\n\nThe output arrays shown above are ivy.Array instances. To obtain the underlying native array, we need to use the to_native() method.\nHowever, if you want the functions to return the native arrays directly, you can disable the array_mode of Ivy using ivy.set_array_mode().\n\nivy.set_array_mode(False)\n\nivy.set_backend(\"jax\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"tensorflow\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_backend(\"torch\")\nx1, x2 = ivy.native_array([[1], [2], [3]]), ivy.native_array([[1, 2, 3]])\noutput = ivy.matmul(x1, x2)\nprint(type(output))\n\nivy.set_array_mode(True)\n\nKeeping this in mind, you can build any function you want as a composition of Ivy functions. When executed, this function will ultimately call the current backend functions from its functional API.\n\ndef sigmoid(z):\n    return ivy.divide(1, (1 + ivy.exp(-z)))\n\nIn essence, this means that by writing your code just once with Ivy, it becomes accessible for for use within any project regardless of the underlying framework being used!\n\n\nIvy Stateful API\nAs we have seen in the slides, Ivy also has a stateful API which builds on its functional API and the ivy.Container class to provide high-level classes such as optimizers, network layers, or trainable modules.\nThe most important stateful class within Ivy is ivy.Module, which can be used to create trainable layers and entire networks. A very simple example of an ivy.Module could be:\n\nclass Regressor(ivy.Module):\n    def __init__(self, input_dim, output_dim):\n        self.linear0 = ivy.Linear(input_dim, 128)\n        self.linear1 = ivy.Linear(128, output_dim)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = self.linear0(x)\n        x = ivy.functional.relu(x)\n        x = self.linear1(x)\n        return x\n\nTo use this model, we would simply have to set a backend and instantiate the model:\n\nivy.set_backend('torch')  # set backend to PyTorch\n\nmodel = Regressor(input_dim=1, output_dim=1)\noptimizer = ivy.Adam(0.1)\n\nNow we can generate some sample data and train the model using Ivy as well.\n\nn_training_examples = 2000\nnoise = ivy.random.random_normal(shape=(n_training_examples, 1), mean=0, std=0.1)\nx = ivy.linspace(-6, 3, n_training_examples).reshape((n_training_examples, 1))\ny = 0.2 * x ** 2 + 0.5 * x + 0.1 + noise\n\n\ndef loss_fn(pred, target):\n    return ivy.mean((pred - target)**2)\n\nfor epoch in range(50):\n    # forward pass\n    pred = model(x)\n\n    # compute loss and gradients\n    loss, grads = ivy.execute_with_gradients(lambda v: loss_fn(pred, y), model.v)\n\n    # update parameters\n    model.v = optimizer.step(model.v, grads)\n\n    # print current loss\n    print(f'Epoch: {epoch + 1:2d} --- Loss: {ivy.to_numpy(loss).item():.5f}')\n\nprint('Finished training!')\n\n\n\nGraph Compiler\nWe have just explored how to create framework agnostic functions and models with Ivy. Nonetheless, due to the wrapping Ivy performs on top of native functions, there is a slight performance overhead introduced with each function call. To address this, we can use Ivy‚Äôs graph compiler.\nThe purpose of the Graph Compiler is to extract a fully functional, efficient graph composed only of functions from the corresponding functional APIs of the underlying framework (backend).\nOn top of using the Graph Compiler to remove the overhead introduced by Ivy, it can also be used with functions and modules written directly with a given framework. In this case, the GC will decompose any high-level API into a fully-functional graph of functions from said framework.\nAs an example, let‚Äôs write a simple normalize function using Ivy:\n\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\nTo compile this function, simply call ivy.compile(). To specify the underlying framework, you can pass the name of the framework as an argument using to. Otherwise, the current backend will be used by default.\n\nimport torch\nx0 = torch.tensor([1., 2., 3.])\nnormalize_comp = ivy.compile(normalize, to=\"torch\", args=(x0,))\n\nAs anticipated, the compiled function, which uses native torch operations directly, is faster than the original function:\n\n%%timeit\nnormalize(x0)\n\n\n%%timeit\nnormalize_comp(x0)\n\nIn the example above, we compiled the function eagerly, which means that the compilation process happened immediately, as we have passed the arguments for tracing. However, if we don‚Äôt pass any arguments to the compile function, compilation will occur lazily, and the graph will be built only when we call the compiled function for the first time. To summarize:\n\nimport torch\n\nx1 = torch.tensor([1., 2., 3.])\n\n\n# Arguments are available -&gt; compilation happens eagerly\neager_graph = ivy.compile(normalize, to=\"torch\", args=(x1,))\n\n# eager_graph is now torch code and runs efficiently\nret = eager_graph(x1)\n\n\n# Arguments are not available -&gt; compilation happens lazily\nlazy_graph = ivy.compile(normalize, to=\"torch\")\n\n# The compiled graph is initialized, compilation will happen here\nret = lazy_graph(x1)\n\n# lazy_graph is now torch code and runs efficiently\nret = lazy_graph(x1)"
  },
  {
    "objectID": "wip/odsc.html#ivy-as-a-transpiler",
    "href": "wip/odsc.html#ivy-as-a-transpiler",
    "title": "ODSC East Ivy Demo",
    "section": "Ivy as a Transpiler",
    "text": "Ivy as a Transpiler\nWe have just learned how to write framework-agnostic code and compile it into an efficient graph. However, many codebases, libraries, and models have already been developed (and will continue to be!) using other frameworks.\nTo allow for speed-of-thought research and development, Ivy also allows you to use any code directly into your project, regardless of the framework it was written in. No matter what ML code you want to use, Ivy‚Äôs Transpiler is the tool for the job üõ†Ô∏è\n\nAny function\nLet‚Äôs start by transpiling a very simple torch function.\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\njax_normalize = ivy.transpile(normalize, source=\"torch\", to=\"jax\")\n\nSimilar to compile, the transpile function can be used eagerly or lazily. In this particular example, transpilation is being performed lazily, since we haven‚Äôt passed any arguments or keyword arguments to ivy.transpile.\n\nimport jax\nkey = jax.random.PRNGKey(42)\njax.config.update('jax_enable_x64', True)\nx = jax.random.uniform(key, shape=(10,))\n\njax_out = jax_normalize(x)\nprint(jax_out, type(jax_out))\n\nThat‚Äôs pretty much it! You can now use any function you need in your projects regardless of the framework you‚Äôre using üöÄ\nHowever, transpiling functions one by one is far from ideal. But don‚Äôt worry, with transpile, you can transpile entire libraries at once and easily bring them into your projects. Let‚Äôs see how this works by transpiling kornia, a wisely-used computer vision library written in torch:\n\n\nAny library\n\nimport kornia\nimport requests\nimport jax.numpy as jnp\nimport numpy as np\nfrom PIL import Image\n\nLet‚Äôs get the transpiled library by calling transpile.\n\njax_kornia = ivy.transpile(kornia, source=\"torch\", to=\"jax\")\n\nNow let‚Äôs get a sample image and preprocess so that it has the format kornia expects:\n\nurl = \"http://images.cocodataset.org/train2017/000000000034.jpg\"\nraw_img = Image.open(requests.get(url, stream=True).raw)\nimg = jnp.transpose(jnp.array(raw_img), (2, 0, 1))\nimg = jnp.expand_dims(img, 0) / 255\ndisplay(raw_img)\n\nAnd we can call any function from kornia in jax, as simple as that!\n\nout = jax_kornia.enhance.sharpness(img, 10)\ntype(out)\n\nFinally, let‚Äôs see if the transformation has been applied correctly:\n\nnp_image = np.uint8(np.array(out[0])*255)\ndisplay(Image.fromarray(np.transpose(np_image, (1, 2, 0))))\n\nIt‚Äôs worth noting that every operation in the transpiled functions is performed natively in the target framework, which means that gradients can be tracked and the resulting functions are fully differentiable. Even after transpilation, you can still take advantage of the powerful features of your chosen framework.\nWhile transpiling functions and libraries is useful, trainable modules play a critical role in ML and DL. The good news is that Ivy makes it just as easy to transpile modules and models from one framework to another with just one line of code.\n\n\nAny model\nFor the purpose of this demonstration, let‚Äôs define a very basic CNN block using the Sequential API of keras.\n\nimport tensorflow as tf\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 3)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(10, activation='softmax')\n])\n\nThe model we just defined is an instance of tf.keras.Model. Using ivy.transpile, we can effortlessly convert it into a torch.nn.Module, for instance.\n\ninput_array = tf.random.normal((1, 28, 28, 3))\ntorch_model = ivy.transpile(model, to=\"torch\", args=(input_array,))\n\nAfter transpilation, we can pass a torch tensor and obtain the expected output. As mentioned previously, all operations are now PyTorch native functions, making them differentiable. Additionally, Ivy automatically converts all parameters of the original model to the new one, allowing you to transpile pre-trained models and fine-tune them in your preferred framework.\n\nisinstance(torch_model, torch.nn.Module)\n\n\ninput_array = torch.rand((1, 28, 28, 3))\noutput_array = torch_model(input_array)\nprint(output_array)\n\nWhile we have only transpiled a simple model for demonstration purposes, we can certainly transpile more complex models as well. Let‚Äôs take a more complex model from timm and see how we can build upon transpiled modules.\n\nimport timm\n\nWe will only be using the encoder, so we can remove the unnecessary layers by setting num_classes=0, and then pass pretrained=True to download the pre-trained parameters.\n\nmlp_encoder = timm.create_model(\"mixer_b16_224\", pretrained=True, num_classes=0)\n\nLet‚Äôs transpile the model to tensorflow with ivy.transpile üîÄ\n\nnoise = torch.randn(1, 3, 224, 224)\ntf_mlp_encoder = ivy.transpile(mlp_encoder, to=\"tensorflow\", args=(noise,))\n\nAnd now let‚Äôs build a model on top of our pretrained encoder!\n\nclass Classifier(tf.keras.Model):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        self.encoder = tf_mlp_encoder\n        self.output_dense = tf.keras.layers.Dense(units=1000, activation=\"softmax\")\n\n    def call(self, x):\n        x = self.encoder(x)\n        return self.output_dense(x)\n\n\nmodel = Classifier()\n\nx = tf.random.normal(shape=(1, 3, 224, 224))\nret = model(x)\nprint(type(ret), ret.shape)\n\nAs the encoder now consists of tensorflow functions, we can extend the transpiled modules as much as we want, leveraging existing weights and the tools and infrastructure of all frameworks üöÄ"
  },
  {
    "objectID": "wip/hf_tensorflow_deit.html",
    "href": "wip/hf_tensorflow_deit.html",
    "title": "HuggingFace Tensorflow DeiT",
    "section": "",
    "text": "This is an example of compilation (using ivy) of DeiT model from Huggingface implemented with tensorflow.\nThe model can be found here.\nfrom transformers import DeiTImageProcessor, TFDeiTForImageClassification\nimport torch\nimport tensorflow as tf\nfrom PIL import Image\nimport requests\nimport numpy as np\nThis image from cocodataset will be used as an input. First it should be preprocessed using DeiTImageProcessor\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n# note: we are loading a TFDeiTForImageClassificationWithTeacher from the hub here,\n# so the head (layers like 'distillation_classifier', 'cls_classifier') will be randomly initialized, hence the predictions will be random.\n\n# To be able to reproduce, lets set the random seed.\ntf.keras.utils.set_random_seed(3)\n\n\nimage_processor = DeiTImageProcessor.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\nmodel = TFDeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\ninputs = image_processor(images=image, return_tensors=\"tf\")\noutputs_from_original_model = model(**inputs)\nlogits = outputs_from_original_model.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = tf.math.argmax(logits, axis=-1)[0]\nprint(\"Predicted class:\", model.config.id2label[int(predicted_class_idx)])\n\nSome layers from the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing TFDeiTForImageClassification: ['distillation_classifier', 'cls_classifier']\n- This IS expected if you are initializing TFDeiTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDeiTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nPredicted class: ptarmigan\nLet‚Äôs install ivy from github\n!rm -rf ivy\n!git clone --branch master https://github.com/unifyai/ivy.git --single-branch\n!cd ivy && pip install -e .\nimport ivy\nivy.set_backend(\"tensorflow\")\nfrom ivy import ModuleConverters as mc\nConverts keras module to ivy module. This takes some time\nivy_model = mc.from_keras_module(model)\nCompile the module with ivy.compile\ncompiled_func, graph=ivy.compile(ivy_model, **inputs, return_graph=True)\nivy_outputs = ivy_model(**inputs)\ncompiled_outputs = compiled_func(**inputs)\n# Check if outputs are the same\nassert np.allclose(outputs_from_original_model.logits, ivy_outputs.logits), \"Error, outputs diverge\"\nassert np.allclose(outputs_from_original_model.logits, compiled_outputs.logits), \"Error, outputs diverge\"\nBelow, all functions which will be executed in order as they are printed.\nprint([fn.__name__ for fn in graph._all_functions])\n\n['convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'transpose_v2', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convolution_v2', 'bias_add', 'reshape', 'concat', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'identity', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', '_slice_helper', 'convert_to_tensor_v2_with_dispatch', 'matmul', 'bias_add']"
  },
  {
    "objectID": "wip/hf_tensorflow_deit.html#graph-can-be-visualized-and-displayed-as-html-file-on-browser",
    "href": "wip/hf_tensorflow_deit.html#graph-can-be-visualized-and-displayed-as-html-file-on-browser",
    "title": "HuggingFace Tensorflow DeiT",
    "section": "Graph can be visualized and displayed as html file on browser",
    "text": "Graph can be visualized and displayed as html file on browser\n\ngraph.show(\n    save_to_disk=True,\n    fname='deit'\n)\n\nLet‚Äôs compare execution times for models. Compiled module/function usually run faster due to optimization by ivy‚Äôs graph compiler\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: ivy_model(**inputs), number=N)\nprint(res/N,'ms')\n\n0.12265048989200113 ms\n\n\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: compiled_func(**inputs), number=N)\nprint(res/N,'ms')\n\n0.11038777417100028 ms\n\n\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: model(**inputs), number=N)\nprint(res/N,'ms')\n\n0.1167045795539998 ms"
  },
  {
    "objectID": "wip/end_to_end_training_pipeline_in_ivy.html",
    "href": "wip/end_to_end_training_pipeline_in_ivy.html",
    "title": "End-to-End Training Pipeline in Ivy",
    "section": "",
    "text": "ToDo: description\n\n \n\n\n# install the latest Ivy version for this purpose\n!pip install git+https://github.com/unifyai/ivy.git@master\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/unifyai/ivy.git@master\n  Cloning https://github.com/unifyai/ivy.git (to revision master) to /tmp/pip-req-build-_3r2_73j\n  Running command git clone --filter=blob:none --quiet https://github.com/unifyai/ivy.git /tmp/pip-req-build-_3r2_73j\n  Resolved https://github.com/unifyai/ivy.git to commit 0edf8c1e8ea835f4c456bdf89737d89032f50b5a\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from ivy-core==1.1.9) (1.22.4)\nCollecting einops==0.4.1\n  Downloading einops-0.4.1-py3-none-any.whl (28 kB)\nCollecting psutil==5.9.1\n  Downloading psutil-5.9.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 281.1/281.1 KB 10.9 MB/s eta 0:00:00\nCollecting termcolor==1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n  Preparing metadata (setup.py) ... done\nCollecting colorama==0.4.5\n  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\nCollecting packaging==21.3\n  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 40.8/40.8 KB 4.7 MB/s eta 0:00:00\nCollecting nvidia-ml-py&lt;=11.495.46\n  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\nRequirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging==21.3-&gt;ivy-core==1.1.9) (3.0.9)\nBuilding wheels for collected packages: ivy-core, termcolor\n  Building wheel for ivy-core (setup.py) ... done\n  Created wheel for ivy-core: filename=ivy_core-1.1.9-py3-none-any.whl size=1297564 sha256=05fcafac1e19fec835a9ac61270b3ac6039a5095f6b0f9fde20bacc2a5abba45\n  Stored in directory: /tmp/pip-ephem-wheel-cache-le3bu3_v/wheels/07/46/2e/ae2d7c5ce8708e605368a33e08d57d1de8e107e3db157c3063\n  Building wheel for termcolor (setup.py) ... done\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4845 sha256=cc6508f5d7e25538c5df5fdae52a41d2bf17b9a517aedd125cfca913bb5b259b\n  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\nSuccessfully built ivy-core termcolor\nInstalling collected packages: termcolor, nvidia-ml-py, einops, psutil, packaging, colorama, ivy-core\n  Attempting uninstall: termcolor\n    Found existing installation: termcolor 2.2.0\n    Uninstalling termcolor-2.2.0:\n      Successfully uninstalled termcolor-2.2.0\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.4\n    Uninstalling psutil-5.9.4:\n      Successfully uninstalled psutil-5.9.4\n  Attempting uninstall: packaging\n    Found existing installation: packaging 23.0\n    Uninstalling packaging-23.0:\n      Successfully uninstalled packaging-23.0\nSuccessfully installed colorama-0.4.5 einops-0.4.1 ivy-core-1.1.9 nvidia-ml-py-11.495.46 packaging-21.3 psutil-5.9.1 termcolor-1.1.0\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\nImporting libraries\n\n# third party libraries\nimport ivy\n\n# built-in libraries\nimport os\nimport random\nimport csv\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n!mkdir /content/drive/MyDrive/Ivy/\nos.chdir('/content/drive/MyDrive/Ivy')\n\n\n\nLet‚Äôs build the pipeline with a Tensorflow backend\nOne can experiment with any other backend as well. Just add the following line at the start with the string name to be the framework you want.\n\nivy.set_backend(\"tensorflow\")\n\n\n\nWe are using MNIST dataset for this Tutorial\nNow we will download the MNIST dataset from Kaggle using its API. More details in this medium article.\n\n!mkdir /root/.kaggle\n\nmkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n\n\nInsert the kaggle.json file in the /root/.kaggle folder. You can download the file here https://www.kaggle.com/{your_kaggle_username}/account by clicking on Create New API Token\n\n!kaggle competitions download -c digit-recognizer \n!unzip digit-recognizer.zip\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\ndigit-recognizer.zip: Skipping, found more recently modified local copy (use --force to force download)\nArchive:  digit-recognizer.zip\n  inflating: sample_submission.csv   \n  inflating: test.csv                \n  inflating: train.csv               \n\n\n\n#Import pandas and read training and testing data in data frames\nimport pandas as pd \ntrain = pd.read_csv(\"train.csv\")\ntest = pd.read_csv(\"test.csv\")\n\n\n# Set the PATH string that will be used to save the dataset in your Google Drive folder, it will be used later to load the dataset\nPATH = '/content/drive/My Drive/mnist'\n\n\n#Create a data frame to store labels, another without labels, df with values only, test value df and a path variable to set location of where to store images\nlabel_df = train['label']\nmod_train = train.drop(columns= 'label')\ndata_values = mod_train.values\ntest_data_values = test.values\n\n\nfor i in range(0, len(data_values)):\n    #read the correct label\n    correct_label = label_df[i]\n\n    #split the data into training and validation sets\n    if np.random.rand() &lt; 0.8:\n        folder = '/train/'\n        train_path = f'{PATH}' + '/train/' + str(correct_label)\n        if not os.path.exists(train_path):\n          os.makedirs(train_path)\n    else:\n        folder = '/valid/'\n        valid_path = f'{PATH}' + '/valid/' + str(correct_label)\n        if not os.path.exists(valid_path):\n          os.makedirs(valid_path)\n    img = data_values[i][:]\n    #reshape into 28x28 pic\n    img = img.reshape(28,28)\n    #we need three channels into the picture\n    img = np.stack((img,)*3,axis = -1)\n    #change the data type to int8\n    img = np.uint8(img)\n    #create PIL Image\n    new_img = Image.fromarray(img)\n    #save the .jpg into correct folder\n    new_img.save(f'{PATH}' + folder + str(correct_label) + '/' + str(i) + '.jpg', 'JPEG')\n\n\n\nTemporary Dataset and Dynamic loader\nSince we don‚Äôt have the builder ready just yet, we will create three functions which help in generating the dataset, randomizing, and batchwise loading at training time.\nNote - We‚Äôre only using a small subset of the entire dataset for the purpose of this demo. Same goes for the number of epochs we train the model for.\nLet‚Äôs set a global seed for randomized operations\n\nivy.seed(seed_value = 0)\n\n\ndef randomize_dataset(images, classes):\n    data = list(zip(images, classes))\n    random.shuffle(data)\n    images, classes = zip(*data)\n    return list(images), list(classes)\n\n \ndef create_dataset(folder, num_examples_per_class = 100):\n    img_array = []\n    class_name = []\n    for dir in os.listdir(folder):\n        for i, file in enumerate(os.listdir(os.path.join(folder, dir))):\n            if i &gt;= num_examples_per_class:\n                continue\n            img_path = os.path.join(folder, dir, file)\n            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            image = ivy.array(image).astype('float32').expand_dims()\n            image /= 255\n            img_array.append(image) \n            class_name.append(dir)\n    \n    return randomize_dataset(img_array, class_name)\n\n\ndef generate_batches(images, classes, dataset_size, batch_size = 32):\n    targets={k: v for v, k in enumerate(np.unique(classes))}\n    y_train= [targets[classes[i]] for i in range(len(classes))]\n    if batch_size &gt; dataset_size:\n        raise ivy.utils.exceptions.IvyError('Use a smaller batch size')\n    for idx in range(0, dataset_size, batch_size):\n        yield ivy.stack(images[idx:min(idx+batch_size, dataset_size)]), ivy.array(y_train[idx:min(idx+batch_size, dataset_size)])\n\n\n#choosing 1000 examples per class for this demo\nimages, classes = create_dataset(PATH + '/train',num_examples_per_class = 1000)\n\n\nprint(f'Number of Training Examples is -: {len(images)}')\n\nNumber of Training Examples is -: 10000\n\n\n\ntargets = {k: v for v, k in enumerate(np.unique(classes))}\nprint(f\"Class labels -: {targets}\")\n\nClass labels -: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\nx_batch_instance, y = next(iter(generate_batches(images, classes, len(images))))\nprint(f\"Data is of the form -: {x_batch_instance.shape} (NCHW)\\n\")\nprint(y.shape[0], y)\n\nData is of the form -: (32, 1, 28, 28) (NCHW)\n\n32 ivy.array([2, 6, 8, 8, 3, 6, 0, 3, 6, 4, 1, 5, 8, 3, 8, 7, 4, 7, 1, 0, 0, 7,\n       9, 8, 2, 0, 2, 8, 8, 9, 1, 2], dev=gpu:0)\n\n\nIntialising some training parameters\n\noptimizer= ivy.Adam(1e-4)\nbatch_size = 64 \nnum_epochs = 20\nnum_classes = 10\n\n\n\nDefining the Ivy Network\nWe inherit from the ivy.Module class for creation of networks. This helps us with the forward pass and computation of the gradients. Note that some keyword arguments below are user-defined, and are purely for the purpose of building this model. You can find more information about the init method inside the docs.\n\nclass IvyNet(ivy.Module):\n    def __init__(self, h_w = (32, 32), input_channels = 3,  output_channels = 512, kernel_size = [3, 3], num_classes = 2, data_format = \"NCHW\", device = \"cpu\"):\n        self.extractor = ivy.Sequential(\n            ivy.Conv2D(input_channels, 6, [5, 5], 1,  \"SAME\", data_format = data_format),\n            ivy.GELU(),\n            ivy.Conv2D(6,  16,  [5, 5], 1,  \"SAME\", data_format = data_format),\n            ivy.GELU(),\n            ivy.Conv2D(16, output_channels, [5, 5],  1,  \"SAME\", data_format = data_format),\n            ivy.GELU()\n        )\n        \n        self.classifier = ivy.Sequential(\n            ivy.Linear(h_w[0]*h_w[1]*output_channels, 512), #since padding is same, this would be image_height x image_widht x output_channels\n            ivy.GELU(),\n            ivy.Linear(512, num_classes)\n        )\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = self.extractor(x)\n        x = ivy.flatten(x, start_dim = 1, end_dim = -1) #flatten all dims except batch dim\n        logits = self.classifier(x)\n        probs = ivy.softmax(logits)\n        return logits, probs\n\n# train the model on gpu if it's available\ndevice = \"cuda:0\" if ivy.gpu_is_available() else \"cpu\"\n\nmodel = IvyNet(h_w = (28, 28), input_channels = 1, output_channels = 120, kernel_size = [5,5], num_classes = num_classes, device = device)\nmodel_name = type(model).__name__.lower()\n\n\nprint(device, model_name)\n\ncuda:0 ivynet\n\n\n\n\nTraining Loop with utility functions\nThe train function is where we do the heavy lifting, and use the loss_fn to compute the gradients. num_correct is used for returning the correct set of predictions.\n\ndef num_correct(preds, labels):\n    return (preds.argmax() == labels).sum().to_numpy().item()\n\ndef loss_fn(params):\n    v, model, x, y = params\n    y_pred, probs = model(x)\n    return ivy.cross_entropy(y, probs), probs\n\ndef train(images, classes, epochs, model, device, num_classes = 10, batch_size = 32):\n    # training metrics\n    epoch_loss = 0.0\n    running_loss = 0.0\n    fields = ['epoch', 'epoch_loss', 'training_accuracy']\n    metrics = []\n    dataset_size = len(images)\n    \n    for epoch in range(epochs):\n        train_loss, train_correct = 0, 0\n        train_loop = tqdm(generate_batches(images, classes, len(images), batch_size = batch_size), total = dataset_size//batch_size\n        , position = 0, leave = True)\n        \n        for xbatch, ybatch in train_loop:\n            print(xbatch.shape)\n            print(ybatch.shape)\n            if device != \"cpu\":\n                 xbatch, ybatch = xbatch.to_device(\"gpu:0\"), ybatch.to_device(\"gpu:0\")\n\n            # since the cross entropy function expects the target classes to be in one-hot encoded format\n            ybatch_encoded = ivy.one_hot(ybatch, num_classes)\n            \n            # update model params\n            loss_probs, grads = ivy.execute_with_gradients(loss_fn, (model.v, model, xbatch, ybatch_encoded), ret_grad_idxs = [[0]], xs_grad_idxs = [[0]])\n            model.v = optimizer.step(model.v, grads['0'])\n            \n            batch_loss = ivy.to_numpy(loss_probs[0]).mean().item() # batch mean loss\n            epoch_loss +=  batch_loss * xbatch.shape[0]\n            train_correct += num_correct(loss_probs[1], ybatch)\n\n            train_loop.set_description(f'Epoch [{epoch+1:2d}/{epochs}]')\n            train_loop.set_postfix(\n                running_loss=batch_loss, accuracy_percentage=(train_correct/dataset_size)*100\n            )\n        epoch_loss = epoch_loss/dataset_size\n        training_accuracy = train_correct/dataset_size\n\n      \n        metrics.append([epoch, epoch_loss, training_accuracy])\n\n        train_loop.write(\n                f'\\nAverage training loss: {epoch_loss:.6f}, Train Correct: {train_correct}', end='\\n'\n            )\n\n    # write metrics for plotting\n    with open(f'/{model_name}_train_summary.csv', 'w') as f:\n        f = csv.writer(f)\n        f.writerow(fields)\n        f.writerows(metrics)\n\n\ntrain(images, classes, num_epochs, model, device, num_classes = num_classes, batch_size = batch_size)\n\nEpoch [ 1/20]: : 157it [01:16,  2.06it/s, accuracy_percentage=0.2, running_loss=0.29]\nEpoch [ 2/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.14, running_loss=0.12]\nEpoch [ 3/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=0.19, running_loss=0.0187]\nEpoch [ 4/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.36, running_loss=0.0324]\nEpoch [ 5/20]: : 157it [01:15,  2.07it/s, accuracy_percentage=0.6, running_loss=0.00456]\nEpoch [ 6/20]: : 157it [01:26,  1.82it/s, accuracy_percentage=0.6, running_loss=0.00277]\nEpoch [ 7/20]: : 157it [01:16,  2.05it/s, accuracy_percentage=0.81, running_loss=0.00175]\nEpoch [ 8/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.81, running_loss=0.00147]\nEpoch [ 9/20]: : 157it [01:14,  2.09it/s, accuracy_percentage=1.06, running_loss=0.00128]\nEpoch [10/20]: : 157it [01:14,  2.10it/s, accuracy_percentage=1.29, running_loss=0.00112]\nEpoch [11/20]: : 157it [01:13,  2.12it/s, accuracy_percentage=1.45, running_loss=0.000989]\nEpoch [12/20]: : 157it [01:15,  2.07it/s, accuracy_percentage=1.68, running_loss=0.000873]\nEpoch [13/20]: : 157it [01:15,  2.08it/s, accuracy_percentage=1.77, running_loss=0.000774]\nEpoch [14/20]: : 157it [01:15,  2.09it/s, accuracy_percentage=1.92, running_loss=0.000688]\nEpoch [15/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=1.92, running_loss=0.000613]\nEpoch [16/20]: : 157it [01:13,  2.12it/s, accuracy_percentage=2.05, running_loss=0.000547]\nEpoch [17/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=2.18, running_loss=0.000488]\nEpoch [18/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=2.25, running_loss=0.000437]\nEpoch [19/20]: : 157it [01:14,  2.10it/s, accuracy_percentage=2.38, running_loss=0.000391]\nEpoch [20/20]: : 157it [01:19,  1.98it/s, accuracy_percentage=2.6, running_loss=0.000351]\n\n\n\nAverage training loss: 0.475401, Train Correct: 20\n\nAverage training loss: 0.081436, Train Correct: 14\n\nAverage training loss: 0.029279, Train Correct: 19\n\nAverage training loss: 0.008382, Train Correct: 36\n\nAverage training loss: 0.003816, Train Correct: 60\n\nAverage training loss: 0.002179, Train Correct: 60\n\nAverage training loss: 0.001569, Train Correct: 81\n\nAverage training loss: 0.001235, Train Correct: 81\n\nAverage training loss: 0.001005, Train Correct: 106\n\nAverage training loss: 0.000837, Train Correct: 129\n\nAverage training loss: 0.000709, Train Correct: 145\n\nAverage training loss: 0.000606, Train Correct: 168\n\nAverage training loss: 0.000524, Train Correct: 177\n\nAverage training loss: 0.000455, Train Correct: 192\n\nAverage training loss: 0.000398, Train Correct: 192\n\nAverage training loss: 0.000350, Train Correct: 205\n\nAverage training loss: 0.000308, Train Correct: 218\n\nAverage training loss: 0.000273, Train Correct: 225\n\nAverage training loss: 0.000243, Train Correct: 238\n\nAverage training loss: 0.000216, Train Correct: 260\n\n\n\n\nPlotting the training metrics\n\ndef plot_summary(path):\n    data = pd.read_csv(path)\n\n    plt.style.use('seaborn-whitegrid')\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n    ax1.plot(data['epoch'], data['epoch_loss'], label='Train Loss')\n    ax2.plot(data['epoch'], data['training_accuracy'], label='Train Accuracy')\n\n    ax1.legend()\n    ax1.set_title('Running Loss', fontweight='bold')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.grid(True)\n\n    ax2.legend()\n    ax2.set_title('Running Accuracy', fontweight='bold')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.grid(True)\n\n    plt.tight_layout()\n    fig.savefig(f'summary_plots.png')\n    plt.show()\n    plt.close()\n\n\nplot_summary(f'/{model_name}_train_summary.csv')\n\n\n\n\n\n\nSave the trained Model\n\nmodel.save_weights('model_params/ivynet_weights.hdf5')"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html",
    "href": "wip/basic_operations_with_ivy.html",
    "title": "Basic Operations with Ivy",
    "section": "",
    "text": "!pip install ivy-core\n!pip install torch\n!pip install tensorflow\n!pip install jax\n!pip install dm-haiku\n!pip install numpy"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html#installs",
    "href": "wip/basic_operations_with_ivy.html#installs",
    "title": "Basic Operations with Ivy",
    "section": "",
    "text": "!pip install ivy-core\n!pip install torch\n!pip install tensorflow\n!pip install jax\n!pip install dm-haiku\n!pip install numpy"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html#imports",
    "href": "wip/basic_operations_with_ivy.html#imports",
    "title": "Basic Operations with Ivy",
    "section": "Imports üõÉ",
    "text": "Imports üõÉ\n\nimport ivy"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html#ivy-as-a-unified-ml-framework",
    "href": "wip/basic_operations_with_ivy.html#ivy-as-a-unified-ml-framework",
    "title": "Basic Operations with Ivy",
    "section": "Ivy as a Unified ML Framework üîÄ",
    "text": "Ivy as a Unified ML Framework üîÄ\nIvy is a unified machine learning framework that aims to provide a single interface for working with various machine learning libraries, such as Numpy, TensorFlow, PyTorch, and Jax. With Ivy, you can use the same code to build and train machine learning models, regardless of the underlying library being used. All you have to do is to change one line of code üòâ\n\nChange frameworks by one line of code ‚òù\nWith Ivy, you can define your data and operations just once and easily switch between different frameworks. To do this, simply write your operations in Ivy and use the ivy.set_framework() function to change the underlying framework.\nP.S. there are some more advanced ways of handling backend frameworks in Ivy, so check it out in our Deep Dive.\n\n\nNo need to worry about data types üé®\nFirstly, let‚Äôs set the backend to Tensorflow\n\nivy.set_framework('tensorflow')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n&lt;class 'tensorflow.python.framework.ops.EagerTensor'&gt;\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nNow let‚Äôs try exactly the same code, but change the used backend framework to Pytorch.\n\nivy.set_framework('torch')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n&lt;class 'torch.Tensor'&gt;\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nYou can see that defined Ivy arrays have either tf.Tensor or torch.Tensor types underneath it without any need to worry about their types.\n\n\nNo need to worry about framework differences üí±\nBy saying that framework can be changed by just one line of code, we really mean it üôÇ! By using Ivy as an ML framework, you do not need to worry about different function namings in different frameworks.\nTake a clip by value operator as an example. It performs the same operation across frameworks, but has different name and argument names. Numpy:\nnp.clip(a, a_min, a_max, out=None)\nTensforflow:\ntf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\nPytorch:\ntorch.clamp(input, min=None, max=None, *, out=None)\nJax:\njax.numpy.clip(a, a_min=None, a_max=None, out=None)\nHere are some examples\n\nimport tensorflow as tf\nt = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\nprint(tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1))\n\nimport numpy as np\nn = np.array([[-10., -1., 0.], [0., 2., 10.]])\nprint(np.clip(n, a_min=-1, a_max=1))\n\ntf.Tensor(\n[[-1. -1.  0.]\n [ 0.  1.  1.]], shape=(2, 3), dtype=float32)\n[[-1. -1.  0.]\n [ 0.  1.  1.]]\n\n\nIvy allows you not to worry about such things. Now let‚Äôs do the same solely in Ivy.\n\nivy.set_framework('numpy')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('torch')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\ntensor([[-1., -1.,  0.],\n        [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('tensorflow')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\n&lt;tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]], dtype=float32)&gt;\n\n\n\nivy.set_framework('jax')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\nDeviceArray([[-1., -1.,  0.],\n             [ 0.,  1.,  1.]], dtype=float32)\n\n\nAs you see, the only line that changed here is ivy.set_framework().\n\n\nUnifying them all! üç≤\nFinally, functions defined in Ivy are framework agnostic. In the example below we show how Ivy‚Äôs concatenation function is compatible with tensors from different frameworks. This is the same for all Ivy functions. They can accept tensors from any framework and return the correct result.\n\nimport jax.numpy as jnp\nimport tensorflow as tf\nimport numpy as np\nimport torch\n\nimport ivy\n\njax_concatted   = ivy.concat((jnp.ones((1,)), jnp.ones((1,))), -1)\ntf_concatted    = ivy.concat((tf.ones((1,)), tf.ones((1,))), -1)\nnp_concatted    = ivy.concat((np.ones((1,)), np.ones((1,))), -1)\ntorch_concatted = ivy.concat((torch.ones((1,)), torch.ones((1,))), -1)"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html#ivy-as-a-standalone-ml-framework",
    "href": "wip/basic_operations_with_ivy.html#ivy-as-a-standalone-ml-framework",
    "title": "Basic Operations with Ivy",
    "section": "Ivy as a standalone ML framework üåÄ",
    "text": "Ivy as a standalone ML framework üåÄ\nFinally, let‚Äôs train a simple two layer network using Ivy.\n\nSet Backend Framework\nYou can change the framework to any of the following: torch, tensforflow, or jax.\n\nivy.set_framework('torch')\n\n\n\nDefine Model\n\nclass MyModel(ivy.Module):\n    def __init__(self):\n        self.linear0 = ivy.Linear(3, 64)\n        self.linear1 = ivy.Linear(64, 1)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = ivy.relu(self.linear0(x))\n        return ivy.sigmoid(self.linear1(x))\n\n\n\nCreate Model\n\nmodel = MyModel()\n\n\n\nCreate Optimizer\n\noptimizer = ivy.Adam(1e-4)\n\n\n\nInput and Target\n\nx_in = ivy.array([1., 2., 3.])\ntarget = ivy.array([0.])\n\n\n\nLoss Function\n\ndef loss_fn(v):\n    out = model(x_in, v=v)\n    return ivy.reduce_mean((out - target)**2)[0]\n\n\n\nTraining Loop\n\nfor step in range(100):\n    loss, grads = ivy.execute_with_gradients(loss_fn, model.v)\n    model.v = optimizer.step(model.v, grads)\n    print('step {} loss {}'.format(step, ivy.to_numpy(loss).item()))\n\nprint('Finished training!')\n\nstep 0 loss 0.49040043354034424\nstep 1 loss 0.48975786566734314\nstep 2 loss 0.4892795979976654\nstep 3 loss 0.48886892199516296\nstep 4 loss 0.4884953498840332\nstep 5 loss 0.4881443977355957\nstep 6 loss 0.4878086447715759\nstep 7 loss 0.48748287558555603\nstep 8 loss 0.48716384172439575\nstep 9 loss 0.48684927821159363\nstep 10 loss 0.48653748631477356\nstep 11 loss 0.48622724413871765\nstep 12 loss 0.4859171509742737\nstep 13 loss 0.48560672998428345\nstep 14 loss 0.48529526591300964\nstep 15 loss 0.4849821627140045\nstep 16 loss 0.48466697335243225\nstep 17 loss 0.4843493402004242\nstep 18 loss 0.4840289056301117\nstep 19 loss 0.4837053418159485\nstep 20 loss 0.4833785891532898\nstep 21 loss 0.4830484390258789\nstep 22 loss 0.48271444439888\nstep 23 loss 0.48237672448158264\nstep 24 loss 0.48203518986701965\nstep 25 loss 0.48168954253196716\nstep 26 loss 0.4813397228717804\nstep 27 loss 0.4809857904911041\nstep 28 loss 0.48062753677368164\nstep 29 loss 0.48026490211486816\nstep 30 loss 0.479898065328598\nstep 31 loss 0.47952669858932495\nstep 32 loss 0.4791509211063385\nstep 33 loss 0.4787706732749939\nstep 34 loss 0.47838595509529114\nstep 35 loss 0.4779967665672302\nstep 36 loss 0.47760307788848877\nstep 37 loss 0.4772048890590668\nstep 38 loss 0.47680220007896423\nstep 39 loss 0.47639501094818115\nstep 40 loss 0.47598329186439514\nstep 41 loss 0.4755673110485077\nstep 42 loss 0.4751465618610382\nstep 43 loss 0.4747215211391449\nstep 44 loss 0.4742920398712158\nstep 45 loss 0.47385817766189575\nstep 46 loss 0.47341999411582947\nstep 47 loss 0.47297725081443787\nstep 48 loss 0.4725303053855896\nstep 49 loss 0.47207894921302795\nstep 50 loss 0.47162333130836487\nstep 51 loss 0.47116345167160034\nstep 52 loss 0.470699280500412\nstep 53 loss 0.47023090720176697\nstep 54 loss 0.4697583019733429\nstep 55 loss 0.46928152441978455\nstep 56 loss 0.46880054473876953\nstep 57 loss 0.4683155119419098\nstep 58 loss 0.4678264260292053\nstep 59 loss 0.46733325719833374\nstep 60 loss 0.46683603525161743\nstep 61 loss 0.4663347601890564\nstep 62 loss 0.4658295214176178\nstep 63 loss 0.465320348739624\nstep 64 loss 0.4648073613643646\nstep 65 loss 0.46429020166397095\nstep 66 loss 0.4637692868709564\nstep 67 loss 0.46324464678764343\nstep 68 loss 0.4627160429954529\nstep 69 loss 0.4621836841106415\nstep 70 loss 0.4616474211215973\nstep 71 loss 0.46110764145851135\nstep 72 loss 0.460563987493515\nstep 73 loss 0.4600166976451874\nstep 74 loss 0.45946577191352844\nstep 75 loss 0.45891112089157104\nstep 76 loss 0.45835286378860474\nstep 77 loss 0.4577910006046295\nstep 78 loss 0.45722562074661255\nstep 79 loss 0.45665669441223145\nstep 80 loss 0.4560841917991638\nstep 81 loss 0.4555082619190216\nstep 82 loss 0.45492875576019287\nstep 83 loss 0.45434585213661194\nstep 84 loss 0.45375964045524597\nstep 85 loss 0.4531698524951935\nstep 86 loss 0.4525766670703888\nstep 87 loss 0.45198020339012146\nstep 88 loss 0.4513803720474243\nstep 89 loss 0.4507772624492645\nstep 90 loss 0.4501707851886749\nstep 91 loss 0.4495610296726227\nstep 92 loss 0.4489481747150421\nstep 93 loss 0.44833192229270935\nstep 94 loss 0.4477125108242035\nstep 95 loss 0.44708991050720215\nstep 96 loss 0.44646409153938293\nstep 97 loss 0.44583529233932495\nstep 98 loss 0.4452032148838043\nstep 99 loss 0.44456806778907776\nFinished training!\n\n\n\nloss_fn(model.v)\n\ntensor(0.4439, grad_fn=&lt;SelectBackward0&gt;)\n\n\nWe hope that this short demo gives you a better understanding of basic Ivy functionality and got your interest in learning more about Ivy!"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html",
    "href": "wip/compilation_of_a_basic_function.html",
    "title": "Compilation of a Basic Function",
    "section": "",
    "text": "!pip install ivy-core\n!pip install numpy"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html#installs",
    "href": "wip/compilation_of_a_basic_function.html#installs",
    "title": "Compilation of a Basic Function",
    "section": "",
    "text": "!pip install ivy-core\n!pip install numpy"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html#imports",
    "href": "wip/compilation_of_a_basic_function.html#imports",
    "title": "Compilation of a Basic Function",
    "section": "Imports üõÉ",
    "text": "Imports üõÉ\n\nimport ivy\nimport numpy as np"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html#import-ivy-compiler",
    "href": "wip/compilation_of_a_basic_function.html#import-ivy-compiler",
    "title": "Compilation of a Basic Function",
    "section": "Import Ivy compiler",
    "text": "Import Ivy compiler\n\nimport ivy_compiler as ic"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html#function-compilation",
    "href": "wip/compilation_of_a_basic_function.html#function-compilation",
    "title": "Compilation of a Basic Function",
    "section": "Function compilation üõ†",
    "text": "Function compilation üõ†\n\nSet backend\n\nivy.set_backend(\"numpy\")\n\n\n\nSample input\n\nx = ivy.array([1., 2., 3.])\n\n\n\nDefine function to compile\nIvy can compile any function that produce numerical outputs. Compiler track values from inputs to outputs and produce a computational graph from those operations and will ignore anything that does not affect final output value. It will ignore all intermediate dummy variables, operations, and print statements.\n\ndef original_fn(x):\n    for _ in range(100000):\n        pass\n    y = (x + 3) * 4\n    z = (x ** y) - 3 * y\n    x = x**2\n    f = ivy.var(y)\n    k = np.cos(x)\n    m = ivy.sin(k)\n    o = np.tan(m)\n    return x\n\n\n\nCompile the function\n\ncomp_fn = ic.compile_graph(original_fn, x)\n\n\n\nCheck results\nGiven that function is compiled, its result can be compared to the original function.\n\nexpected_result = original_fn(x)\ncompiled_result = comp_fn(x)\n\nprint(expected_result)\nprint(compiled_result)\n\nAs you can see, both functions produce the same results, which is what we want üôÇ!"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html#compiling-simple-neural-network",
    "href": "wip/compilation_of_a_basic_function.html#compiling-simple-neural-network",
    "title": "Compilation of a Basic Function",
    "section": "Compiling simple neural network üß†",
    "text": "Compiling simple neural network üß†\nSimilarly to compiling functions, you can compile a neural network. The compilation works in exactly the same manner and will ignore all irrelevant opeations.\n\nDefine Model\n\nclass Network(ivy.Module):\n    def __init__(self):\n        self._layer = ivy.Linear(3, 3)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        return self._layer(x)\n\n\n\nCreate model\n\nnet = Network()\n\n\n\nDefine input\n\nx = ivy.array([1., 2., 3.])\n\n\n\nCompile network\n\ncompiled_net = ic.compile_graph(net, x)\n\n\n\nCheck results\n\nprint(net(x))\nprint(compiled_net(x))"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Welcome to Ivy‚Äôs tutorials webpage! Our goal is to provide you with a comprehensive learning experience on a variety of topics. We have organized our tutorials into three main sections to help you find exactly what you need.\n\nIn the Learn the basics section, you will find basic and to the point tutorials to help you get started with Ivy.\nThe Guides section includes more involved tutorials for those who want to dive deeper into the framework.\nFinally, in the Examples and Demos section, you will find start-to-finish projects and applications that showcase real-world applications of Ivy. Whether you‚Äôre a beginner or an advanced user, we‚Äôve got you covered!\n\n\n\n\n\n\n  \n    \n      Write Ivy code\n      \n      \n    \n  \n  \n    \n      Unify code\n      \n      \n    \n  \n  \n    \n      Compile code\n      \n      \n    \n  \n  \n    \n      Transpile code\n      \n      \n    \n  \n  \n    \n      Lazy vs Eager\n      \n      \n    \n  \n  \n    \n      How to use decorators\n      \n      \n    \n  \n  \n    \n      Transpile any library\n      \n      \n    \n  \n  \n    \n      Transpile any model\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      Transpiling a PyTorch model to build on top\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#tutorials-and-examples",
    "href": "index.html#tutorials-and-examples",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Welcome to Ivy‚Äôs tutorials webpage! Our goal is to provide you with a comprehensive learning experience on a variety of topics. We have organized our tutorials into three main sections to help you find exactly what you need.\n\nIn the Learn the basics section, you will find basic and to the point tutorials to help you get started with Ivy.\nThe Guides section includes more involved tutorials for those who want to dive deeper into the framework.\nFinally, in the Examples and Demos section, you will find start-to-finish projects and applications that showcase real-world applications of Ivy. Whether you‚Äôre a beginner or an advanced user, we‚Äôve got you covered!\n\n\n\n\n\n\n  \n    \n      Write Ivy code\n      \n      \n    \n  \n  \n    \n      Unify code\n      \n      \n    \n  \n  \n    \n      Compile code\n      \n      \n    \n  \n  \n    \n      Transpile code\n      \n      \n    \n  \n  \n    \n      Lazy vs Eager\n      \n      \n    \n  \n  \n    \n      How to use decorators\n      \n      \n    \n  \n  \n    \n      Transpile any library\n      \n      \n    \n  \n  \n    \n      Transpile any model\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      Transpiling a PyTorch model to build on top\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Quickstart",
    "section": "",
    "text": "Get up to speed with Ivy with a quick, general introduction of its features and capabilities!\nFirstly, let‚Äôs import the dependencies and define the torch function.\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\nNow, let‚Äôs unify the function!\nnormalize = ivy.unify(normalize)\nAnd that‚Äôs it! The normalize function can now be used with any ML framework. It‚Äôs as simple as that!\nSo, let‚Äôs give it a try!\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\n# numpy\nprint(normalize(x, mean, std))\n\n# jax\nimport jax.numpy as jnp\nx_ = jnp.array(x)\nmean_ = jnp.array(mean)\nstd_ = jnp.array(std)\nprint(normalize(x_, mean_, std_))\n\n# tensorflow\nimport tensorflow as tf\nx_ = tf.constant(x)\nmean_ = tf.constant(mean)\nstd_ = tf.constant(std)\nprint(normalize(x_, mean_, std_))\n\n# torch\nx_ = torch.tensor(x)\nmean_ = torch.tensor(mean)\nstd_ = torch.tensor(std)\nprint(normalize(x_, mean_, std_))\nWe can see that the new normalize function can operate with any ML framework. ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks."
  },
  {
    "objectID": "quickstart.html#round-up",
    "href": "quickstart.html#round-up",
    "title": "Quickstart",
    "section": "Round Up",
    "text": "Round Up\nThat‚Äôs it, you can now unify ML code! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to make our unified Ivy code run much more efficiently! ‚ö°"
  }
]