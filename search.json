[
  {
    "objectID": "unify_existing_code/0_building_blocks/0_2_transpile.html",
    "href": "unify_existing_code/0_building_blocks/0_2_transpile.html",
    "title": "",
    "section": "",
    "text": "In this example, we transpile the original normalize function from torch to jax in one line of code. This is a common use case, where there is one target framework in mind.\n\n\n\n\n\nUsing what we learnt in the previous two notebooks for Unify and Compile, the workflow for converting directly from torch to jax would be as follows, first unifying to ivy code, and then compiling to the jax backend:\n\nimport ivy\nimport torch\nivy.set_backend(\"jax\")\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.compile(ivy.unify(normalize))\n\nnormalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project.\nThis workflow is common, and so in order to avoid repeated calls to ivy.unify followed by ivy.compile, there is another convenience function ivy.transpile, which basically acts as a shorthand for this pair of function calls:\n\nnormalize = ivy.transpile(normalize)\n\nAgain, normalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project.\n\n\nThat‚Äôs it, you can now transpile code from one framework to another with one line of code! That concludes the collection of notebooks on the ‚ÄúBuilding Blocks‚Äù. However, there are still many other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. In the next collection of notebooks ‚ÄúThe Basics‚Äù, we‚Äôll be learning about the various different ways that ivy.unify, ivy.compile and ivy.transpile can be called, and what implications each of these have."
  },
  {
    "objectID": "unify_existing_code/0_building_blocks/0_1_compile.html",
    "href": "unify_existing_code/0_building_blocks/0_1_compile.html",
    "title": "",
    "section": "",
    "text": "In this example, we compile our simple unified ivy function normalize from the last notebook. We then show how this newly compiled normalize function exhibits much better runtime performance than the non-compiled version.\n\n\n\n\n\nFirstly, let‚Äôs pick up where we left off in the last notebook, with our unified normalize function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.unify(normalize)\n\nFor the purpose of illustration, we will use jax as our backend framework:\n\n# set ivy's backend to jax\nivy.set_backend(\"jax\")\n\n# Import jax numpy API\nimport jax.numpy as jnp\n\n# create random jax arrays for testing\nx = jnp.randon.uniform(size=10)\nmean = jnp.mean(x)\nstd = jnp.std(x)\n\nAs in the previous example, the unified function can be executed like so (in this case it will trigger lazy unification, see the Lazy vs Eager section for more details):\n\nnormalize(x, mean, std)\n\nWhen calling this function, all of ivy‚Äôs function wrapping is included in the call stack of normalize, which adds runtime overhead. In general, ivy.compile strips any arbitrary function down to its constituent functions in the functional API of the target framework. It will then also be compiled to machine-code if the target framework supports low-level compiling (via functions such as tf.function, torch.jit.script, torch.jit.trace, torch.compile, jax.jit etc.). The code can be compiled like so:\n\ncomp = ivy.compile(normalize)  # compiles to jax, due to ivy.set_backend\n\nThe compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\n\ncomp(x, mean, std)\n\nThe machine-code compilation can be turned off by setting the argument low_level = False, in which case it will simply return a chain of Python function in the functional API of the target framework (in this case JAX). This will still improve the runtime efficiency over the original un-compiled version due to the removal of all ivy wrapping overhead, but it will not be as runtime efficient as the low-level compiled version:\n\npartial_comp = ivy.compile(normalize, low_level=False)\n\nAgain, the compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\n\npartial_comp(x, mean, std)\n\nWith all lazy unification and compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:\n\nivy.time_function(normalize)(x, mean, std)\nivy.time_function(partial_comp)(x, mean, std)\nivy.time_function(comp)(x, mean, std)\n\nAs expected, we can see that the slowest is normalize, which includes all ivy wrapping overhead. Next is partial_comp which has no wrapping overhead but is still expressed entirely in Python, without compiling to low-level code. The fastest is comp because the wrapping overhead is removed and the function is compiled to low-level code for maximal efficiency.\n\n\nThat‚Äôs it, you can now compile ivy code for more efficient inference! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to transpile code from one framework to another in a single line of code üîÑ"
  },
  {
    "objectID": "unify_existing_code/0_building_blocks/0_0_unify.html",
    "href": "unify_existing_code/0_building_blocks/0_0_unify.html",
    "title": "",
    "section": "",
    "text": "In this example, we unify a simple torch function normalize. We then show how this newly unified normalize function can be used alongside any ML framework!\n\n\n\n\n\nFirstly, let‚Äôs import the dependencies and define the torch function.\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nNow, let‚Äôs unify the function!\n\nnormalize = ivy.unify(normalize)\n\nAnd that‚Äôs it! The normalize function can now be used with any ML framework. It‚Äôs as simple as that!\nSo, let‚Äôs give it a try!\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\n# numpy\nprint(normalize(x, mean, std))\n\n# jax\nimport jax.numpy as jnp\nx_ = jnp.array(x)\nmean_ = jnp.array(mean)\nstd_ = jnp.array(std)\nprint(normalize(x_, mean_, std_))\n\n# tensorflow\nimport tensorflow as tf\nx_ = tf.constant(x)\nmean_ = tf.constant(mean)\nstd_ = tf.constant(std)\nprint(normalize(x_, mean_, std_))\n\n# torch\nx_ = torch.tensor(x)\nmean_ = torch.tensor(mean)\nstd_ = torch.tensor(std)\nprint(normalize(x_, mean_, std_))\n\nWe can see that the new normalize function can operate with any ML framework. ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks.\n\n\nThat‚Äôs it, you can now unify ML code! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how to make our unified Ivy code run much more efficiently! ‚ö°"
  },
  {
    "objectID": "unify_existing_code/1_the_basics/1_2_as_a_decorator.html",
    "href": "unify_existing_code/1_the_basics/1_2_as_a_decorator.html",
    "title": "",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be called either as a function decorator or as a standalone function. All examples in the Building Blocks section and all previous examples in The Basics are called as standalone functions. In this section, we‚Äôll see how they can each be instead called as function decorators.\n\n\n\n\n\n\n\nFirstly, let‚Äôs create the dummy numpy arrays as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the ivy.unify function is called as a decorator.\n\nimport ivy\nimport torch\n\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.unify(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally. This argument can be passed when ivy.unify is used as a decorator.\n\n\n\nIn the example below, the ivy.compile function is also called as a decorator.\n\n@ivy.compile\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLikewise, the function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.compile(args=(x, mean, std))\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as to for specifying the target framework locally. This argument can be passed when ivy.compile is used as a decorator.\n\n\n\nIn the example below, the ivy.transpile function is called as a decorator.\n\n@ivy.transpile\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.transpile(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally, and to for specifying the target framework locally. These arguments can be passed when ivy.transpile is used as a decorator.\n\n\n\nThat‚Äôs it, you now know how ivy.unify, ivy.compile and ivy.transpile can all be used as function decorators! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be exploring the difference between dynamic vs static computation graphs!"
  },
  {
    "objectID": "unify_existing_code/1_the_basics/1_1_framework_selection.html",
    "href": "unify_existing_code/1_the_basics/1_1_framework_selection.html",
    "title": "",
    "section": "",
    "text": "The source and target frameworks for ivy.unify, ivy.compile and ivy.transpile can be: (a) inferred from the arguments and/or inspection of the function, (b) specified globally or (c) specified locally. All examples in the Building Blocks either infer the source and target frameworks or specify them globally (via ivy.set_backend). We‚Äôll explore these various options, and also explore which modes take priority. For these examples, all functions are called eagerly. Please go through the Lazy vs Eager notebook if you haven‚Äôt already.\n\n\n\n\n\n\n\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet‚Äôs also create the dummy data as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nThis time, let‚Äôs assume that our target framework is jax:\n\nimport jax.numpy as jnp\n\nx = jnp.array(x)\nmean = jnp.array(mean)\nstd = jnp.array(std)\n\nIn the example below, the source framework of torch is inferred from the function normalize.\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\n\nAs mentioned in the Unify notebook, ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic Ivy implementation, which is compatible with all frameworks.\nFor some functions, this would not be possible. Consider the example below:\n\ndef normalize_via_operators(x, mean, std):\n    return (x - mean) / std\n\nThere is no way to determine the source framework from this function via the inspection module. This code uses built-in operators only, which are compatible with all ML frameworks. You might therefore think ‚Äúthis is already unified‚Äù, but that‚Äôs not true. Every ML framework has its own unique rules for broadcasting shapes and data types for elementwise functions, which must all be taken into account when converting code to ivy.\nRather than inferring the framework, the framework can be specified locally as follows:\n\nnorm = ivy.unify(normalize_via_operators, args=(x, mean, std), from=\"torch\")\n\nNote that in all of the examples above, the arguments are in fact jax arrays. During function tracing, the jax arrays are converted to torch tensors automatically.\n\n\n\nIn the example below, the target framework of jax is inferred from the arguments.\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nHowever, if the Ivy function norm was purely generative (not consuming any arrays in the input), then this would not be possible. In such cases, we could set the target framework globally like so. If the type of the arguments conflicts with the globally set backend, then an error will be thrown.\n\nivy.set_backend(\"jax\")\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nFinally, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm_comp = ivy.compile(norm, args=(x, mean, std), to=\"jax\") # doesn't matter, jax specified locally\n\n\n\n\nAll consideration for both ivy.unify and ivy.compile are combined for ivy.transpile, which is effectively shorthand for the combination of these two functions (as explained in the Transpile section).\nIn the example below, the source framework of torch is inferred from the function normalize, and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std))\n\nIn the example below, the source framework is specified locally (would be necessary if transpiling normalize_via_operators for example) and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nIn the example below, the source framework is specified locally and the target framework of jax is specified globally. This might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"jax\")\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nAs with ivy.compile, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\nIn the example below, the source framework is specified locally and the target framework of jax is also specified locally. Again, this might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\", to=\"jax\") # doesn't matter, jax specified locally\n\n\n\n\nThat‚Äôs it, you now know the difference between inferring, locally specifying, and globally specifying source and target frameworks for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be exploring how these three functions can all be called as function decorators!"
  },
  {
    "objectID": "unify_existing_code/1_the_basics/1_0_lazy_vs_eager.html",
    "href": "unify_existing_code/1_the_basics/1_0_lazy_vs_eager.html",
    "title": "",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be performed either eagerly or lazily. All examples in the Building Blocks section are performed lazily, which means that the unification/compilation/transpilation process actually occurs during the first call of the returned function. This is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the ivy.unify, ivy.compile or ivy.transpile call itself, in which case the process is performed eagerly. We show some simple examples for each case below.\n\n\n\n\n\n\n\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet‚Äôs also create the dummy numpy arrays as before:\n\n# import NumPy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet‚Äôs assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the function is unified lazily, which means the first function call will execute slowly, as this is when the unification process actually occurs.\n\nnorm = ivy.unify(normalize)\nnorm(x, mean, std) # slow, lazy unification\nnorm(x, mean, std) # fast, unified on previous call\n\nHowever, in the following example the unification occurs eagerly, and both function calls will be fast:\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\nnorm(x, mean, std) # fast, unified at ivy.unify\nnorm(x, mean, std) # fast, unified at ivy.unify\n\n\n\n\nThe same is true for compiling. In the example below, the function is compiled lazily, which means the first function call will execute slowly, as this is when the compilation process actually occurs.\n\nnorm_comp = ivy.compile(norm)\nnorm_comp(x, mean, std) # slow, lazy compilation\nnorm_comp(x, mean, std) # fast, compiled on previous call\n\nHowever, in the following example the compilation occurs eagerly, and both function calls will be fast:\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile\n\n\n\n\nThe same is true for transpiling. In the example below, the function is transpiled lazily, which means the first function call will execute slowly, as this is when the transpilation process actually occurs.\n\nnorm_trans = ivy.transpile(normalize)\nnorm_trans(x, mean, std) # slow, lazy transpilation\nnorm_trans(x, mean, std) # fast, transpiled on previous call\n\nHowever, in the following example the transpilation occurs eagerly, and both function calls will be fast:\n\nnorm_trans = ivy.transpile(normalize, args=(x, mean, std))\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile\n\n\n\n\nThat‚Äôs it, you now know the difference between lazy vs eager execution for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you‚Äôre ready to unify ML code like a pro ü•∑. Next, we‚Äôll be learning how the frameworks are selected, either inferred from the inputs and the function, specified globally, or specified locally. We‚Äôll also learn what the implications are for each of these approaches!"
  },
  {
    "objectID": "unify_existing_code/1_the_basics/1_3_dynamic_vs_static.html",
    "href": "unify_existing_code/1_the_basics/1_3_dynamic_vs_static.html",
    "title": "",
    "section": "",
    "text": "The functions ivy.unify, ivy.compile and ivy.transpile can all be executed either in dynamic mode or static mode. In this demo, we explore how this mode is set, and what implications this has.\n\n\n\n\n\n\n\nDynamic mode means that Python dynamic control flow is included in the extracted computation graph. For example, if statements, for loops, while loops etc. would all be included if dynamic mode is set to True.\n\n\n\nStatic mode means the opposite, where Python dynamic control flow is not included in the extracted computation graph. if statements, for loops, while loops etc. would all be excluded if dynamic mode is set to False."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Demo Collections\n\n\n\n\n\n\n\n\n\n  \n    \n      0.0: Unify\n      \n      \n    \n  \n  \n    \n      0.1: Compile\n      \n      \n    \n  \n  \n    \n      0.2: Transpile\n      \n      \n    \n  \n  \n    \n      1.0: Lazy vs Eager\n      \n      \n    \n  \n  \n    \n      1.1: Framework Selection\n      \n      \n    \n  \n  \n    \n      1.2: As a Decorator\n      \n      \n    \n  \n  \n    \n      1.3: Dynamic vs Static\n      \n      \n    \n  \n  \n    \n      Basic Operations with Ivy\n      \n      \n    \n  \n  \n    \n      Compilation of a Basic Function\n      \n      \n    \n  \n  \n    \n      Deepmind PerceiverIO on GPU\n      \n      \n    \n  \n  \n    \n      Ivy as a Transpiler Introduction\n      \n      \n    \n  \n  \n    \n      \n      \n      \n    \n  \n  \n    \n      \n      \n      \n    \n  \n  \n    \n      \n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "temp_demos/compilation_of_a_basic_function.html",
    "href": "temp_demos/compilation_of_a_basic_function.html",
    "title": "",
    "section": "",
    "text": "!pip install ivy-core\n!pip install numpy\n\n\n\n\n\nimport ivy\nimport numpy as np\n\n\n\n\n\nimport ivy_compiler as ic\n\n\n\n\n\n\n\nivy.set_backend(\"numpy\")\n\n\n\n\n\nx = ivy.array([1., 2., 3.])\n\n\n\n\nIvy can compile any function that produce numerical outputs. Compiler track values from inputs to outputs and produce a computational graph from those operations and will ignore anything that does not affect final output value. It will ignore all intermediate dummy variables, operations, and print statements.\n\ndef original_fn(x):\n    for _ in range(100000):\n        pass\n    y = (x + 3) * 4\n    z = (x ** y) - 3 * y\n    x = x**2\n    f = ivy.var(y)\n    k = np.cos(x)\n    m = ivy.sin(k)\n    o = np.tan(m)\n    return x\n\n\n\n\n\ncomp_fn = ic.compile_graph(original_fn, x)\n\n\n\n\nGiven that function is compiled, its result can be compared to the original function.\n\nexpected_result = original_fn(x)\ncompiled_result = comp_fn(x)\n\nprint(expected_result)\nprint(compiled_result)\n\nAs you can see, both functions produce the same results, which is what we want üôÇ!\n\n\n\n\nSimilarly to compiling functions, you can compile a neural network. The compilation works in exactly the same manner and will ignore all irrelevant opeations.\n\n\n\nclass Network(ivy.Module):\n    def __init__(self):\n        self._layer = ivy.Linear(3, 3)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        return self._layer(x)\n\n\n\n\n\nnet = Network()\n\n\n\n\n\nx = ivy.array([1., 2., 3.])\n\n\n\n\n\ncompiled_net = ic.compile_graph(net, x)\n\n\n\n\n\nprint(net(x))\nprint(compiled_net(x))"
  },
  {
    "objectID": "temp_demos/basic_operations_with_ivy.html",
    "href": "temp_demos/basic_operations_with_ivy.html",
    "title": "",
    "section": "",
    "text": "!pip install ivy-core\n!pip install torch\n!pip install tensorflow\n!pip install jax\n!pip install dm-haiku\n!pip install numpy\n\n\n\n\n\nimport ivy\n\n\n\n\nIvy is a unified machine learning framework that aims to provide a single interface for working with various machine learning libraries, such as Numpy, TensorFlow, PyTorch, and Jax. With Ivy, you can use the same code to build and train machine learning models, regardless of the underlying library being used. All you have to do is to change one line of code üòâ\n\n\nWith Ivy, you can define your data and operations just once and easily switch between different frameworks. To do this, simply write your operations in Ivy and use the ivy.set_framework() function to change the underlying framework.\nP.S. there are some more advanced ways of handling backend frameworks in Ivy, so check it out in our Deep Dive.\n\n\n\nFirstly, let‚Äôs set the backend to Tensorflow\n\nivy.set_framework('tensorflow')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n<class 'tensorflow.python.framework.ops.EagerTensor'>\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nNow let‚Äôs try exactly the same code, but change the used backend framework to Pytorch.\n\nivy.set_framework('torch')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n<class 'torch.Tensor'>\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nYou can see that defined Ivy arrays have either tf.Tensor or torch.Tensor types underneath it without any need to worry about their types.\n\n\n\nBy saying that framework can be changed by just one line of code, we really mean it üôÇ! By using Ivy as an ML framework, you do not need to worry about different function namings in different frameworks.\nTake a clip by value operator as an example. It performs the same operation across frameworks, but has different name and argument names. Numpy:\nnp.clip(a, a_min, a_max, out=None)\nTensforflow:\ntf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\nPytorch:\ntorch.clamp(input, min=None, max=None, *, out=None)\nJax:\njax.numpy.clip(a, a_min=None, a_max=None, out=None)\nHere are some examples\n\nimport tensorflow as tf\nt = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\nprint(tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1))\n\nimport numpy as np\nn = np.array([[-10., -1., 0.], [0., 2., 10.]])\nprint(np.clip(n, a_min=-1, a_max=1))\n\ntf.Tensor(\n[[-1. -1.  0.]\n [ 0.  1.  1.]], shape=(2, 3), dtype=float32)\n[[-1. -1.  0.]\n [ 0.  1.  1.]]\n\n\nIvy allows you not to worry about such things. Now let‚Äôs do the same solely in Ivy.\n\nivy.set_framework('numpy')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('torch')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\ntensor([[-1., -1.,  0.],\n        [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('tensorflow')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]], dtype=float32)>\n\n\n\nivy.set_framework('jax')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\nDeviceArray([[-1., -1.,  0.],\n             [ 0.,  1.,  1.]], dtype=float32)\n\n\nAs you see, the only line that changed here is ivy.set_framework().\n\n\n\nFinally, functions defined in Ivy are framework agnostic. In the example below we show how Ivy‚Äôs concatenation function is compatible with tensors from different frameworks. This is the same for all Ivy functions. They can accept tensors from any framework and return the correct result.\n\nimport jax.numpy as jnp\nimport tensorflow as tf\nimport numpy as np\nimport torch\n\nimport ivy\n\njax_concatted   = ivy.concat((jnp.ones((1,)), jnp.ones((1,))), -1)\ntf_concatted    = ivy.concat((tf.ones((1,)), tf.ones((1,))), -1)\nnp_concatted    = ivy.concat((np.ones((1,)), np.ones((1,))), -1)\ntorch_concatted = ivy.concat((torch.ones((1,)), torch.ones((1,))), -1)\n\n\n\n\n\nFinally, let‚Äôs train a simple two layer network using Ivy.\n\n\nYou can change the framework to any of the following: torch, tensforflow, or jax.\n\nivy.set_framework('torch')\n\n\n\n\n\nclass MyModel(ivy.Module):\n    def __init__(self):\n        self.linear0 = ivy.Linear(3, 64)\n        self.linear1 = ivy.Linear(64, 1)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = ivy.relu(self.linear0(x))\n        return ivy.sigmoid(self.linear1(x))\n\n\n\n\n\nmodel = MyModel()\n\n\n\n\n\noptimizer = ivy.Adam(1e-4)\n\n\n\n\n\nx_in = ivy.array([1., 2., 3.])\ntarget = ivy.array([0.])\n\n\n\n\n\ndef loss_fn(v):\n    out = model(x_in, v=v)\n    return ivy.reduce_mean((out - target)**2)[0]\n\n\n\n\n\nfor step in range(100):\n    loss, grads = ivy.execute_with_gradients(loss_fn, model.v)\n    model.v = optimizer.step(model.v, grads)\n    print('step {} loss {}'.format(step, ivy.to_numpy(loss).item()))\n\nprint('Finished training!')\n\nstep 0 loss 0.49040043354034424\nstep 1 loss 0.48975786566734314\nstep 2 loss 0.4892795979976654\nstep 3 loss 0.48886892199516296\nstep 4 loss 0.4884953498840332\nstep 5 loss 0.4881443977355957\nstep 6 loss 0.4878086447715759\nstep 7 loss 0.48748287558555603\nstep 8 loss 0.48716384172439575\nstep 9 loss 0.48684927821159363\nstep 10 loss 0.48653748631477356\nstep 11 loss 0.48622724413871765\nstep 12 loss 0.4859171509742737\nstep 13 loss 0.48560672998428345\nstep 14 loss 0.48529526591300964\nstep 15 loss 0.4849821627140045\nstep 16 loss 0.48466697335243225\nstep 17 loss 0.4843493402004242\nstep 18 loss 0.4840289056301117\nstep 19 loss 0.4837053418159485\nstep 20 loss 0.4833785891532898\nstep 21 loss 0.4830484390258789\nstep 22 loss 0.48271444439888\nstep 23 loss 0.48237672448158264\nstep 24 loss 0.48203518986701965\nstep 25 loss 0.48168954253196716\nstep 26 loss 0.4813397228717804\nstep 27 loss 0.4809857904911041\nstep 28 loss 0.48062753677368164\nstep 29 loss 0.48026490211486816\nstep 30 loss 0.479898065328598\nstep 31 loss 0.47952669858932495\nstep 32 loss 0.4791509211063385\nstep 33 loss 0.4787706732749939\nstep 34 loss 0.47838595509529114\nstep 35 loss 0.4779967665672302\nstep 36 loss 0.47760307788848877\nstep 37 loss 0.4772048890590668\nstep 38 loss 0.47680220007896423\nstep 39 loss 0.47639501094818115\nstep 40 loss 0.47598329186439514\nstep 41 loss 0.4755673110485077\nstep 42 loss 0.4751465618610382\nstep 43 loss 0.4747215211391449\nstep 44 loss 0.4742920398712158\nstep 45 loss 0.47385817766189575\nstep 46 loss 0.47341999411582947\nstep 47 loss 0.47297725081443787\nstep 48 loss 0.4725303053855896\nstep 49 loss 0.47207894921302795\nstep 50 loss 0.47162333130836487\nstep 51 loss 0.47116345167160034\nstep 52 loss 0.470699280500412\nstep 53 loss 0.47023090720176697\nstep 54 loss 0.4697583019733429\nstep 55 loss 0.46928152441978455\nstep 56 loss 0.46880054473876953\nstep 57 loss 0.4683155119419098\nstep 58 loss 0.4678264260292053\nstep 59 loss 0.46733325719833374\nstep 60 loss 0.46683603525161743\nstep 61 loss 0.4663347601890564\nstep 62 loss 0.4658295214176178\nstep 63 loss 0.465320348739624\nstep 64 loss 0.4648073613643646\nstep 65 loss 0.46429020166397095\nstep 66 loss 0.4637692868709564\nstep 67 loss 0.46324464678764343\nstep 68 loss 0.4627160429954529\nstep 69 loss 0.4621836841106415\nstep 70 loss 0.4616474211215973\nstep 71 loss 0.46110764145851135\nstep 72 loss 0.460563987493515\nstep 73 loss 0.4600166976451874\nstep 74 loss 0.45946577191352844\nstep 75 loss 0.45891112089157104\nstep 76 loss 0.45835286378860474\nstep 77 loss 0.4577910006046295\nstep 78 loss 0.45722562074661255\nstep 79 loss 0.45665669441223145\nstep 80 loss 0.4560841917991638\nstep 81 loss 0.4555082619190216\nstep 82 loss 0.45492875576019287\nstep 83 loss 0.45434585213661194\nstep 84 loss 0.45375964045524597\nstep 85 loss 0.4531698524951935\nstep 86 loss 0.4525766670703888\nstep 87 loss 0.45198020339012146\nstep 88 loss 0.4513803720474243\nstep 89 loss 0.4507772624492645\nstep 90 loss 0.4501707851886749\nstep 91 loss 0.4495610296726227\nstep 92 loss 0.4489481747150421\nstep 93 loss 0.44833192229270935\nstep 94 loss 0.4477125108242035\nstep 95 loss 0.44708991050720215\nstep 96 loss 0.44646409153938293\nstep 97 loss 0.44583529233932495\nstep 98 loss 0.4452032148838043\nstep 99 loss 0.44456806778907776\nFinished training!\n\n\n\nloss_fn(model.v)\n\ntensor(0.4439, grad_fn=<SelectBackward0>)\n\n\nWe hope that this short demo gives you a better understanding of basic Ivy functionality and got your interest in learning more about Ivy!"
  },
  {
    "objectID": "temp_demos/ivy_as_a_transpiler_intro.html",
    "href": "temp_demos/ivy_as_a_transpiler_intro.html",
    "title": "",
    "section": "",
    "text": "Head to our website\nCreate an account and generate an API Key\nSet theIVY_API_KEYenvironment variable to your generated key\n\n\n\n\n    def transpile(\n        self,\n        *objs,\n        to: Optional[str] = None,\n        args: Optional[tuple] = None,\n        kwargs: Optional[dict] = None,\n    ) -> Callable:\n        \"\"\"\n        objs\n            the functions, models or modules to be transpiled\n        to \n            the framework to be transpiled to\n        args\n            The positional arguments passed to the function for tracing\n        kwargs\n            The keyword arguments passed to the function for tracing\n        \"\"\"\n\nTranspile either functions, trainable models or importable python modules, with any number and combo permitted\nIf no ‚Äúobjs‚Äù are provided, the function returns a new transpilation function which receives only one object as input, making it usable as a decorator\nIf neither ‚Äúargs‚Äù nor ‚Äúkwargs‚Äù are specified, then the transpilation will occur lazily, upon the first call of the transpiled function, otherwise transpilation is eager\n\n\n\nTelemetry helps us better understand how users are interacting with the transpiler & how to make it better, we specifcally collect: 1. Invocations of the transpiler 2. graph representation of the transpiled object(s) 3. General machine information (e.g.¬†number of CPUs, GPUs, OS)\nadd option to opt-out?\n\n\n\n\n!pip install kornia ivy-core\n!wget https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/logo.png -O image.png\n\n\nfrom google.colab.patches import cv2_imshow\nimport cv2 \n\nimg = cv2.imread(\"image.png\")\ncv2_imshow(img)\n\n\n\n\n\nimport os\nimport ivy\nimport kornia\nimport jax.numpy as jnp\n\n# set the API key\nos.envrion[\"IVY_API_KEY\"] = \"\"\n\n# load image into jax \nimg = jnp.array(cv2.imread('image.png'))/255\nimg = jnp.expand_dims(jnp.transpose(img, (2, 0, 1)), 0)\n\n# transpile function lazily\ncanny = ivy.transpile(kornia.feature.canny,to='jax')\n\ncv2_imshow(canny(img))\n\n\nimport tensorflow as tf\n# load image in tensorflow\noriginal_img = tf.array(cv2.imread('image.png'))/255\noriginal_img = tf.expand_dims(tf.transpose(img, (2, 0, 1)), 0)\n\n\n#@title Run transpilation in eager/lazy mode { display-mode: \"form\" }\nimport torch \nmode = \"Lazy\" #@param [\"Eager\", \"Lazy\"]\nfn_args = (original_img,) if mode == \"Eager\" else None\n\n@ivy.transpile(to=\"tensorflow\",args=fn_args)\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n\n%%time\nnew_img = dilate_edges(original_img)\n\nYou selected Eager\n\n\n\ncv2_imshow(new_img)\n\n\n\n\n\n# transpile module lazily\nkornia = ivy.transpile(kornia,to=\"tensorflow\")\n\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n%%time\nnew_img = dilate_edges(original_img)\n\n\ncv2_imshow(img)\n\n\n\n\n\nimport haiku as hk\n\nnum_classes = 10\nhk_model = hk.nets.ResNet18(num_classes)\n\n# transpile to framework of your choice\ntorch_model = ivy.transpile(hk_model,to=torch.nn.Module)\nkeras_model = ivy.transpile(hk_model,to=tf.keras.Model)\n\n\n# visualize function counting as in the odsc talk"
  },
  {
    "objectID": "temp_demos/deepmind_perceiverio.html",
    "href": "temp_demos/deepmind_perceiverio.html",
    "title": "",
    "section": "",
    "text": "This notebook presents a demonstration of how to run the PercieverIO model on GPU using any of the following backends currently supported by ivy (numpy, torch, tensorflow and JAX). To find out more about ivy please feel free to checkout the ivy repo https://github.com/unifyai/ivy as well as the docs https://lets-unify.ai/ivy/index.html. Contributions are highly welcomed and you can interact with our community as well on discord https://discord.com/invite/G4aR9Q7DTN.\n\n\nNOTE: After running the cell below, you‚Äôll need to restart the runtime for the newly installed kernel to work.\n\n# Step1\n!printf \"#Step1 start Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n!printf \"10_wfp1U4rMzc20eiGNrdQa9V2S9ByJwV\" > ./FILE_ID ;\\\n printf \"wget  -O `cat ./FILE_ID`.tar.gz \\\"https://drive.google.com/uc?export=download&id=`cat ./FILE_ID`&confirm=t&\"|tee  ./FILE_ID_WGET_CMD  ;\\\n  printf \"`wget -q \"https://drive.google.com/uc?export=download&id=\\`cat ./FILE_ID\\`\" -O - | perl -pe 's/\\r*\\n//g' | perl -pe 's/^.*(uuid\\=[^\\\"]+)\\\".*$/${1}/g'`\"|tee -a ./FILE_ID_WGET_CMD  ;\\\n    printf \"&uc-download-link=Download anyway\\\"\" |tee -a ./FILE_ID_WGET_CMD ;\\\n      /bin/bash ./FILE_ID_WGET_CMD\n!printf \"#Step1 End Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n\n# Step2 Install python3.8 interpreter and python3-pip\n!printf \"Step2 Start python3.8 interpreter and install python3-pip\\n\"\n!sudo apt-get install -y python3.8\n!sudo apt-get install -yf python3-pip\n!printf \"Step2 End install python3.8 interpreter and python3-pip\\n\"\n\n# Step3 \n!printf \"#Step3 Start Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n!mkdir ./dist-packages\n!tar xvzf `cat ./FILE_ID`.tar.gz -C ./dist-packages\n!rm `cat ./FILE_ID`.tar.gz\n!printf \"#Step3 End Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n\n# Step4 \n!printf \"# Step4 Start symlink python3.7 google* package to unzipped python3.8 package directory\\n\"\n!rm -fr /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!ls -d /usr/local/lib/python3.7/dist-packages/google* \\\n  | perl -pe 's/^(.+)$/sudo ln -sf ${1} \\/content\\/dist-packages\\/usr\\/local\\/lib\\/python3.8\\/dist-packages\\//g' \\\n    |/bin/bash -\n!ls -la /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!printf \"# Step4 End symlink the python3.7 google* package to the unzipped python3.8 package directory\\n\"\n\n# Step5\n!printf \"# Step5 Start and replace the unzipped python3.8 package directory with a regular python3.8 package directory with a symlink\\n\"\n!rm -fr /usr/local/lib/python3.8/dist-packages\n!sudo ln -s /content/dist-packages/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!printf \"# Step5 End And symlink the unzipped python3.8 package directory to the regular python3.8 package directory\\n\"\n\n# Step6\n!printf \"# Step6 Start change python interpreter to 3.8\\n\"\n!printf \"python3.8 restart step 4\\n\"\n!sudo ln -sf `which python3.8` /etc/alternatives/python3\n!python --version\n!printf \"# Step6 End Change python interpreter to 3.8\\n\"\n\n# Run ipkykernel with Step7 3.8 python, name it \"engbjapanpython3.8\" and install the runtime separately\n!printf \"#Step7 Start Name engbjapanpython3.8 and start runtime (Python 3.8) ipykernel\\n\"\n!sudo python -m ipykernel install --name \"engbjapanpython3.8\" --user\n!printf \"#Step7 End Start the runtime (Python 3.8) ipykernel named engbjapanpython3.8\\n\"\n!printf \"When everything is finished, please execute ``Change runtime type and reconnect''\\n\"\n\n\nimport sys\nprint(\"User Current Version:-\", sys.version)\n\nUser Current Version:- 3.8.15 (default, Oct 12 2022, 19:14:39) \n[GCC 7.5.0]\n\n\n\n\n\n\n!git lfs clone --depth 1 https://github.com/unifyai/models.git\n\n\n!git clone --depth 1 https://github.com/unifyai/ivy.git \n\n\n\n\n\n!pip install models/ --upgrade\n\n\n!pip install ivy/\n\n\n# install the optional requirements to use JAX, tensorflow and torch backends\n!pip install -r ivy/requirements/optional.txt\n\n\n# Install jaxlib with with the corresponding CUDA version.\n!pip install  https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14+cuda11.cudnn805-cp38-none-manylinux2014_x86_64.whl\n\n\n\n\n\n\n#!/usr/local/bin/python3.8\n\nimport os\nimport ivy\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom ivy_models.transformers.helpers import FeedForward, PreNorm\nfrom ivy_models.transformers.perceiver_io import PerceiverIOSpec, PerceiverIO\n\n\n# Fetch sample images for inference\n!git clone https://github.com/ogbanugot/imagenet-samples.git\n\n#Fetch the class labels \n!git clone https://github.com/xmartlabs/caffeflow.git\n\n\ndef fetch_classes():\n  with open(\"/content/caffeflow/examples/imagenet/imagenet-classes.txt\", \"r\") as class_labels:\n    lines = class_labels.readlines()\n\n    classes = []\n    for l in lines:\n          classes.append(l.replace(\"\\n\", \"\"))\n    return classes\n\nclasses = fetch_classes()\nground_truth = [127, 31, 101, 32, 1]\npath_to_images = \"/content/imagenet-samples/\"\n\n\n#Helpers\n\ndef get_image(path, display=True, normalize=False):\n  img = Image.open(path).resize((224, 224))\n  if display:\n    return img\n\n  img = np.array(img)\n  img = img.astype(\"float32\")\n  img /= 255\n  if normalize:\n    mean = np.array([0.5, 0.5, 0.5])\n    std = np.array([0.5, 0.5, 0.5])\n    img[:, :] -= mean\n    img[:, :] /= std\n  return img\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    return ax\n\ndef show_results(path_to_images, preds, ground_truth):\n  # plot the images in the batch, along with predicted and true labels\n  fig = plt.figure(figsize=(25, 25))\n  idx = 0\n  for image in os.listdir(path_to_images):\n    if (image.endswith(\".JPEG\")):\n      this_dir = os.path.dirname(path_to_images)\n      image = get_image(os.path.join(this_dir, image))\n      ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n      imshow(image, ax)\n      ax.set_title(\"{} ({})\".format(classes[preds[idx]].split(',')[0], classes[ground_truth[idx]].split(',')[0]),\n                  color=(\"green\" if preds[idx]==ground_truth[idx] else \"red\"))\n      idx += 1\n    \n\n\n# Perceiver IO #\n# -------------#\ndef perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights):\n    # params\n    input_dim = 3\n    num_input_axes = 2\n    output_dim = 1000\n    network_depth = 8 if load_weights else 1\n    num_lat_att_per_layer = 6 if load_weights else 1\n\n    model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                        num_input_axes=num_input_axes,\n                                        output_dim=output_dim,\n                                        queries_dim=queries_dim,\n                                        network_depth=network_depth,\n                                        learn_query=learn_query,\n                                        query_shape=[1],\n                                        num_fourier_freq_bands=64,\n                                        num_lat_att_per_layer=num_lat_att_per_layer,\n                                        device=device))\n    \n    this_dir = os.path.dirname(\"models/\")\n    # maybe load weights\n    if load_weights:\n        weight_fpath = os.path.join(this_dir, 'ivy_models/transformers/pretrained_weights/perceiver_io.pickled')\n        assert os.path.isfile(weight_fpath)\n        # noinspection PyBroadException\n        try:\n            v = ivy.Container.from_disk_as_pickled(weight_fpath)\n            v = ivy.asarray(v)\n        except Exception:\n            # If git large-file-storage is not enabled (for example when testing in github actions workflow), then the\n            #  code will fail here. A placeholder file does exist, but the file cannot be loaded as pickled variables.\n            raise\n        # noinspection PyUnboundLocalVariable\n        model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                            num_input_axes=num_input_axes,\n                                            output_dim=output_dim,\n                                            queries_dim=queries_dim,\n                                            network_depth=network_depth,\n                                            learn_query=learn_query,\n                                            query_shape=[1],\n                                            max_fourier_freq=img_dims[0],\n                                            num_fourier_freq_bands=64,\n                                            num_lat_att_per_layer=num_lat_att_per_layer,\n                                            device=device), v=v)\n        \n    logits = []\n    for image in os.listdir(path_to_images):\n      if (image.endswith(\".JPEG\")):\n        # inputs\n        this_dir = os.path.dirname(\"/content/imagenet-samples/\")\n\n        img = get_image(os.path.join(this_dir, image), False, normalize_images)\n        img = ivy.array(img[None], dtype='float32', device=device)\n        queries = None if learn_query else ivy.random_uniform(shape=batch_shape + [1, queries_dim], device=device)\n        # output\n        output = model(img, queries=queries)\n        logits.append(ivy.argmax(output, axis=2).to_numpy()[0][0])\n\n    return logits\n    \n\n\n\n\nivy.set_backend(\"torch\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n\n\nivy.set_backend(\"tensorflow\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n2022-11-03 20:04:33.817437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n\n\n\n\n\n\n\n\n\nivy.set_backend(\"jax\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n\n\nivy.set_backend(\"numpy\")\ndevice = \"cpu\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend."
  }
]