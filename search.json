[
  {
    "objectID": "learn_the_basics/01_write_ivy_code.html",
    "href": "learn_the_basics/01_write_ivy_code.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Write Ivy code\nGet familiar with Ivy’s basic concepts and learn to write framework-agnostic code.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/09_write_a_model_using_ivy.html",
    "href": "learn_the_basics/09_write_a_model_using_ivy.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Write a model using Ivy\nDevelop a simple linear regression model using Ivy’s stateful API.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/03_compile_code.html",
    "href": "learn_the_basics/03_compile_code.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Turn your Ivy code into an efficient fully-functional graph, removing wrappers and unused parts of the code.\n\n \n\nFirstly, let’s pick up where we left off in the last notebook, with our unified normalize function:\n\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.unify(normalize, source=\"torch\")\n\nFor the purpose of illustration, we will use jax as our backend framework:\n\n# set ivy's backend to jax\nivy.set_backend(\"jax\")\n\n# Import jax\nimport jax\n\n# create random jax arrays for testing\nkey = jax.random.PRNGKey(42)\nx = jax.random.uniform(key, shape=(10,))\n\nAs in the previous example, the Ivy function can be executed like so (in this case it will trigger lazy unification, see the Lazy vs Eager section for more details):\n\nnormalize(x)\n\nivy.array([ 0.55563945, -0.65538704, -1.14150524,  1.46951997,  1.30220294,\n       -1.14739668, -0.57017946, -0.91962677,  0.51029003,  0.59644395])\n\n\nWhen calling this function, all of ivy’s function wrapping is included in the call stack of normalize, which adds runtime overhead. In general, ivy.compile strips any arbitrary function down to its constituent functions in the functional API of the target framework. The code can be compiled like so:\n\nivy.set_backend(\"jax\")\ncomp = ivy.compile(normalize)  # compiles to jax, due to ivy.set_backend\n\nThe compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\n\ncomp(x)\n\nArray([ 0.5556394 , -0.655387  , -1.1415051 ,  1.4695197 ,  1.3022028 ,\n       -1.1473966 , -0.5701794 , -0.91962665,  0.51028997,  0.5964439 ],      dtype=float32)\n\n\nWith all lazy compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:\n\n%%timeit\nnormalize(x)\n\n985 µs ± 6.76 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\ncomp(x)\n\n69.5 µs ± 1.24 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nAs expected, we can see that normalize is slower, as it includes all ivy wrapping overhead. On the other hand, comp has no wrapping overhead and it’s more efficient!\n\n\nThat’s it, you can now compile ivy code for more efficient inference! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how to transpile code from one framework to another in a single line of code 🔄"
  },
  {
    "objectID": "learn_the_basics/02_unify_code.html",
    "href": "learn_the_basics/02_unify_code.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Unify a simple torch function and use it alongside any ML framework!\n\n \n\nFirstly, let’s import the dependencies and define the torch function.\n\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\nNow, let’s unify the function!\n\nnormalize = ivy.unify(normalize, source=\"torch\")\n\nAnd that’s it! The normalize function can now be used with any ML framework. It’s as simple as that!\nSo, let’s give it a try!\n\n# import the frameworks\nimport numpy as np\nimport jax.numpy as jnp\nimport tensorflow as tf\n\n\n# create random numpy arrays for testing\nx = np.random.uniform(size=10).astype(np.float32)\nivy.set_backend(\"numpy\")\nprint(normalize(x))\n\n# jax\nx_ = jnp.array(x)\nivy.set_backend(\"jax\")\nprint(normalize(x_))\n\n# tensorflow\nx_ = tf.constant(x)\nivy.set_backend(\"tensorflow\")\nprint(normalize(x_))\n\n# torch\nx_ = torch.tensor(x)\nivy.set_backend(\"torch\")\nprint(normalize(x_))\n\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\nivy.array([ 0.82997245,  0.44733784, -0.32163444, -1.93330479, -0.52438271,\n       -0.20438017,  1.252316  ,  0.0827222 ,  1.26017165, -0.88881904])\n\n\nWe can see that the new normalize function can operate with any ML framework. ivy.unify converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks.\n\n\nThat’s it, you can now unify ML code! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how to make our unified Ivy code run much more efficiently! ⚡"
  },
  {
    "objectID": "learn_the_basics/04_transpile_code.html",
    "href": "learn_the_basics/04_transpile_code.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Convert a torch function to jax with just one line of code.\n\n \n\nUsing what we learnt in the previous two notebooks for Unify and Compile, the workflow for converting directly from torch to jax would be as follows, first unifying to ivy code, and then compiling to the jax backend:\n\nimport ivy\nimport torch\nivy.set_backend(\"jax\")\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n# convert the function to Ivy code\nivy_normalize = ivy.unify(normalize)\n\n# compile the Ivy code into jax functions\njax_normalize = ivy.compile(ivy_normalize)\n\nnormalize is now compiled to jax, ready to be integrated into your wider jax project.\nThis workflow is common, and so in order to avoid repeated calls to ivy.unify followed by ivy.compile, there is another convenience function ivy.transpile, which basically acts as a shorthand for this pair of function calls:\n\njax_normalize = ivy.transpile(normalize, source=\"torch\", to=\"jax\")\n\nAgain, normalize is now a jax function, ready to be integrated into your jax project.\n\nimport jax\n\nkey = jax.random.PRNGKey(42)\njax.config.update('jax_enable_x64', True)\nx = jax.random.uniform(key, shape=(10,))\n\nprint(jax_normalize(x))\n\n[-0.93968587  0.26075466 -0.22723222 -1.06276492 -0.47426987  1.72835908\n  1.71737559 -0.50411096 -0.65419174  0.15576624]\n\n\n\n\nThat’s it, you can now transpile code from one framework to another with one line of code! However, there are still other important topics to master before you’re ready to unify ML code like a pro 🥷. In the next notebooks we’ll be learning about the various different ways that ivy.unify, ivy.compile and ivy.transpile can be called, and what implications each of these have!"
  },
  {
    "objectID": "learn_the_basics/07_transpile_any_library.html",
    "href": "learn_the_basics/07_transpile_any_library.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Transpile any library\nTranspile the kornia library to jax with just one line of code.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/06_how_to_use_decorators.html",
    "href": "learn_the_basics/06_how_to_use_decorators.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Learn about the different ways to use compilation and transpilation functions.\n\n \n\n\n\nFirstly, let’s create the dummy numpy arrays as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.random.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet’s assume that our target framework is tensorflow:\n\nimport ivy\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\n\nIn the example below, the ivy.unify function is called as a decorator.\n\nimport torch\n\n@ivy.unify(source=\"torch\")\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # unification happens here\n\nivy.array([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])\n\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.unify(source=\"torch\", args=(x,))\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # already unified\n\nivy.array([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])\n\n\n\n\n\nIn the example below, the ivy.compile function is also called as a decorator. (Note that this is now an Ivy function!)\n\n@ivy.compile\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x, correction=1)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\n\nnormalize(x) # compilation happens here\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])>\n\n\nLikewise, the function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.compile(args=(x,))\ndef normalize(x):\n    mean = ivy.mean(x)\n    std = ivy.std(x, correction=1)\n    return ivy.divide(ivy.subtract(x, mean), std)\n\n\nnormalize(x) # already compiled\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])>\n\n\n\n\n\nIn the example below, the ivy.transpile function is called as a decorator.\n\n@ivy.transpile(source=\"torch\", to=\"tensorflow\")\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # transpilation happens here\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])>\n\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.transpile(source=\"torch\", to=\"tensorflow\", args=(x,))\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\n\nnormalize(x) # already transpiled\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-1.09422972, -0.46009917,  1.0881108 ,  1.86487021,  0.83629996,\n       -1.10654466, -0.89883457,  0.02893805,  0.15644584, -0.41495672])>\n\n\n\n\n\nThat’s it, you now know how ivy.unify, ivy.compile and ivy.transpile can all be used as function decorators! Next, we’ll start exploring the transpilation of more involved objects, beginning with libraries 📚"
  },
  {
    "objectID": "learn_the_basics/08_transpile_any_model.html",
    "href": "learn_the_basics/08_transpile_any_model.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Transpile any model\nTranspile a tensorflow model into a PyTorch module.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "learn_the_basics/05_lazy_vs_eager.html",
    "href": "learn_the_basics/05_lazy_vs_eager.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Understand the difference between eager and lazy compilation and transpilation.\n\n \n\nivy.unify, ivy.compile and ivy.transpile can all be performed either eagerly or lazily. All previous examples have been performed lazily, which means that the unification, compilation, or transpilation process actually occurs during the first call of the returned function.\nThis is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the ivy.unify, ivy.compile or ivy.transpile call itself, in which case the process is performed eagerly. We show some simple examples for each case below.\n\n\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x):\n    mean = torch.mean(x)\n    std = torch.std(x)\n    return torch.div(torch.sub(x, mean), std)\n\nAnd let’s also create the dummy numpy arrays as before:\n\n# import NumPy\nimport numpy as np\n\n# create random numpy array for testing\nx = np.random.uniform(size=10)\n\nLet’s assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\n\nIn the example below, the function is unified lazily, which means the first function call will execute slowly, as this is when the unification process actually occurs.\n\nnorm = ivy.unify(normalize, source=\"torch\")\nnorm(x) # slow, lazy unification\nnorm(x) # fast, unified on previous call\n\nivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])\n\n\nHowever, in the following example the unification occurs eagerly, and both function calls will be fast:\n\nivy.set_backend(\"tensorflow\")\nnorm = ivy.unify(normalize, source=\"torch\", args=(x,))\nnorm(x) # fast, unified at ivy.unify\nnorm(x) # fast, unified at ivy.unify\n\nivy.array([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])\n\n\n\n\n\nThe same is true for compiling. In the example below, the function is compiled lazily, which means the first function call will execute slowly, as this is when the compilation process actually occurs.\n\nnorm_comp = ivy.compile(norm)\nnorm_comp(x) # slow, lazy compilation\nnorm_comp(x) # fast, compiled on previous call\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>\n\n\nHowever, in the following example the compilation occurs eagerly, and both function calls will be fast:\n\nnorm_comp = ivy.compile(norm, args=(x,))\nnorm_comp(x) # fast, compiled at ivy.compile\nnorm_comp(x) # fast, compiled at ivy.compile\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>\n\n\n\n\n\nThe same is true for transpiling. In the example below, the function is transpiled lazily, which means the first function call will execute slowly, as this is when the transpilation process actually occurs.\n\nnorm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\")\nnorm_trans(x) # slow, lazy transpilation\nnorm_trans(x) # fast, transpiled on previous call\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>\n\n\nHowever, in the following example the transpilation occurs eagerly, and both function calls will be fast:\n\nnorm_trans = ivy.transpile(normalize, source=\"torch\", to=\"tensorflow\", args=(x,))\nnorm_trans(x) # fast, transpiled at ivy.transpile\nnorm_trans(x) # fast, transpiled at ivy.transpile\n\n<tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([-0.54320029,  1.30825614,  1.17176882,  1.14351968, -0.98934778,\n        0.82910388, -0.89044143, -0.71881472, -0.1666683 , -1.14417601])>\n\n\n\n\n\nThat’s it, you now know the difference between lazy vs eager execution for ivy.unify, ivy.compile and ivy.transpile! Next, we’ll be exploring how these three functions can all be called as function decorators!"
  },
  {
    "objectID": "quickstart.html",
    "href": "quickstart.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Get up to speed with Ivy with a quick, general introduction of its features and capabilities!\n\n \n\nFirstly, let’s import the dependencies and define the torch function.\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nNow, let’s unify the function!\n\nnormalize = ivy.unify(normalize)\n\nAnd that’s it! The normalize function can now be used with any ML framework. It’s as simple as that!\nSo, let’s give it a try!\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\n# numpy\nprint(normalize(x, mean, std))\n\n# jax\nimport jax.numpy as jnp\nx_ = jnp.array(x)\nmean_ = jnp.array(mean)\nstd_ = jnp.array(std)\nprint(normalize(x_, mean_, std_))\n\n# tensorflow\nimport tensorflow as tf\nx_ = tf.constant(x)\nmean_ = tf.constant(mean)\nstd_ = tf.constant(std)\nprint(normalize(x_, mean_, std_))\n\n# torch\nx_ = torch.tensor(x)\nmean_ = torch.tensor(mean)\nstd_ = torch.tensor(std)\nprint(normalize(x_, mean_, std_))\n\nWe can see that the new normalize function can operate with any ML framework. ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks.\n\n\nThat’s it, you can now unify ML code! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how to make our unified Ivy code run much more efficiently! ⚡"
  },
  {
    "objectID": "examples_and_demos/01_template.html",
    "href": "examples_and_demos/01_template.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "TO REPLACE: Title\nTO REPLACE: description\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "assets/template.html",
    "href": "assets/template.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "TO REPLACE: Title\nTO REPLACE: description\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Welcome to Ivy’s tutorials webpage! Our goal is to provide you with a comprehensive learning experience on a variety of topics. We have organized our tutorials into three main sections to help you find exactly what you need.\nIf you are in a rush, you can jump straight into the Quickstart, a quick and general introduction to Ivy’s features and capabilities!\n\nIn the Learn the basics section, you will find basic and to the point tutorials to help you get started with Ivy.\nThe Guides section includes more involved tutorials for those who want to dive deeper into the framework.\nFinally, in the Examples and Demos section, you will find start-to-finish projects and applications that showcase real-world applications of Ivy. Whether you’re a beginner or an advanced user, we’ve got you covered!\n\n\n\n\n\n\n  \n    \n      Write Ivy code\n      \n      \n    \n  \n  \n    \n      Unify code\n      \n      \n    \n  \n  \n    \n      Compile code\n      \n      \n    \n  \n  \n    \n      Transpile code\n      \n      \n    \n  \n  \n    \n      Lazy vs Eager\n      \n      \n    \n  \n  \n    \n      How to use decorators\n      \n      \n    \n  \n  \n    \n      Transpile any library\n      \n      \n    \n  \n  \n    \n      Transpile any model\n      \n      \n    \n  \n  \n    \n      Write a model using Ivy\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      Transpiling a PyTorch model to build on top\n      \n      \n    \n  \n  \n    \n      Transpiling a haiku model to build on top\n      \n      \n    \n  \n  \n    \n      Transpiling a TensorFlow model to build on top\n      \n      \n    \n  \n  \n    \n      Developing a convolutional network using Ivy\n      \n      \n    \n  \n\n\nNo matching items\n\n\n\n\n\n\n\n\n  \n    \n      TO REPLACE: Title\n      \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "wip/ivy_as_a_transpiler_intro.html",
    "href": "wip/ivy_as_a_transpiler_intro.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Head to our website\nCreate an account and generate an API Key\nSet theIVY_API_KEYenvironment variable to your generated key\n\n\n\n\n    def transpile(\n        self,\n        *objs,\n        to: Optional[str] = None,\n        args: Optional[tuple] = None,\n        kwargs: Optional[dict] = None,\n    ) -> Callable:\n        \"\"\"\n        objs\n            the functions, models or modules to be transpiled\n        to \n            the framework to be transpiled to\n        args\n            The positional arguments passed to the function for tracing\n        kwargs\n            The keyword arguments passed to the function for tracing\n        \"\"\"\n\nTranspile either functions, trainable models or importable python modules, with any number and combo permitted\nIf no “objs” are provided, the function returns a new transpilation function which receives only one object as input, making it usable as a decorator\nIf neither “args” nor “kwargs” are specified, then the transpilation will occur lazily, upon the first call of the transpiled function, otherwise transpilation is eager\n\n\n\nTelemetry helps us better understand how users are interacting with the transpiler & how to make it better, we specifcally collect: 1. Invocations of the transpiler 2. graph representation of the transpiled object(s) 3. General machine information (e.g. number of CPUs, GPUs, OS)\nadd option to opt-out?\n\n\n\n\n!pip install kornia ivy-core\n!wget https://raw.githubusercontent.com/unifyai/unifyai.github.io/master/img/externally_linked/logo.png -O image.png\n\n\nfrom google.colab.patches import cv2_imshow\nimport cv2 \n\nimg = cv2.imread(\"image.png\")\ncv2_imshow(img)\n\n\n\n\n\nimport os\nimport ivy\nimport kornia\nimport jax.numpy as jnp\n\n# set the API key\nos.envrion[\"IVY_API_KEY\"] = \"\"\n\n# load image into jax \nimg = jnp.array(cv2.imread('image.png'))/255\nimg = jnp.expand_dims(jnp.transpose(img, (2, 0, 1)), 0)\n\n# transpile function lazily\ncanny = ivy.transpile(kornia.feature.canny,to='jax')\n\ncv2_imshow(canny(img))\n\n\nimport tensorflow as tf\n# load image in tensorflow\noriginal_img = tf.array(cv2.imread('image.png'))/255\noriginal_img = tf.expand_dims(tf.transpose(img, (2, 0, 1)), 0)\n\n\n#@title Run transpilation in eager/lazy mode { display-mode: \"form\" }\nimport torch \nmode = \"Lazy\" #@param [\"Eager\", \"Lazy\"]\nfn_args = (original_img,) if mode == \"Eager\" else None\n\n@ivy.transpile(to=\"tensorflow\",args=fn_args)\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n\n%%time\nnew_img = dilate_edges(original_img)\n\nYou selected Eager\n\n\n\ncv2_imshow(new_img)\n\n\n\n\n\n# transpile module lazily\nkornia = ivy.transpile(kornia,to=\"tensorflow\")\n\ndef dilate_edges(img):\n  edges = kornia.filters.canny(img)[1]\n  return kornia.morphology.dilation(edges,torch.ones(7,7))\n\n%%time\nnew_img = dilate_edges(original_img)\n\n\ncv2_imshow(img)\n\n\n\n\n\nimport haiku as hk\n\nnum_classes = 10\nhk_model = hk.nets.ResNet18(num_classes)\n\n# transpile to framework of your choice\ntorch_model = ivy.transpile(hk_model,to=torch.nn.Module)\nkeras_model = ivy.transpile(hk_model,to=tf.keras.Model)\n\n\n# visualize function counting as in the odsc talk"
  },
  {
    "objectID": "wip/deepmind_perceiverio.html",
    "href": "wip/deepmind_perceiverio.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "This notebook presents a demonstration of how to run the PercieverIO model on GPU using any of the following backends currently supported by ivy (numpy, torch, tensorflow and JAX). To find out more about ivy please feel free to checkout the ivy repo https://github.com/unifyai/ivy as well as the docs https://lets-unify.ai/ivy/index.html. Contributions are highly welcomed and you can interact with our community as well on discord https://discord.com/invite/G4aR9Q7DTN.\n\n\nNOTE: After running the cell below, you’ll need to restart the runtime for the newly installed kernel to work.\n\n# Step1\n!printf \"#Step1 start Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n!printf \"10_wfp1U4rMzc20eiGNrdQa9V2S9ByJwV\" > ./FILE_ID ;\\\n printf \"wget  -O `cat ./FILE_ID`.tar.gz \\\"https://drive.google.com/uc?export=download&id=`cat ./FILE_ID`&confirm=t&\"|tee  ./FILE_ID_WGET_CMD  ;\\\n  printf \"`wget -q \"https://drive.google.com/uc?export=download&id=\\`cat ./FILE_ID\\`\" -O - | perl -pe 's/\\r*\\n//g' | perl -pe 's/^.*(uuid\\=[^\\\"]+)\\\".*$/${1}/g'`\"|tee -a ./FILE_ID_WGET_CMD  ;\\\n    printf \"&uc-download-link=Download anyway\\\"\" |tee -a ./FILE_ID_WGET_CMD ;\\\n      /bin/bash ./FILE_ID_WGET_CMD\n!printf \"#Step1 End Download python3.8packages(tar.gz) of Share File from GoogleDrive\\n\"\n\n# Step2 Install python3.8 interpreter and python3-pip\n!printf \"Step2 Start python3.8 interpreter and install python3-pip\\n\"\n!sudo apt-get install -y python3.8\n!sudo apt-get install -yf python3-pip\n!printf \"Step2 End install python3.8 interpreter and python3-pip\\n\"\n\n# Step3 \n!printf \"#Step3 Start Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n!mkdir ./dist-packages\n!tar xvzf `cat ./FILE_ID`.tar.gz -C ./dist-packages\n!rm `cat ./FILE_ID`.tar.gz\n!printf \"#Step3 End Create ./dist-packages in the current directory, extract python3.8packages (tar.gz), and delete\\n\"\n\n# Step4 \n!printf \"# Step4 Start symlink python3.7 google* package to unzipped python3.8 package directory\\n\"\n!rm -fr /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!ls -d /usr/local/lib/python3.7/dist-packages/google* \\\n  | perl -pe 's/^(.+)$/sudo ln -sf ${1} \\/content\\/dist-packages\\/usr\\/local\\/lib\\/python3.8\\/dist-packages\\//g' \\\n    |/bin/bash -\n!ls -la /content/dist-packages/usr/local/lib/python3.8/dist-packages/google*\n!printf \"# Step4 End symlink the python3.7 google* package to the unzipped python3.8 package directory\\n\"\n\n# Step5\n!printf \"# Step5 Start and replace the unzipped python3.8 package directory with a regular python3.8 package directory with a symlink\\n\"\n!rm -fr /usr/local/lib/python3.8/dist-packages\n!sudo ln -s /content/dist-packages/usr/local/lib/python3.8/dist-packages /usr/local/lib/python3.8/\n!printf \"# Step5 End And symlink the unzipped python3.8 package directory to the regular python3.8 package directory\\n\"\n\n# Step6\n!printf \"# Step6 Start change python interpreter to 3.8\\n\"\n!printf \"python3.8 restart step 4\\n\"\n!sudo ln -sf `which python3.8` /etc/alternatives/python3\n!python --version\n!printf \"# Step6 End Change python interpreter to 3.8\\n\"\n\n# Run ipkykernel with Step7 3.8 python, name it \"engbjapanpython3.8\" and install the runtime separately\n!printf \"#Step7 Start Name engbjapanpython3.8 and start runtime (Python 3.8) ipykernel\\n\"\n!sudo python -m ipykernel install --name \"engbjapanpython3.8\" --user\n!printf \"#Step7 End Start the runtime (Python 3.8) ipykernel named engbjapanpython3.8\\n\"\n!printf \"When everything is finished, please execute ``Change runtime type and reconnect''\\n\"\n\n\nimport sys\nprint(\"User Current Version:-\", sys.version)\n\nUser Current Version:- 3.8.15 (default, Oct 12 2022, 19:14:39) \n[GCC 7.5.0]\n\n\n\n\n\n\n!git lfs clone --depth 1 https://github.com/unifyai/models.git\n\n\n!git clone --depth 1 https://github.com/unifyai/ivy.git \n\n\n\n\n\n!pip install models/ --upgrade\n\n\n!pip install ivy/\n\n\n# install the optional requirements to use JAX, tensorflow and torch backends\n!pip install -r ivy/requirements/optional.txt\n\n\n# Install jaxlib with with the corresponding CUDA version.\n!pip install  https://storage.googleapis.com/jax-releases/cuda11/jaxlib-0.3.14+cuda11.cudnn805-cp38-none-manylinux2014_x86_64.whl\n\n\n\n\n\n\n#!/usr/local/bin/python3.8\n\nimport os\nimport ivy\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nfrom ivy_models.transformers.helpers import FeedForward, PreNorm\nfrom ivy_models.transformers.perceiver_io import PerceiverIOSpec, PerceiverIO\n\n\n# Fetch sample images for inference\n!git clone https://github.com/ogbanugot/imagenet-samples.git\n\n#Fetch the class labels \n!git clone https://github.com/xmartlabs/caffeflow.git\n\n\ndef fetch_classes():\n  with open(\"/content/caffeflow/examples/imagenet/imagenet-classes.txt\", \"r\") as class_labels:\n    lines = class_labels.readlines()\n\n    classes = []\n    for l in lines:\n          classes.append(l.replace(\"\\n\", \"\"))\n    return classes\n\nclasses = fetch_classes()\nground_truth = [127, 31, 101, 32, 1]\npath_to_images = \"/content/imagenet-samples/\"\n\n\n#Helpers\n\ndef get_image(path, display=True, normalize=False):\n  img = Image.open(path).resize((224, 224))\n  if display:\n    return img\n\n  img = np.array(img)\n  img = img.astype(\"float32\")\n  img /= 255\n  if normalize:\n    mean = np.array([0.5, 0.5, 0.5])\n    std = np.array([0.5, 0.5, 0.5])\n    img[:, :] -= mean\n    img[:, :] /= std\n  return img\n\n\ndef imshow(image, ax=None, title=None, normalize=True):\n    if ax is None:\n        fig, ax = plt.subplots()\n    ax.imshow(image)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n    ax.tick_params(axis='both', length=0)\n    ax.set_xticklabels('')\n    ax.set_yticklabels('')\n    return ax\n\ndef show_results(path_to_images, preds, ground_truth):\n  # plot the images in the batch, along with predicted and true labels\n  fig = plt.figure(figsize=(25, 25))\n  idx = 0\n  for image in os.listdir(path_to_images):\n    if (image.endswith(\".JPEG\")):\n      this_dir = os.path.dirname(path_to_images)\n      image = get_image(os.path.join(this_dir, image))\n      ax = fig.add_subplot(1, 5, idx+1, xticks=[], yticks=[])\n      imshow(image, ax)\n      ax.set_title(\"{} ({})\".format(classes[preds[idx]].split(',')[0], classes[ground_truth[idx]].split(',')[0]),\n                  color=(\"green\" if preds[idx]==ground_truth[idx] else \"red\"))\n      idx += 1\n    \n\n\n# Perceiver IO #\n# -------------#\ndef perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights):\n    # params\n    input_dim = 3\n    num_input_axes = 2\n    output_dim = 1000\n    network_depth = 8 if load_weights else 1\n    num_lat_att_per_layer = 6 if load_weights else 1\n\n    model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                        num_input_axes=num_input_axes,\n                                        output_dim=output_dim,\n                                        queries_dim=queries_dim,\n                                        network_depth=network_depth,\n                                        learn_query=learn_query,\n                                        query_shape=[1],\n                                        num_fourier_freq_bands=64,\n                                        num_lat_att_per_layer=num_lat_att_per_layer,\n                                        device=device))\n    \n    this_dir = os.path.dirname(\"models/\")\n    # maybe load weights\n    if load_weights:\n        weight_fpath = os.path.join(this_dir, 'ivy_models/transformers/pretrained_weights/perceiver_io.pickled')\n        assert os.path.isfile(weight_fpath)\n        # noinspection PyBroadException\n        try:\n            v = ivy.Container.from_disk_as_pickled(weight_fpath)\n            v = ivy.asarray(v)\n        except Exception:\n            # If git large-file-storage is not enabled (for example when testing in github actions workflow), then the\n            #  code will fail here. A placeholder file does exist, but the file cannot be loaded as pickled variables.\n            raise\n        # noinspection PyUnboundLocalVariable\n        model = PerceiverIO(PerceiverIOSpec(input_dim=input_dim,\n                                            num_input_axes=num_input_axes,\n                                            output_dim=output_dim,\n                                            queries_dim=queries_dim,\n                                            network_depth=network_depth,\n                                            learn_query=learn_query,\n                                            query_shape=[1],\n                                            max_fourier_freq=img_dims[0],\n                                            num_fourier_freq_bands=64,\n                                            num_lat_att_per_layer=num_lat_att_per_layer,\n                                            device=device), v=v)\n        \n    logits = []\n    for image in os.listdir(path_to_images):\n      if (image.endswith(\".JPEG\")):\n        # inputs\n        this_dir = os.path.dirname(\"/content/imagenet-samples/\")\n\n        img = get_image(os.path.join(this_dir, image), False, normalize_images)\n        img = ivy.array(img[None], dtype='float32', device=device)\n        queries = None if learn_query else ivy.random_uniform(shape=batch_shape + [1, queries_dim], device=device)\n        # output\n        output = model(img, queries=queries)\n        logits.append(ivy.argmax(output, axis=2).to_numpy()[0][0])\n\n    return logits\n    \n\n\n\n\nivy.set_backend(\"torch\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n\n\nivy.set_backend(\"tensorflow\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n2022-11-03 20:04:33.817437: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n\n\n\n\n\n\n\n\n\nivy.set_backend(\"jax\")\ndevice = \"gpu:0\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\n\n\n\n\n\n\n\nivy.set_backend(\"numpy\")\ndevice = \"cpu\"\nbatch_shape = [1]\nimg_dims = [224, 224]\nqueries_dim = 1024\nlearn_query = [True]\nload_weights = True\nnormalize_images=False\npreds = perceiver_io_img_classification(path_to_images, normalize_images, device, batch_shape, img_dims, queries_dim, learn_query,\n                                         load_weights)\n\nshow_results(path_to_images, preds, ground_truth)\n\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend.\nWARNING:root:NumPy does not support autograd, declaring a 'variable' is identical to declaring an 'array' when using numpy backend."
  },
  {
    "objectID": "wip/1_the_basics/1_2_as_a_decorator.html",
    "href": "wip/1_the_basics/1_2_as_a_decorator.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be called either as a function decorator or as a standalone function. All examples in the Building Blocks section and all previous examples in The Basics are called as standalone functions. In this section, we’ll see how they can each be instead called as function decorators.\n\n \n\n\n\nFirstly, let’s create the dummy numpy arrays as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet’s assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the ivy.unify function is called as a decorator.\n\nimport ivy\nimport torch\n\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.unify(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally. This argument can be passed when ivy.unify is used as a decorator.\n\n\n\nIn the example below, the ivy.compile function is also called as a decorator.\n\n@ivy.compile\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLikewise, the function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.compile(args=(x, mean, std))\n@ivy.unify\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as to for specifying the target framework locally. This argument can be passed when ivy.compile is used as a decorator.\n\n\n\nIn the example below, the ivy.transpile function is called as a decorator.\n\n@ivy.transpile\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe function can still be called either eagerly or lazily when calling as a decorator. The example above is lazy, whereas the example below is eager:\n\n@ivy.transpile(args=(x, mean, std))\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nThe same is true for all other arguments, such as from for specifying the source framework locally, and to for specifying the target framework locally. These arguments can be passed when ivy.transpile is used as a decorator.\n\n\n\nThat’s it, you now know how ivy.unify, ivy.compile and ivy.transpile can all be used as function decorators! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be exploring the difference between dynamic vs static computation graphs!"
  },
  {
    "objectID": "wip/1_the_basics/1_1_framework_selection.html",
    "href": "wip/1_the_basics/1_1_framework_selection.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "The source and target frameworks for ivy.unify, ivy.compile and ivy.transpile can be: (a) inferred from the arguments and/or inspection of the function, (b) specified globally or (c) specified locally. All examples in the Building Blocks either infer the source and target frameworks or specify them globally (via ivy.set_backend). We’ll explore these various options, and also explore which modes take priority. For these examples, all functions are called eagerly. Please go through the Lazy vs Eager notebook if you haven’t already.\n\n \n\n\n\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet’s also create the dummy data as before:\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nThis time, let’s assume that our target framework is jax:\n\nimport jax.numpy as jnp\n\nx = jnp.array(x)\nmean = jnp.array(mean)\nstd = jnp.array(std)\n\nIn the example below, the source framework of torch is inferred from the function normalize.\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\n\nAs mentioned in the Unify notebook, ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic Ivy implementation, which is compatible with all frameworks.\nFor some functions, this would not be possible. Consider the example below:\n\ndef normalize_via_operators(x, mean, std):\n    return (x - mean) / std\n\nThere is no way to determine the source framework from this function via the inspection module. This code uses built-in operators only, which are compatible with all ML frameworks. You might therefore think “this is already unified”, but that’s not true. Every ML framework has its own unique rules for broadcasting shapes and data types for elementwise functions, which must all be taken into account when converting code to ivy.\nRather than inferring the framework, the framework can be specified locally as follows:\n\nnorm = ivy.unify(normalize_via_operators, args=(x, mean, std), from=\"torch\")\n\nNote that in all of the examples above, the arguments are in fact jax arrays. During function tracing, the jax arrays are converted to torch tensors automatically.\n\n\n\nIn the example below, the target framework of jax is inferred from the arguments.\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nHowever, if the Ivy function norm was purely generative (not consuming any arrays in the input), then this would not be possible. In such cases, we could set the target framework globally like so. If the type of the arguments conflicts with the globally set backend, then an error will be thrown.\n\nivy.set_backend(\"jax\")\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\n\nFinally, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm_comp = ivy.compile(norm, args=(x, mean, std), to=\"jax\") # doesn't matter, jax specified locally\n\n\n\n\nAll consideration for both ivy.unify and ivy.compile are combined for ivy.transpile, which is effectively shorthand for the combination of these two functions (as explained in the Transpile section).\nIn the example below, the source framework of torch is inferred from the function normalize, and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std))\n\nIn the example below, the source framework is specified locally (would be necessary if transpiling normalize_via_operators for example) and the target framework of jax is inferred from the arguments.\n\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nIn the example below, the source framework is specified locally and the target framework of jax is specified globally. This might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"jax\")\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\")\n\nAs with ivy.compile, the target framework can be provided locally. This will override any globally set backend, but again the arguments must be of the correct type in order to avoid errors.\nIn the example below, the source framework is specified locally and the target framework of jax is also specified locally. Again, this might be necessary if there are no array arguments for the function.\n\nivy.set_backend(\"tensorflow\") # a different global backend\nnorm = ivy.transpile(normalize, args=(x, mean, std), from=\"torch\", to=\"jax\") # doesn't matter, jax specified locally\n\n\n\n\nThat’s it, you now know the difference between inferring, locally specifying, and globally specifying source and target frameworks for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be exploring how these three functions can all be called as function decorators!"
  },
  {
    "objectID": "wip/1_the_basics/1_0_lazy_vs_eager.html",
    "href": "wip/1_the_basics/1_0_lazy_vs_eager.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "ivy.unify, ivy.compile and ivy.transpile can all be performed either eagerly or lazily. All examples in the Building Blocks section are performed lazily, which means that the unification, compilation, or transpilation process actually occurs during the first call of the returned function. This is because all three of these processes depend on function tracing, which requires function arguments to use for the tracing. Alternatively, the arguments can be provided during the ivy.unify, ivy.compile or ivy.transpile call itself, in which case the process is performed eagerly. We show some simple examples for each case below.\n\n \n\n\n\nConsider again this simple torch function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nLet’s also create the dummy numpy arrays as before:\n\n# import NumPy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\nLet’s assume that our target framework is tensorflow:\n\nimport tensorflow as tf\nivy.set_backend(\"tensorflow\")\n\nx = tf.constant(x)\nmean = tf.constant(mean)\nstd = tf.constant(std)\n\nIn the example below, the function is unified lazily, which means the first function call will execute slowly, as this is when the unification process actually occurs.\n\nnorm = ivy.unify(normalize)\nnorm(x, mean, std) # slow, lazy unification\nnorm(x, mean, std) # fast, unified on previous call\n\nHowever, in the following example the unification occurs eagerly, and both function calls will be fast:\n\nnorm = ivy.unify(normalize, args=(x, mean, std))\nnorm(x, mean, std) # fast, unified at ivy.unify\nnorm(x, mean, std) # fast, unified at ivy.unify\n\n\n\n\nThe same is true for compiling. In the example below, the function is compiled lazily, which means the first function call will execute slowly, as this is when the compilation process actually occurs.\n\nnorm_comp = ivy.compile(norm)\nnorm_comp(x, mean, std) # slow, lazy compilation\nnorm_comp(x, mean, std) # fast, compiled on previous call\n\nHowever, in the following example the compilation occurs eagerly, and both function calls will be fast:\n\nnorm_comp = ivy.compile(norm, args=(x, mean, std))\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile\nnorm_comp(x, mean, std) # fast, compiled at ivy.compile\n\n\n\n\nThe same is true for transpiling. In the example below, the function is transpiled lazily, which means the first function call will execute slowly, as this is when the transpilation process actually occurs.\n\nnorm_trans = ivy.transpile(normalize)\nnorm_trans(x, mean, std) # slow, lazy transpilation\nnorm_trans(x, mean, std) # fast, transpiled on previous call\n\nHowever, in the following example the transpilation occurs eagerly, and both function calls will be fast:\n\nnorm_trans = ivy.transpile(normalize, args=(x, mean, std))\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile\nnorm_trans(x, mean, std) # fast, transpiled at ivy.transpile\n\n\n\n\nThat’s it, you now know the difference between lazy vs eager execution for ivy.unify, ivy.compile and ivy.transpile! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how the frameworks are selected, either inferred from the inputs and the function, specified globally, or specified locally. We’ll also learn what the implications are for each of these approaches!"
  },
  {
    "objectID": "wip/1_the_basics/1_3_dynamic_vs_static.html",
    "href": "wip/1_the_basics/1_3_dynamic_vs_static.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "The functions ivy.unify, ivy.compile and ivy.transpile can all be executed either in dynamic mode or static mode. In this demo, we explore how this mode is set, and what implications this has.\n\n \n\n\n\nDynamic mode means that Python dynamic control flow is included in the extracted computation graph. For example, if statements, for loops, while loops etc. would all be included if dynamic mode is set to True.\n\n\n\nStatic mode means the opposite, where Python dynamic control flow is not included in the extracted computation graph. if statements, for loops, while loops etc. would all be excluded if dynamic mode is set to False."
  },
  {
    "objectID": "wip/3_models/3_1_stable_diffusion.html",
    "href": "wip/3_models/3_1_stable_diffusion.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "3.1: Stable Diffusion\nToDo: description"
  },
  {
    "objectID": "wip/3_models/3_0_perceiver.html",
    "href": "wip/3_models/3_0_perceiver.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "3.0: Perceiver\nToDo: description"
  },
  {
    "objectID": "wip/end_to_end_training_pipeline_in_ivy.html",
    "href": "wip/end_to_end_training_pipeline_in_ivy.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "End-to-End Training Pipeline in Ivy\nToDo: description\n\n \n\n\n# install the latest Ivy version for this purpose\n!pip install git+https://github.com/unifyai/ivy.git@master\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting git+https://github.com/unifyai/ivy.git@master\n  Cloning https://github.com/unifyai/ivy.git (to revision master) to /tmp/pip-req-build-04cmgfjn\n  Running command git clone --filter=blob:none --quiet https://github.com/unifyai/ivy.git /tmp/pip-req-build-04cmgfjn\n  Resolved https://github.com/unifyai/ivy.git to commit d42a654e71ad3b1235c757c79130b50f4c7cbfa2\n  Running command git submodule update --init --recursive -q\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (1.22.4)\nRequirement already satisfied: einops==0.4.1 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (0.4.1)\nRequirement already satisfied: psutil==5.9.1 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (5.9.1)\nRequirement already satisfied: termcolor==1.1.0 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (1.1.0)\nRequirement already satisfied: colorama==0.4.5 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (0.4.5)\nRequirement already satisfied: packaging==21.3 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (21.3)\nRequirement already satisfied: nvidia-ml-py<=11.495.46 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (11.495.46)\nRequirement already satisfied: flask==2.2.2 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (2.2.2)\nRequirement already satisfied: cython==0.29.33 in /usr/local/lib/python3.8/dist-packages (from ivy-core==1.1.9) (0.29.33)\nRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.8/dist-packages (from flask==2.2.2->ivy-core==1.1.9) (2.2.3)\nRequirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from flask==2.2.2->ivy-core==1.1.9) (6.0.0)\nRequirement already satisfied: click>=8.0 in /usr/local/lib/python3.8/dist-packages (from flask==2.2.2->ivy-core==1.1.9) (8.1.3)\nRequirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.8/dist-packages (from flask==2.2.2->ivy-core==1.1.9) (2.1.2)\nRequirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.8/dist-packages (from flask==2.2.2->ivy-core==1.1.9) (3.1.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging==21.3->ivy-core==1.1.9) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=3.6.0->flask==2.2.2->ivy-core==1.1.9) (3.15.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=3.0->flask==2.2.2->ivy-core==1.1.9) (2.1.2)\n\n\n\n\nImporting libraries\n\n# third party libararies\nimport ivy\n\n# built-in libraries\nimport os\nimport random\nimport csv\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\n\n\n! mkdir 'content/drive/MyDrive/Ivy/'\nos.chdir('content/drive/MyDrive/Ivy')\n\n\n\nLet’s build the pipeline with a Tensorflow backend\nOne can experiment with any other backend as well. Just add the following line at the start with the string name to be the framework you want.\n\nivy.set_backend(\"tensorflow\")\n\n\n\nWe are using MNIST dataset for this Tutorial\nWe have downloaded this data from kaggle. Here’s a medium article which will help you in preparing the dataset.\nLet’s set a global seed for randomized operations\n\nivy.seed(seed_value = 0)\n\n\n\nTemporary Dataset and Dynamic loader\nSince we don’t have the builder ready just yet, we will create three functions which help in generating the dataset, randomizing, and batchwise loading at training time.\nNote - We’re only using a small subset of the entire dataset for the purpose of this demo. Same goes for the number of epochs we train the model for.\n\ndef randomize_dataset(images, classes):\n    data = list(zip(images, classes))\n    random.shuffle(data)\n    images, classes = zip(*data)\n    return list(images), list(classes)\n\n \ndef create_dataset(folder, num_examples_per_class = 100):\n    img_array = []\n    class_name = []\n    for dir in os.listdir(folder):\n        for i, file in enumerate(os.listdir(os.path.join(folder, dir))):\n            if i >= num_examples_per_class:\n                continue\n            img_path = os.path.join(folder, dir, file)\n            image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n            image = ivy.array(image).astype('float32').expand_dims()\n            image /= 255\n            img_array.append(image) \n            class_name.append(dir)\n    \n    return randomize_dataset(img_array, class_name)\n\n\ndef generate_batches(images, classes, dataset_size, batch_size = 32):\n    targets={k: v for v, k in enumerate(np.unique(classes))}\n    y_train= [targets[classes[i]] for i in range(len(classes))]\n    if batch_size > dataset_size:\n        raise ivy.utils.exceptions.IvyError('Use a smaller batch size')\n    for idx in range(0, dataset_size, batch_size):\n        yield ivy.stack(images[idx:min(idx+batch_size, dataset_size)]), ivy.array(y_train[idx:min(idx+batch_size, dataset_size)])\n\n\n#choosing 1000 examples per class for this demo\nimages, classes = create_dataset('/content/gdrive/My Drive/Ivy Model/mnist/train',num_examples_per_class = 1000)\n\n\nprint(f'Number of Training Examples is -: {len(images)}')\n\nNumber of Training Examples is -: 10000\n\n\n\ntargets={k: v for v, k in enumerate(np.unique(classes))}\nprint(f\"Class labels -: {targets}\")\n\nClass labels -: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9}\n\n\n\nx_batch_instance, y = next(iter(generate_batches(images, classes, len(images))))\nprint(f\"Data is of the form -: {x_batch_instance.shape} (NCHW)\\n\")\nprint(y.shape[0], y)\n\nData is of the form -: (32, 1, 28, 28) (NCHW)\n32 ivy.array([4, 7, 9, 1, 9, 4, 9, 5, 5, 8, 7, 1, 5, 4, 6, 9, 5, 7, 8, 6, 5, 5,\n       2, 3, 5, 6, 3, 7, 3, 6, 7, 4], dev=gpu:0)\n\n\nIntialising some training parameters\n\n#params\n\noptimizer= ivy.Adam(1e-4)\nbatch_size = 64 \nnum_epochs = 20\nnum_classes = 10\n\n\n\nDefining the Ivy Network\nWe inherit from the ivy.Module class for creation of networks. This helps us with the forward pass and computation of the gradients. Note that some keyword arguments below are user-defined, and are purely for the purpose of building this model. You can find more information about the init method inside the docs.\n\nclass IvyNet(ivy.Module):\n    def __init__(self, h_w = (32, 32), input_channels = 3,  output_channels = 512, kernel_size = [3, 3], num_classes = 2, data_format = \"NCHW\", device = \"cpu\"):\n        self.extractor = ivy.Sequential(\n            ivy.Conv2D(input_channels, 6, [5, 5], 1,  \"SAME\", data_format = data_format),\n            ivy.GELU(),\n            ivy.Conv2D(6,  16,  [5, 5], 1,  \"SAME\", data_format = data_format),\n            ivy.GELU(),\n            ivy.Conv2D(16, output_channels, [5, 5],  1,  \"SAME\", data_format = data_format),\n            ivy.GELU()\n        )\n        \n        self.classifier = ivy.Sequential(\n            ivy.Linear(h_w[0]*h_w[1]*output_channels, 512), #since padding is same, this would be image_height x image_widht x output_channels\n            ivy.GELU(),\n            ivy.Linear(512, num_classes)\n        )\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = self.extractor(x)\n        x = ivy.flatten(x, start_dim = 1, end_dim = -1) #flatten all dims except batch dim\n        logits = self.classifier(x)\n        probs = ivy.softmax(logits)\n        return logits, probs\n\n# train the model on gpu if it's available\ndevice = \"cuda:0\" if ivy.gpu_is_available() else \"cpu\"\n\nmodel = IvyNet(h_w = (28, 28), input_channels = 1, output_channels = 120, kernel_size = [5,5], num_classes = num_classes, device = device)\nmodel_name = type(model).__name__.lower()\n\n\nprint(device, model_name)\n\ncuda:0 ivynet\n\n\n\n\nTraining Loop with utility functions\nThe train function is where we do the heavy lifting, and use the loss_fn to compute the gradients. num_correct is used for returning the correct set of predictions.\n\ndef num_correct(preds, labels):\n    return (preds.argmax() == labels).sum().to_numpy().item()\n\ndef loss_fn(params):\n    v, model, x, y = params\n    y_pred, probs = model(x)\n    return ivy.cross_entropy(y, probs), probs\n\ndef train(images, classes, epochs, model, device, num_classes = 10, batch_size = 32):\n\n    # training metrics\n    epoch_loss = 0.0\n    running_loss = 0.0\n    fields = ['epoch', 'epoch_loss', 'training_accuracy']\n    metrics = []\n    dataset_size = len(images)\n    \n    for epoch in range(epochs):\n        train_loss, train_correct = 0, 0\n        train_loop = tqdm(generate_batches(images, classes, len(images), batch_size = batch_size), total = dataset_size//batch_size\n        , position = 0, leave = True)\n        \n        for xbatch, ybatch in train_loop:\n            if device != \"cpu\":\n                 xbatch, ybatch = xbatch.to_device(\"gpu:0\"), ybatch.to_device(\"gpu:0\")\n\n            # since the cross entropy function expects the target classes to be in one-hot encoded format\n            ybatch_encoded = ivy.one_hot(ybatch, num_classes)\n            \n            # update model params\n            loss_probs, grads = ivy.execute_with_gradients(loss_fn, (model.v, model, xbatch, ybatch_encoded), ret_grad_idxs = [[0]], xs_grad_idxs = [[0]])\n            model.v = optimizer.step(model.v, grads['0'])\n            \n            batch_loss = ivy.to_numpy(loss_probs[0]).mean().item() # batch mean loss\n            epoch_loss +=  batch_loss * xbatch.shape[0]\n            train_correct += num_correct(loss_probs[1], ybatch)\n\n            train_loop.set_description(f'Epoch [{epoch+1:2d}/{epochs}]')\n            train_loop.set_postfix(\n                running_loss=batch_loss, accuracy_percentage=(train_correct/dataset_size)*100\n            )\n        epoch_loss = epoch_loss/dataset_size\n        training_accuracy = train_correct/dataset_size\n\n      \n        metrics.append([epoch, epoch_loss, training_accuracy])\n\n        train_loop.write(\n                f'\\nAverage training loss: {epoch_loss:.6f}, Train Correct: {train_correct}', end='\\n'\n            )\n\n    # write metrics for plotting\n    with open(f'/{model_name}_train_summary.csv', 'w') as f:\n        f = csv.writer(f)\n        f.writerow(fields)\n        f.writerows(metrics)\n\n\ntrain(images, classes, num_epochs, model, device, num_classes = num_classes, batch_size = batch_size)\n\nEpoch [ 1/20]: : 157it [01:16,  2.06it/s, accuracy_percentage=0.2, running_loss=0.29]\n\n\n\nAverage training loss: 0.475401, Train Correct: 20\n\n\nEpoch [ 2/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.14, running_loss=0.12]\n\n\n\nAverage training loss: 0.081436, Train Correct: 14\n\n\nEpoch [ 3/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=0.19, running_loss=0.0187]\n\n\n\nAverage training loss: 0.029279, Train Correct: 19\n\n\nEpoch [ 4/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.36, running_loss=0.0324]\n\n\n\nAverage training loss: 0.008382, Train Correct: 36\n\n\nEpoch [ 5/20]: : 157it [01:15,  2.07it/s, accuracy_percentage=0.6, running_loss=0.00456]\n\n\n\nAverage training loss: 0.003816, Train Correct: 60\n\n\nEpoch [ 6/20]: : 157it [01:26,  1.82it/s, accuracy_percentage=0.6, running_loss=0.00277]\n\n\n\nAverage training loss: 0.002179, Train Correct: 60\n\n\nEpoch [ 7/20]: : 157it [01:16,  2.05it/s, accuracy_percentage=0.81, running_loss=0.00175]\n\n\n\nAverage training loss: 0.001569, Train Correct: 81\n\n\nEpoch [ 8/20]: : 157it [01:14,  2.11it/s, accuracy_percentage=0.81, running_loss=0.00147]\n\n\n\nAverage training loss: 0.001235, Train Correct: 81\n\n\nEpoch [ 9/20]: : 157it [01:14,  2.09it/s, accuracy_percentage=1.06, running_loss=0.00128]\n\n\n\nAverage training loss: 0.001005, Train Correct: 106\n\n\nEpoch [10/20]: : 157it [01:14,  2.10it/s, accuracy_percentage=1.29, running_loss=0.00112]\n\n\n\nAverage training loss: 0.000837, Train Correct: 129\n\n\nEpoch [11/20]: : 157it [01:13,  2.12it/s, accuracy_percentage=1.45, running_loss=0.000989]\n\n\n\nAverage training loss: 0.000709, Train Correct: 145\n\n\nEpoch [12/20]: : 157it [01:15,  2.07it/s, accuracy_percentage=1.68, running_loss=0.000873]\n\n\n\nAverage training loss: 0.000606, Train Correct: 168\n\n\nEpoch [13/20]: : 157it [01:15,  2.08it/s, accuracy_percentage=1.77, running_loss=0.000774]\n\n\n\nAverage training loss: 0.000524, Train Correct: 177\n\n\nEpoch [14/20]: : 157it [01:15,  2.09it/s, accuracy_percentage=1.92, running_loss=0.000688]\n\n\n\nAverage training loss: 0.000455, Train Correct: 192\n\n\nEpoch [15/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=1.92, running_loss=0.000613]\n\n\n\nAverage training loss: 0.000398, Train Correct: 192\n\n\nEpoch [16/20]: : 157it [01:13,  2.12it/s, accuracy_percentage=2.05, running_loss=0.000547]\n\n\n\nAverage training loss: 0.000350, Train Correct: 205\n\n\nEpoch [17/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=2.18, running_loss=0.000488]\n\n\n\nAverage training loss: 0.000308, Train Correct: 218\n\n\nEpoch [18/20]: : 157it [01:13,  2.13it/s, accuracy_percentage=2.25, running_loss=0.000437]\n\n\n\nAverage training loss: 0.000273, Train Correct: 225\n\n\nEpoch [19/20]: : 157it [01:14,  2.10it/s, accuracy_percentage=2.38, running_loss=0.000391]\n\n\n\nAverage training loss: 0.000243, Train Correct: 238\n\n\nEpoch [20/20]: : 157it [01:19,  1.98it/s, accuracy_percentage=2.6, running_loss=0.000351]\n\n\n\nAverage training loss: 0.000216, Train Correct: 260\n\n\n\n\n\n\n\nPlotting the training metrics\n\ndef plot_summary(path):\n    data = pd.read_csv(path)\n\n    plt.style.use('seaborn-whitegrid')\n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n    ax1.plot(data['epoch'], data['epoch_loss'], label='Train Loss')\n    ax2.plot(data['epoch'], data['training_accuracy'], label='Train Accuracy')\n\n    ax1.legend()\n    ax1.set_title('Running Loss', fontweight='bold')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Loss')\n    ax1.grid(True)\n\n    ax2.legend()\n    ax2.set_title('Running Accuracy', fontweight='bold')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Accuracy')\n    ax2.grid(True)\n\n    plt.tight_layout()\n    fig.savefig(f'summary_plots.png')\n    plt.show()\n    plt.close()\n\n\nplot_summary(f'/{model_name}_train_summary.csv')\n\n\n\n\n\n\nSave the trained Model\n\nmodel.save_weights('model_params/ivynet_weights.hdf5')"
  },
  {
    "objectID": "wip/hf_tensorflow_deit.html",
    "href": "wip/hf_tensorflow_deit.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "This is an example of compilation (using ivy) of DeiT model from Huggingface implemented with tensorflow.\n\n \n\nThe model can be found here.\n\nfrom transformers import DeiTImageProcessor, TFDeiTForImageClassification\nimport torch\nimport tensorflow as tf\nfrom PIL import Image\nimport requests\nimport numpy as np\n\nThis image from cocodataset will be used as an input. First it should be preprocessed using DeiTImageProcessor\n\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimage\n\n\n\n\n\n# note: we are loading a TFDeiTForImageClassificationWithTeacher from the hub here,\n# so the head (layers like 'distillation_classifier', 'cls_classifier') will be randomly initialized, hence the predictions will be random.\n\n# To be able to reproduce, lets set the random seed.\ntf.keras.utils.set_random_seed(3)\n\n\nimage_processor = DeiTImageProcessor.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\nmodel = TFDeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\ninputs = image_processor(images=image, return_tensors=\"tf\")\noutputs_from_original_model = model(**inputs)\nlogits = outputs_from_original_model.logits\n# model predicts one of the 1000 ImageNet classes\npredicted_class_idx = tf.math.argmax(logits, axis=-1)[0]\nprint(\"Predicted class:\", model.config.id2label[int(predicted_class_idx)])\n\nSome layers from the model checkpoint at facebook/deit-base-distilled-patch16-224 were not used when initializing TFDeiTForImageClassification: ['distillation_classifier', 'cls_classifier']\n- This IS expected if you are initializing TFDeiTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDeiTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\nPredicted class: ptarmigan\n\n\nLet’s install ivy from github\n\n!rm -rf ivy\n!git clone --branch master https://github.com/unifyai/ivy.git --single-branch\n!cd ivy && pip install -e .\n\n\nimport ivy\nivy.set_backend(\"tensorflow\")\nfrom ivy import ModuleConverters as mc\n\nConverts keras module to ivy module. This takes some time\n\nivy_model = mc.from_keras_module(model)\n\nCompile the module with ivy.compile\n\ncompiled_func, graph=ivy.compile(ivy_model, **inputs, return_graph=True)\n\n\nivy_outputs = ivy_model(**inputs)\n\n\ncompiled_outputs = compiled_func(**inputs)\n\n\n# Check if outputs are the same\nassert np.allclose(outputs_from_original_model.logits, ivy_outputs.logits), \"Error, outputs diverge\"\nassert np.allclose(outputs_from_original_model.logits, compiled_outputs.logits), \"Error, outputs diverge\"\n\nBelow, all functions which will be executed in order as they are printed.\n\nprint([fn.__name__ for fn in graph._all_functions])\n\n['convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'transpose_v2', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convolution_v2', 'bias_add', 'reshape', 'concat', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'identity', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'reshape', 'transpose_v2', 'matmul', 'divide', 'binary_op_wrapper', 'softmax_v2', 'convert_to_tensor_v2_with_dispatch', 'identity', 'matmul', 'transpose_v2', 'reshape', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'gelu', 'convert_to_tensor_v2_with_dispatch', 'convert_to_tensor_v2_with_dispatch', 'tensordot', 'bias_add', 'convert_to_tensor_v2_with_dispatch', 'identity', 'binary_op_wrapper', 'convert_to_tensor_v2_with_dispatch', 'cast', 'moments_v2', 'batch_normalization', 'cast', '_slice_helper', 'convert_to_tensor_v2_with_dispatch', 'matmul', 'bias_add']\n\n\n\n\n\ngraph.show(\n    save_to_disk=True,\n    fname='deit'\n)\n\nLet’s compare execution times for models. Compiled module/function usually run faster due to optimization by ivy’s graph compiler\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: ivy_model(**inputs), number=N)\nprint(res/N,'ms')\n\n0.12265048989200113 ms\n\n\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: compiled_func(**inputs), number=N)\nprint(res/N,'ms')\n\n0.11038777417100028 ms\n\n\n\nimport timeit\nN = 1000\nres = timeit.timeit(lambda: model(**inputs), number=N)\nprint(res/N,'ms')\n\n0.1167045795539998 ms"
  },
  {
    "objectID": "wip/2_libraries/2_0_kornia.html",
    "href": "wip/2_libraries/2_0_kornia.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "2.0: Kornia\nToDo: description"
  },
  {
    "objectID": "wip/resnet_18.html",
    "href": "wip/resnet_18.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Resnet 18\nToDo: description\n\n \n\nLets first install Ivy (pip install ivy-core) and compatable version of matplotlib to be able to see generated graph\n\n!pip install ivy-core\n!pip install matplotlib==3.5.2\n!pip install networkx==2.8.4\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting ivy-core\n  Downloading ivy_core-1.1.10-py3-none-any.whl (228 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.8/228.8 KB 19.0 MB/s eta 0:00:00\nCollecting einops\n  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.6/41.6 KB 6.0 MB/s eta 0:00:00\nRequirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from ivy-core) (5.4.8)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from ivy-core) (2.1.1)\nCollecting nvidia-ml-py3\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... done\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from ivy-core) (1.21.6)\nBuilding wheels for collected packages: nvidia-ml-py3\n  Building wheel for nvidia-ml-py3 (setup.py) ... done\n  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19190 sha256=241af6b4a51197474b0da3ee7bfa32d847756c8f0d93b51448655d6458312714\n  Stored in directory: /root/.cache/pip/wheels/b9/b1/68/cb4feab29709d4155310d29a421389665dcab9eb3b679b527b\nSuccessfully built nvidia-ml-py3\nInstalling collected packages: nvidia-ml-py3, einops, ivy-core\nSuccessfully installed einops-0.6.0 ivy-core-1.1.10 nvidia-ml-py3-7.352.0\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting matplotlib==3.5.2\n  Downloading matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 82.1 MB/s eta 0:00:00\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (0.11.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (21.3)\nCollecting fonttools>=4.22.0\n  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 KB 69.7 MB/s eta 0:00:00\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (7.1.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (3.0.9)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (1.21.6)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (1.4.4)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.5.2) (2.8.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.2) (1.15.0)\nInstalling collected packages: fonttools, matplotlib\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.2.2\n    Uninstalling matplotlib-3.2.2:\n      Successfully uninstalled matplotlib-3.2.2\nSuccessfully installed fonttools-4.38.0 matplotlib-3.5.2\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting networkx==2.8.4\n  Downloading networkx-2.8.4-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 65.0 MB/s eta 0:00:00\nInstalling collected packages: networkx\n  Attempting uninstall: networkx\n    Found existing installation: networkx 2.8.8\n    Uninstalling networkx-2.8.8:\n      Successfully uninstalled networkx-2.8.8\nSuccessfully installed networkx-2.8.4\n\n\n\nimport ivy\nimport numpy as np\nimport timeit\nimport torch\nimport urllib\nfrom ivy import compile_graph\nfrom ivy import show_graph\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\n\nHere we load resnet18 pytorch model. Copy model to chosen device and convert the model into ivy (using ivy.to_ivy_module). Then, we compile the model (using compile_graph) and check if it is working as expected.\n\nivy.set_framework(\"torch\")\n# dev = 'cpu'\ndev = 'gpu:0'\nbatch_size = 1\nimage_dims = [64,64]\n\ntry:\n    model = torch.hub.load(\n        \"pytorch/vision:v0.10.0\", \"resnet18\", pretrained=True\n    )\nexcept urllib.error.URLError:\n  pass\n\nnet = ivy.to_ivy_module(model.to('cuda:0' if dev == 'gpu:0' else 'cpu'))\n\nx0 = ivy.random_uniform(\n    low=0.0,\n    high=1.0,\n    shape=[batch_size] + [3] + image_dims,\n    dev_str=dev,\n)\nx1 = ivy.random_uniform(\n    low=0.0,\n    high=1.0,\n    shape=[batch_size] + [3] + image_dims,\n    dev_str=dev,\n)\n\ncomp_network, graph = compile_graph(net, x0, return_graph=True, time_chronological=False)\n\nshow_graph(\n    net,\n    x0,\n    save_to_disk=True,\n    fname='resnet18'\n)\n\n\nret0_nc = net(x0)\nret1_nc = net(x1)\n\nassert not np.allclose(ivy.to_numpy(ret0_nc), ivy.to_numpy(ret1_nc))\n\nret0_c = comp_network(x0)\nret1_c = comp_network(x1)\n\nassert not np.allclose(ivy.to_numpy(ret0_c), ivy.to_numpy(ret1_c))\nassert np.allclose(ivy.to_numpy(ret0_nc), ivy.to_numpy(ret0_c))\nassert np.allclose(ivy.to_numpy(ret1_nc), ivy.to_numpy(ret1_c))\n\nUsing cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n  warnings.warn(\n\n\nLet’s compare inference speed of original network and compiled network. In general compiled network is faster due to graph compiler optimizations.\n\nN = 1000\nres = timeit.timeit(lambda: net(x0), number=N)\nprint(res/N,'ms')\n\n0.004645566477999864 ms\n\n\n\nN = 1000\nres = timeit.timeit(lambda: comp_network(x0), number=N)\nprint(res/N,'ms')\n\n0.0044566806820000695 ms\n\n\nHere is how graph of the network looks like.\n\nImage('resnet18.png')\n\n\n\n\nBy passing return_graph=True parameter to compile_graph you can obtain graph object. It has _all_functions attribute that stores all the functions in order of their execution.\nIf time_chronological=True the order of functions will the same as in the definition of network. If time_chronological=False Ivy’s graph compiler will try to find optimal order for better inference speed without affecting the accuracy.\n\ncomp_network, graph = compile_graph(net, x0, return_graph=True, time_chronological=True)\nprint([fn.__name__ for fn in graph._all_functions])\n\n/usr/local/lib/python3.8/dist-packages/torch/distributed/distributed_c10d.py:262: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n  warnings.warn(\n\n\n['conv2d', 'batch_norm', 'relu', 'max_pool2d', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'conv2d', 'batch_norm', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'conv2d', 'batch_norm', 'relu', 'conv2d', 'batch_norm', '__iadd__', 'relu', 'adaptive_avg_pool2d', 'flatten', 'linear']"
  },
  {
    "objectID": "wip/compilation_of_a_basic_function.html",
    "href": "wip/compilation_of_a_basic_function.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "!pip install ivy-core\n!pip install numpy\n\n\n\n\n\nimport ivy\nimport numpy as np\n\n\n\n\n\nimport ivy_compiler as ic\n\n\n\n\n\n\n\nivy.set_backend(\"numpy\")\n\n\n\n\n\nx = ivy.array([1., 2., 3.])\n\n\n\n\nIvy can compile any function that produce numerical outputs. Compiler track values from inputs to outputs and produce a computational graph from those operations and will ignore anything that does not affect final output value. It will ignore all intermediate dummy variables, operations, and print statements.\n\ndef original_fn(x):\n    for _ in range(100000):\n        pass\n    y = (x + 3) * 4\n    z = (x ** y) - 3 * y\n    x = x**2\n    f = ivy.var(y)\n    k = np.cos(x)\n    m = ivy.sin(k)\n    o = np.tan(m)\n    return x\n\n\n\n\n\ncomp_fn = ic.compile_graph(original_fn, x)\n\n\n\n\nGiven that function is compiled, its result can be compared to the original function.\n\nexpected_result = original_fn(x)\ncompiled_result = comp_fn(x)\n\nprint(expected_result)\nprint(compiled_result)\n\nAs you can see, both functions produce the same results, which is what we want 🙂!\n\n\n\n\nSimilarly to compiling functions, you can compile a neural network. The compilation works in exactly the same manner and will ignore all irrelevant opeations.\n\n\n\nclass Network(ivy.Module):\n    def __init__(self):\n        self._layer = ivy.Linear(3, 3)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        return self._layer(x)\n\n\n\n\n\nnet = Network()\n\n\n\n\n\nx = ivy.array([1., 2., 3.])\n\n\n\n\n\ncompiled_net = ic.compile_graph(net, x)\n\n\n\n\n\nprint(net(x))\nprint(compiled_net(x))"
  },
  {
    "objectID": "wip/basic_operations_with_ivy.html",
    "href": "wip/basic_operations_with_ivy.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "!pip install ivy-core\n!pip install torch\n!pip install tensorflow\n!pip install jax\n!pip install dm-haiku\n!pip install numpy\n\n\n\n\n\nimport ivy\n\n\n\n\nIvy is a unified machine learning framework that aims to provide a single interface for working with various machine learning libraries, such as Numpy, TensorFlow, PyTorch, and Jax. With Ivy, you can use the same code to build and train machine learning models, regardless of the underlying library being used. All you have to do is to change one line of code 😉\n\n\nWith Ivy, you can define your data and operations just once and easily switch between different frameworks. To do this, simply write your operations in Ivy and use the ivy.set_framework() function to change the underlying framework.\nP.S. there are some more advanced ways of handling backend frameworks in Ivy, so check it out in our Deep Dive.\n\n\n\nFirstly, let’s set the backend to Tensorflow\n\nivy.set_framework('tensorflow')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n<class 'tensorflow.python.framework.ops.EagerTensor'>\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\n\nNow let’s try exactly the same code, but change the used backend framework to Pytorch.\n\nivy.set_framework('torch')\n\n\nx = ivy.array([1, 2, 3])\ny = ivy.array([4, 5, 6])\nprint((type(ivy.to_native(x))))\nprint(ivy.stack((x, y)))\n\n<class 'torch.Tensor'>\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n\n\nYou can see that defined Ivy arrays have either tf.Tensor or torch.Tensor types underneath it without any need to worry about their types.\n\n\n\nBy saying that framework can be changed by just one line of code, we really mean it 🙂! By using Ivy as an ML framework, you do not need to worry about different function namings in different frameworks.\nTake a clip by value operator as an example. It performs the same operation across frameworks, but has different name and argument names. Numpy:\nnp.clip(a, a_min, a_max, out=None)\nTensforflow:\ntf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\nPytorch:\ntorch.clamp(input, min=None, max=None, *, out=None)\nJax:\njax.numpy.clip(a, a_min=None, a_max=None, out=None)\nHere are some examples\n\nimport tensorflow as tf\nt = tf.constant([[-10., -1., 0.], [0., 2., 10.]])\nprint(tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1))\n\nimport numpy as np\nn = np.array([[-10., -1., 0.], [0., 2., 10.]])\nprint(np.clip(n, a_min=-1, a_max=1))\n\ntf.Tensor(\n[[-1. -1.  0.]\n [ 0.  1.  1.]], shape=(2, 3), dtype=float32)\n[[-1. -1.  0.]\n [ 0.  1.  1.]]\n\n\nIvy allows you not to worry about such things. Now let’s do the same solely in Ivy.\n\nivy.set_framework('numpy')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('torch')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\ntensor([[-1., -1.,  0.],\n        [ 0.,  1.,  1.]])\n\n\n\nivy.set_framework('tensorflow')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[-1., -1.,  0.],\n       [ 0.,  1.,  1.]], dtype=float32)>\n\n\n\nivy.set_framework('jax')\ni = ivy.array([[-10., -1., 0.], [0., 2., 10.]])\nivy.clip(i, -1, 1)\n\nDeviceArray([[-1., -1.,  0.],\n             [ 0.,  1.,  1.]], dtype=float32)\n\n\nAs you see, the only line that changed here is ivy.set_framework().\n\n\n\nFinally, functions defined in Ivy are framework agnostic. In the example below we show how Ivy’s concatenation function is compatible with tensors from different frameworks. This is the same for all Ivy functions. They can accept tensors from any framework and return the correct result.\n\nimport jax.numpy as jnp\nimport tensorflow as tf\nimport numpy as np\nimport torch\n\nimport ivy\n\njax_concatted   = ivy.concat((jnp.ones((1,)), jnp.ones((1,))), -1)\ntf_concatted    = ivy.concat((tf.ones((1,)), tf.ones((1,))), -1)\nnp_concatted    = ivy.concat((np.ones((1,)), np.ones((1,))), -1)\ntorch_concatted = ivy.concat((torch.ones((1,)), torch.ones((1,))), -1)\n\n\n\n\n\nFinally, let’s train a simple two layer network using Ivy.\n\n\nYou can change the framework to any of the following: torch, tensforflow, or jax.\n\nivy.set_framework('torch')\n\n\n\n\n\nclass MyModel(ivy.Module):\n    def __init__(self):\n        self.linear0 = ivy.Linear(3, 64)\n        self.linear1 = ivy.Linear(64, 1)\n        ivy.Module.__init__(self)\n\n    def _forward(self, x):\n        x = ivy.relu(self.linear0(x))\n        return ivy.sigmoid(self.linear1(x))\n\n\n\n\n\nmodel = MyModel()\n\n\n\n\n\noptimizer = ivy.Adam(1e-4)\n\n\n\n\n\nx_in = ivy.array([1., 2., 3.])\ntarget = ivy.array([0.])\n\n\n\n\n\ndef loss_fn(v):\n    out = model(x_in, v=v)\n    return ivy.reduce_mean((out - target)**2)[0]\n\n\n\n\n\nfor step in range(100):\n    loss, grads = ivy.execute_with_gradients(loss_fn, model.v)\n    model.v = optimizer.step(model.v, grads)\n    print('step {} loss {}'.format(step, ivy.to_numpy(loss).item()))\n\nprint('Finished training!')\n\nstep 0 loss 0.49040043354034424\nstep 1 loss 0.48975786566734314\nstep 2 loss 0.4892795979976654\nstep 3 loss 0.48886892199516296\nstep 4 loss 0.4884953498840332\nstep 5 loss 0.4881443977355957\nstep 6 loss 0.4878086447715759\nstep 7 loss 0.48748287558555603\nstep 8 loss 0.48716384172439575\nstep 9 loss 0.48684927821159363\nstep 10 loss 0.48653748631477356\nstep 11 loss 0.48622724413871765\nstep 12 loss 0.4859171509742737\nstep 13 loss 0.48560672998428345\nstep 14 loss 0.48529526591300964\nstep 15 loss 0.4849821627140045\nstep 16 loss 0.48466697335243225\nstep 17 loss 0.4843493402004242\nstep 18 loss 0.4840289056301117\nstep 19 loss 0.4837053418159485\nstep 20 loss 0.4833785891532898\nstep 21 loss 0.4830484390258789\nstep 22 loss 0.48271444439888\nstep 23 loss 0.48237672448158264\nstep 24 loss 0.48203518986701965\nstep 25 loss 0.48168954253196716\nstep 26 loss 0.4813397228717804\nstep 27 loss 0.4809857904911041\nstep 28 loss 0.48062753677368164\nstep 29 loss 0.48026490211486816\nstep 30 loss 0.479898065328598\nstep 31 loss 0.47952669858932495\nstep 32 loss 0.4791509211063385\nstep 33 loss 0.4787706732749939\nstep 34 loss 0.47838595509529114\nstep 35 loss 0.4779967665672302\nstep 36 loss 0.47760307788848877\nstep 37 loss 0.4772048890590668\nstep 38 loss 0.47680220007896423\nstep 39 loss 0.47639501094818115\nstep 40 loss 0.47598329186439514\nstep 41 loss 0.4755673110485077\nstep 42 loss 0.4751465618610382\nstep 43 loss 0.4747215211391449\nstep 44 loss 0.4742920398712158\nstep 45 loss 0.47385817766189575\nstep 46 loss 0.47341999411582947\nstep 47 loss 0.47297725081443787\nstep 48 loss 0.4725303053855896\nstep 49 loss 0.47207894921302795\nstep 50 loss 0.47162333130836487\nstep 51 loss 0.47116345167160034\nstep 52 loss 0.470699280500412\nstep 53 loss 0.47023090720176697\nstep 54 loss 0.4697583019733429\nstep 55 loss 0.46928152441978455\nstep 56 loss 0.46880054473876953\nstep 57 loss 0.4683155119419098\nstep 58 loss 0.4678264260292053\nstep 59 loss 0.46733325719833374\nstep 60 loss 0.46683603525161743\nstep 61 loss 0.4663347601890564\nstep 62 loss 0.4658295214176178\nstep 63 loss 0.465320348739624\nstep 64 loss 0.4648073613643646\nstep 65 loss 0.46429020166397095\nstep 66 loss 0.4637692868709564\nstep 67 loss 0.46324464678764343\nstep 68 loss 0.4627160429954529\nstep 69 loss 0.4621836841106415\nstep 70 loss 0.4616474211215973\nstep 71 loss 0.46110764145851135\nstep 72 loss 0.460563987493515\nstep 73 loss 0.4600166976451874\nstep 74 loss 0.45946577191352844\nstep 75 loss 0.45891112089157104\nstep 76 loss 0.45835286378860474\nstep 77 loss 0.4577910006046295\nstep 78 loss 0.45722562074661255\nstep 79 loss 0.45665669441223145\nstep 80 loss 0.4560841917991638\nstep 81 loss 0.4555082619190216\nstep 82 loss 0.45492875576019287\nstep 83 loss 0.45434585213661194\nstep 84 loss 0.45375964045524597\nstep 85 loss 0.4531698524951935\nstep 86 loss 0.4525766670703888\nstep 87 loss 0.45198020339012146\nstep 88 loss 0.4513803720474243\nstep 89 loss 0.4507772624492645\nstep 90 loss 0.4501707851886749\nstep 91 loss 0.4495610296726227\nstep 92 loss 0.4489481747150421\nstep 93 loss 0.44833192229270935\nstep 94 loss 0.4477125108242035\nstep 95 loss 0.44708991050720215\nstep 96 loss 0.44646409153938293\nstep 97 loss 0.44583529233932495\nstep 98 loss 0.4452032148838043\nstep 99 loss 0.44456806778907776\nFinished training!\n\n\n\nloss_fn(model.v)\n\ntensor(0.4439, grad_fn=<SelectBackward0>)\n\n\nWe hope that this short demo gives you a better understanding of basic Ivy functionality and got your interest in learning more about Ivy!"
  },
  {
    "objectID": "wip/deepmind_perceiver_io.html",
    "href": "wip/deepmind_perceiver_io.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "In this example, we will unify and transpile a classifier built on top of DeepMind’s original implementation of the PerceiverIO module written in JAX!\n\n\n\nDefining the model\nSome helper functions\nGetting images (and noise!) for the classifier\nCompiling the model\nUnifying the model\nTranspiling the model\n\nFirst of all, let’s import Ivy 🚀\n\nimport ivy\n\n2023-01-13 17:53:19.518928: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-01-13 17:53:19.518952: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n\n\nWARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/util/decorator_utils.py:153: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n\n\n\n\n\n(Psst! The PerceiverIO code we will use comes from the original repo)\nIn order to use an image classifier on top of the PerceiverIO model, we must import some things first from the DeepMind repo:\n\n# utils\nimport os\nimport cv2\nimport pickle\nimport imageio\nimport urllib.request\n# JAX imports\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nrng = jax.random.PRNGKey(23)\n# Perceiver implementation\nfrom demos.perceiver_io.perceiver_io_deepmind.imagenet_labels import IMAGENET_LABELS\nfrom demos.perceiver_io.perceiver_io_deepmind.model import imagenet_classifier, fourier_pos_configs\n\nWARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n\n\nUsing the PerceiverIO module, we have now loaded an Imagenet classifier which is a Haiku (i.e. JAX) TransformedWithState instance.\n\ntype(imagenet_classifier)\n\nhaiku._src.transform.TransformedWithState\n\n\nWe also need to download the original parameters published by DeepMind if we want to save some computing 😅\nThankfully Ivy will transform those for us as well 😉\n\ncwd = os.getcwd()\ncheckpoint_path = os.path.join(cwd, \"demos\", \"imagenet_checkpoint.pystate\")\nif not os.path.exists(checkpoint_path):\n    url = \"https://storage.googleapis.com/perceiver_io/imagenet_fourier_position_encoding.pystate\"\n    _ = urllib.request.urlretrieve(url, checkpoint_path)\n\nwith open(checkpoint_path, \"rb\") as f:\n    ckpt = pickle.loads(f.read())\n\nparams = ckpt[\"params\"]\nstate = ckpt[\"state\"]\n\nFinally, let’s write a function that applies the downloaded parameters.\n\ndef imagenet_classify(image):\n    return imagenet_classifier.apply(params, state, rng, fourier_pos_configs, image)\n\n\n\n\nIn order to correctly use the classifier, we will need to use some preprocessing functions (also from the DeepMind repo)\n\nMEAN_RGB = (0.485 * 255, 0.456 * 255, 0.406 * 255)\nSTDDEV_RGB = (0.229 * 255, 0.224 * 255, 0.225 * 255)\n\n\ndef normalize(im):\n    return (im - np.array(MEAN_RGB)) / np.array(STDDEV_RGB)\n\ndef denormalize(im):\n    return im * np.array(STDDEV_RGB) + np.array(MEAN_RGB)\n\n\ndef resize_and_center_crop(image):\n    \"\"\"Crops to center of image with padding then scales.\"\"\"\n    shape = image.shape\n    image_height = shape[0]\n    image_width = shape[1]\n    padded_center_crop_size = (\n        (224 / (224 + 32)) * np.minimum(image_height, image_width).astype(np.float32)\n    ).astype(np.int32)\n    offset_height = ((image_height - padded_center_crop_size) + 1) // 2\n    offset_width = ((image_width - padded_center_crop_size) + 1) // 2\n    crop_window = [\n        offset_height,\n        offset_width,\n        padded_center_crop_size,\n        padded_center_crop_size,\n    ]\n    image = image[\n        crop_window[0] : crop_window[0] + crop_window[2],\n        crop_window[1] : crop_window[1] + crop_window[3],\n    ]\n    return cv2.resize(image, (224, 224), interpolation=cv2.INTER_CUBIC)\n\nSame for the visualization of results, let’s write some helpers to have nice outputs!\n\nfrom matplotlib.pyplot import figure, imshow, axis\n\ndef print_labels(probs, indices):\n    for i in list(indices):\n        i = int(i)\n        print(f\"{IMAGENET_LABELS[i].split(',')[0][:15].ljust(20)}: {probs[i]}\")\n\ndef show_images_row(images):\n    fig = figure(figsize=(12,8))\n    number_of_files = len(images)\n    for i, image in enumerate(images):\n        a=fig.add_subplot(1,number_of_files,i+1)\n        imshow(np.array(image))\n        axis('off')\n\ndef show_device_array(device_array):\n    im = np.array(device_array[0])\n    im = denormalize(im)\n    im = im/256.0\n    im = np.clip(im, 0, 1)\n    fig = figure(figsize=(3,2))\n    imshow(im)\n    axis('off')\n\n\n\n\nTo compile and test the classifier we are going to need two types of images: - First, an image with random noise, which we will use to compile the graph while ensuring that input-specific information is not registered. - Secondly, a few images to test the compiled and transpiled models on!\n\ntest_urls = {\n    \"goldfish\": \"https://github.com/EliSchwartz/imagenet-sample-images/raw/master/n01443537_goldfish.JPEG\",\n    \"roof\": \"https://github.com/EliSchwartz/imagenet-sample-images/raw/master/n04435653_tile_roof.JPEG\",\n    \"camera\": \"https://github.com/EliSchwartz/imagenet-sample-images/raw/master/n04069434_reflex_camera.JPEG\",\n}\nimage_paths = {c: os.path.join(cwd, \"demos\", f\"{c}.jpg\") for c in test_urls.keys()}\n\nfor class_, image_path in image_paths.items():\n    if not os.path.exists(image_path):\n        url = test_urls[class_]\n        _ = urllib.request.urlretrieve(url, image_path)\nimgs = []\n\nfor image_path in image_paths.values():\n    with open(image_path, \"rb\") as f:\n        img = imageio.imread(f)\n        img = resize_and_center_crop(img)\n        imgs.append(img)\n\n# generate noise\nnoise = np.random.uniform(size=(224, 224, 3))\n\nLet’s see what we got:\n\nshow_images_row([noise, *imgs])\n\n\n\n\nLooking good! We can now convert the images to jax.numpy.Arrays.\n\nnoise = jnp.expand_dims(jnp.array(noise), 0)\n\nfor i, img in enumerate(imgs):\n    img = normalize(img)\n    img = jnp.array(img)[None]\n    imgs[i] = img\n\n\n\n\nThe first thing we can do with the model is compiling it into a graph, this way, we remove any part of the code that doesn’t contribute to the output directly which, you guessed it, makes it faster 🚀\n\nivy.set_backend(\"jax\")\ngraph = ivy.compile(imagenet_classify, noise)\n\nAfter compiling with the noisy image, let’s use our first real image to check the inference speed and the accuracy of the compiled result.\n\nimg = imgs[0]\nshow_device_array(img)\n\n\n\n\n\n%%time\noriginal_output, _ = imagenet_classify(img)\n\nCPU times: user 41 s, sys: 4.24 s, total: 45.3 s\nWall time: 9.5 s\n\n\n\n%%time\ncompiled_output, _ = graph(img)\n\nCPU times: user 15.5 s, sys: 1.08 s, total: 16.6 s\nWall time: 3.65 s\n\n\n\nnp.allclose(original_output, compiled_output, atol=0.0001)\n\nTrue\n\n\nAs we can see above, the compiled graph is faster than the original model while yielding the same result!\n\n\n\nBy unifying the model, we will get a model that can be used with any ML framework. Don’t forget that we aren’t just converting the inputs, all the operations within the model will happen in your framework of choice!\n\nivy_graph = ivy.unify(\n    imagenet_classify,\n    original_backend=\"jax\",\n    args=(noise,),\n)\n\nLet’s pick the second image as an example:\n\nimg = imgs[1]\nshow_device_array(img)\n\n\n\n\nAnd calculate the output using each one of the frameworks:\n\noriginal_logits, _ = imagenet_classify(img)\n\n\nivy.set_backend(\"jax\")\njax_logits, _ = ivy_graph(img)\nnp.allclose(original_logits, jax_logits, atol=0.0001)\n\nTrue\n\n\n\nivy.set_backend(\"torch\")\ntorch_logits, _ = ivy_graph(img)\nnp.allclose(original_logits, torch_logits, atol=0.0001)\n\nTrue\n\n\n\nivy.set_backend(\"tensorflow\")\ntf_logits, _ = ivy_graph(img)\nnp.allclose(original_logits, tf_logits, atol=0.0001)\n\nTrue\n\n\n\nivy.set_backend(\"numpy\")\nnp_logits, _ = ivy_graph(img)\nnp.allclose(original_logits, np_logits, atol=0.0001)\n\nTrue\n\n\nAs expected, running the unified model in any of the frameworks results in the same outputs as the original model!\n\n\n\nUnifying is pretty cool, but sometimes we need to use code from a framework in projects which are already built using another framework. If we have, a project written in PyTorch but still want to use the PerceiverIO module, we can use transpile to convert it directly into a PyTorch module. It only takes one line of code. Let’s test this with the third image.\n\nimg = imgs[2]\nshow_device_array(img)\n\n\n\n\n\nivy.set_backend(\"jax\")\ntranspiled_graph = ivy.transpile(imagenet_classify, original_backend=\"jax\", to=\"torch\", args=(noise,))\n\nFirst, we call the original function, which as expected, yields a JAX DeviceArray instance.\n\nlogits, _ = imagenet_classify(img)\ntype(logits)\n\njaxlib.xla_extension.DeviceArray\n\n\nIf we now call the transpiled graph, we can see that the output is, in fact, a torch.Tensor, due to the transpiled graph being now fully composed of torch code.\n\ntranspiled_logits, _ = transpiled_graph(img)\ntype(transpiled_logits)\n\ntorch.Tensor\n\n\nOnce again, we can check the predictions of both the original and the transpiled model, to see that their results match correctly!\n\n_, indices = ivy.to_numpy(ivy.top_k(logits[0], 5))\nprobs = ivy.to_numpy(ivy.softmax(logits[0]))\nprint(\"Original - Top 5 labels:\")\nprint_labels(probs, indices)\n\nOriginal - Top 5 labels:\nreflex camera       : 0.9125176974309779\nPolaroid camera     : 0.0062769926141319965\nlens cap            : 0.002572748444021628\ntripod              : 0.0013543223305103043\nwasher              : 0.0008780962522585626\n\n\n\nivy.set_backend(\"torch\")\n_, indices = ivy.top_k(transpiled_logits[0], 5)\nprobs = ivy.to_numpy(ivy.softmax(transpiled_logits[0]))\nprint(\"Transpiled - Top 5 labels:\")\nprint_labels(probs, indices)\n\nTranspiled - Top 5 labels:\nreflex camera       : 0.9125140905380249\nPolaroid camera     : 0.006276924163103104\nlens cap            : 0.0025728049222379923\ntripod              : 0.0013543171808123589\nwasher              : 0.0008780973730608821\n\n\n\n\n\nThat’s pretty much it! Did you have an Image Augmentation pipeline which used Kornia or some other PyTorch specific package? Maybe some complex meta-learning functions already written in Torch? Now you are free to use, finetune, or deploy DeepMind’s PerceiverIO model and trained parameters in every framework and every hardware!"
  },
  {
    "objectID": "wip/0_building_blocks/0_0_unify.html",
    "href": "wip/0_building_blocks/0_0_unify.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "In this example, we unify a simple torch function normalize. We then show how this newly unified normalize function can be used alongside any ML framework!\n\n \n\nFirstly, let’s import the dependencies and define the torch function.\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nNow, let’s unify the function!\n\nnormalize = ivy.unify(normalize)\n\nAnd that’s it! The normalize function can now be used with any ML framework. It’s as simple as that!\nSo, let’s give it a try!\n\n# import numpy\nimport numpy as np\n\n# create random numpy arrays for testing\nx = np.randon.uniform(size=10)\nmean = np.mean(x)\nstd = np.std(x)\n\n# numpy\nprint(normalize(x, mean, std))\n\n# jax\nimport jax.numpy as jnp\nx_ = jnp.array(x)\nmean_ = jnp.array(mean)\nstd_ = jnp.array(std)\nprint(normalize(x_, mean_, std_))\n\n# tensorflow\nimport tensorflow as tf\nx_ = tf.constant(x)\nmean_ = tf.constant(mean)\nstd_ = tf.constant(std)\nprint(normalize(x_, mean_, std_))\n\n# torch\nx_ = torch.tensor(x)\nmean_ = torch.tensor(mean)\nstd_ = torch.tensor(std)\nprint(normalize(x_, mean_, std_))\n\nWe can see that the new normalize function can operate with any ML framework. ivy.unify is able to detect that the original normalize function is implemented in torch by using the inspection module. ivy.unify then converts the framework-specific torch implementation into a framework-agnostic ivy implementation, which is compatible with all frameworks.\n\n\nThat’s it, you can now unify ML code! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how to make our unified Ivy code run much more efficiently! ⚡"
  },
  {
    "objectID": "wip/0_building_blocks/0_2_transpile.html",
    "href": "wip/0_building_blocks/0_2_transpile.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "In this example, we transpile the original normalize function from torch to jax in one line of code. This is a common use case, where there is one target framework in mind.\n\n \n\nUsing what we learnt in the previous two notebooks for Unify and Compile, the workflow for converting directly from torch to jax would be as follows, first unifying to ivy code, and then compiling to the jax backend:\n\nimport ivy\nimport torch\nivy.set_backend(\"jax\")\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.compile(ivy.unify(normalize))\n\nnormalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project.\nThis workflow is common, and so in order to avoid repeated calls to ivy.unify followed by ivy.compile, there is another convenience function ivy.transpile, which basically acts as a shorthand for this pair of function calls:\n\nnormalize = ivy.transpile(normalize)\n\nAgain, normalize is now compiled to machine-code, specifically for jax, ready to be integrated into your wider jax project.\n\n\nThat’s it, you can now transpile code from one framework to another with one line of code! That concludes the collection of notebooks on the “Building Blocks”. However, there are still many other important topics to master before you’re ready to unify ML code like a pro 🥷. In the next collection of notebooks “The Basics”, we’ll be learning about the various different ways that ivy.unify, ivy.compile and ivy.transpile can be called, and what implications each of these have."
  },
  {
    "objectID": "wip/0_building_blocks/0_1_compile.html",
    "href": "wip/0_building_blocks/0_1_compile.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "In this example, we compile our simple unified ivy function normalize from the last notebook. We then show how this newly compiled normalize function exhibits much better runtime performance than the non-compiled version.\n\n \n\nFirstly, let’s pick up where we left off in the last notebook, with our unified normalize function:\n\nimport ivy\nimport torch\n\ndef normalize(x, mean, std):\n    return torch.div(torch.sub(x, mean), std)\n\nnormalize = ivy.unify(normalize)\n\nFor the purpose of illustration, we will use jax as our backend framework:\n\n# set ivy's backend to jax\nivy.set_backend(\"jax\")\n\n# Import jax numpy API\nimport jax.numpy as jnp\n\n# create random jax arrays for testing\nx = jnp.randon.uniform(size=10)\nmean = jnp.mean(x)\nstd = jnp.std(x)\n\nAs in the previous example, the unified function can be executed like so (in this case it will trigger lazy unification, see the Lazy vs Eager section for more details):\n\nnormalize(x, mean, std)\n\nWhen calling this function, all of ivy’s function wrapping is included in the call stack of normalize, which adds runtime overhead. In general, ivy.compile strips any arbitrary function down to its constituent functions in the functional API of the target framework. It will then also be compiled to machine-code if the target framework supports low-level compiling (via functions such as tf.function, torch.jit.script, torch.jit.trace, torch.compile, jax.jit etc.). The code can be compiled like so:\n\ncomp = ivy.compile(normalize)  # compiles to jax, due to ivy.set_backend\n\nThe compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\n\ncomp(x, mean, std)\n\nThe machine-code compilation can be turned off by setting the argument low_level = False, in which case it will simply return a chain of Python function in the functional API of the target framework (in this case JAX). This will still improve the runtime efficiency over the original un-compiled version due to the removal of all ivy wrapping overhead, but it will not be as runtime efficient as the low-level compiled version:\n\npartial_comp = ivy.compile(normalize, low_level=False)\n\nAgain, the compiled function can be executed in exactly the same manner as the non-compiled function (in this case it will also trigger lazy compilation, see the Lazy vs Eager section for more details):\n\npartial_comp(x, mean, std)\n\nWith all lazy unification and compilation calls now performed (which all increase runtime during the very first call of the function), we can now assess the runtime efficiencies of each function:\n\nivy.time_function(normalize)(x, mean, std)\nivy.time_function(partial_comp)(x, mean, std)\nivy.time_function(comp)(x, mean, std)\n\nAs expected, we can see that the slowest is normalize, which includes all ivy wrapping overhead. Next is partial_comp which has no wrapping overhead but is still expressed entirely in Python, without compiling to low-level code. The fastest is comp because the wrapping overhead is removed and the function is compiled to low-level code for maximal efficiency.\n\n\nThat’s it, you can now compile ivy code for more efficient inference! However, there are several other important topics to master before you’re ready to unify ML code like a pro 🥷. Next, we’ll be learning how to transpile code from one framework to another in a single line of code 🔄"
  },
  {
    "objectID": "guides/02_transpiling_a_haiku_model.html",
    "href": "guides/02_transpiling_a_haiku_model.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Transpiling a haiku model to build on top\nTranspile DeepMind’s PerceiverIO to PyTorch to use it as a pretrained backbone.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "guides/03_transpiling_a_tf_model.html",
    "href": "guides/03_transpiling_a_tf_model.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Transpiling a TensorFlow model to build on top\nTranspile a keras model to haiku and leverage Jax efficiency.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "guides/04_developing_a_convnet_with_ivy.html",
    "href": "guides/04_developing_a_convnet_with_ivy.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Developing a convolutional network using Ivy\nBuild a more involved convolutional network using Ivy’s stateful API.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  },
  {
    "objectID": "guides/01_transpiling_a_torch_model.html",
    "href": "guides/01_transpiling_a_torch_model.html",
    "title": "Tutorials and Examples - Ivy",
    "section": "",
    "text": "Transpiling a PyTorch model to build on top\nTranspile a timm model to tensorflow and build a new model around it.\n\n \n\n\n# TO REPLACE:\n# All contents start from here!\n# Use ## tag (h2) for section titles\n# Use ### tag (h3) for subsection titles\n# Use normal paragraph (p, no tag) for steps and explanations"
  }
]